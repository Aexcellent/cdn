arXiv:2008.10838v3 [cs.LG] 15 Jun 2024

1
FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training
YAN KANG, WeBank, China YANG LIU*, Institute for AI Industry Research, Tsinghua University, China XINLE LIANG, WeBank, China
Federated learning allows multiple parties to build machine learning models collaboratively without exposing data. In particular, vertical federated learning (VFL) enables participating parties to build a joint machine learning model based upon distributed features of aligned samples. However, VFL requires all parties to share a sufficient amount of aligned samples. In reality, the set of aligned samples may be small, leaving the majority of the non-aligned data unused. In this paper, we propose Federated Cross-View Training (FedCVT), a semisupervised learning approach that improves the performance of the VFL model with limited aligned samples. More specifically, FedCVT estimates representations for missing features, predicts pseudo-labels for unlabeled samples to expand the training set, and trains three classifiers jointly based upon different views of the expanded training set to improve the VFL modelâ€™s performance. FedCVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on NUS-WIDE, Vehicle, and CIFAR10 datasets. The experimental results demonstrate that FedCVT significantly outperforms vanilla VFL that only utilizes aligned samples. Finally, we perform ablation studies to investigate the contribution of each component of FedCVT to the performance of FedCVT.
CCS Concepts: â€¢ Theory of computation â†’ Semi-supervised learning; â€¢ Computing methodologies â†’ Distributed computing methodologies.
Additional Key Words and Phrases: vertical federated learning, semi-supervised learning, cross-view training
ACM Reference Format: Yan Kang, Yang Liu, and Xinle Liang. 2022. FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training. J. ACM 1, 1, Article 1 (January 2022), 17 pages. https://doi.org/10.1145/3510031
1 INTRODUCTION With the increasingly stricter privacy-protection laws implemented worldwide [6, 9], federated learning has received significant attention and become a popular research topic recently [17]. As the research goes deeper and wider, the practice of federated learning has been expanded from building powerful mobile applications based on data resided in millions of mobile devices [28] to solving the problem of data silos among or within organizations [40]. For example, many business decisions of a bank may rely on its customersâ€™ purchasing preferences. This bank may share some customers with a local retail company that owns local peopleâ€™s purchasing preference data. Thus, the bank can invite
*corresponding author
Authorsâ€™ addresses: Yan Kang, yangkang@webank.com, WeBank, Shenzhen, China; Yang Liu, liuy03@air.tsinghua.edu.cn, Institute for AI Industry Research, Tsinghua University, Beijing, China; Xinle Liang, shindler@mail.ustc.edu.cn, WeBank, Shenzhen, China.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0004-5411/2022/1-ART1 $15.00 https://doi.org/10.1145/3510031
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:2

Kang, et al.

the retail company to build a joint model collaboratively by leveraging the data features of both sides to improve its business. The retail company can also benefit from this joint model.
This and various other similar practical demands [40] motivate the development of vertical federated learning (VFL)[39] (also called feature-partitioned federated learning), which enables participating parties to train a joint machine learning model collaboratively by utilizing scattered features of their aligned samples without disclosing original data. However, a critical prerequisite of VFL is that it requires all parties to share a sufficient amount of aligned samples to achieve competitive model performance. [26] proposes a federated transfer learning framework to address weak supervision (few labels) problems in the VFL setting. Nonetheless, it does not take full advantage of unlabeled samples for improving learning quality, and it aims to build a model only for the party of the target domain. Some other similar approaches, such as domain adaptation [24, 33, 34] and knowledge distillation [11, 22], have been applied in the federated learning setting. However, they mainly focus on scenarios where all parties share the same feature space (also known as horizontal federated learning [39]). Therefore, research on applying semi-supervised techniques in the VFL setting is insufficient.
This paper proposes a novel semi-supervised algorithm in the VFL setting, termed FedCVT, to address the limitations of existing vertical federated learning approaches. Our contributions are as follows:
(1) FedCVT significantly boosts up the performance of the federated model when the aligned samples between participating parties are limited;
(2) FedCVT works with data of various types. This feature is important in practice since real-world VFL applications often need to deal with heterogeneous features;
(3) FedCVT does not require participating parties to share their original data and model parameters, but only intermediate representations and gradients, which can be further protected by the privacy-preserving VFL-DNN (Deep Neural Network) framework proposed by [18] and implemented in FATE [25]1.
2 RELATED WORK
In this section, we focus on reviewing closely related fields and approaches.
Vertical Federated Learning (VFL). VFL is also known as feature-partitioned machine learning in some literature. The secure linear machine learning with data partitioned in the feature space has been well studied in [8, 14, 29]. They apply either hybrid secure multi-party computation (SMPC) protocol or homomorphic encryption (HE) [35] to protect privacy in model training and inference. [3] proposes a secure federated tree-boosting (SecureBoost) approach in the VFL setting. It enables participating parties with different features to build a set of boosting trees collaboratively and proves that the SecureBoost provides the same level of accuracy as its non-privacy-preserving centralized counterparts. FATE [25] designed and implemented a VFL-DNN (Deep Neural Network) framework that supports DNN in the VFL setting. VFL-DNN leverages a hybrid encryption scheme in the forward and backward stages of the VFL training procedure to protect data privacy and model security. [38] proposed several configurations of splitting a deep neural network to support various forms of collaborations among health facilities that each holds a partial deep network and owns a different modality of patient data.
Semi-Supervised Learning (SSL). SSL aims to alleviate the need for numerous labeled data to train machine learning models by leveraging unlabeled data. Recent semi-supervised learning works utilize transfer learning [32], consistency regularization [21, 36] and pseudo-labeling [5, 15] to learn
1https://github.com/FederatedAI/FATE

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:3

from unlabeled data. [1] proposed MixMatch that unifies three dominant semi-supervised methods: entropy minimization [12], consistency regularization and generic regularization. It demonstrates that MixMatch can approach similar error rates as fully supervised training but depends on significantly fewer training data. [5] proposed a semi-supervised Cross-View Training (CVT) approach that simultaneously trains a full model based upon all labeled data and multiple auxiliary models that only see restricted views of the unlabeled data. During training, CVT teaches auxiliary models to match predictions made by the full model to improve representation learning, thereby improving the performance of the full model. While successful, these works are not designed for VFL scenarios where features are scattered among parties. [26] proposed a secure Federated Transfer Learning (FTL) framework, which is the first framework that enables VFL to benefit from transfer learning. FTL helps the target domain party build a competitive prediction model by utilizing rich label resources from the source domain.

3 PROBLEM DEFINITION

We consider a two-party vertical federated learning setting where only one party has labels. This is

the typical VFL setting defined in [40] where the party having labels is short of features to build an

accurate model, and thus it leverages complimentary features provided by a second party. Specifically,

party A has dataset Dğ´ := {(ğ‘‹ ğ´,ğ‘–, ğ‘Œ ğ´,ğ‘– )}ğ‘›ğ‘–=ğ´1 where ğ‘‹ ğ´,ğ‘– is the feature vector of the ğ‘–th sample and ğ‘Œ ğ´,ğ‘– âˆˆ {0, 1}ğ¶ is the corresponding one-hot encoding ground-truth label with ğ¶ classes for the ğ‘–th

sample, while party B has dataset Dğµ := {ğ‘‹ ğµ,ğ‘– }ğ‘›ğ‘–=ğµ1. Dğ´ and Dğµ are held privately by the two parties and cannot be exposed to each other. ğ‘›ğ´ and ğ‘›ğµ are numbers of samples for Dğ´ and Dğµ, respectively.

If we concatenate Dğ´ and Dğµ with samples (of different parties) having the same identity aligned,

we obtain a single dataset depicted in Figure 1. This dataset is vertically partitioned, and each

party owns one vertical partition (or a partial view) of this dataset. This is where the term "vertical

federated learning" comes from. We assume that there exists only a limited amount of aligned samples

Dğ‘ğ‘™

:=

{ğ‘‹ ğµ,ğ‘–
ğ‘ğ‘™

,

ğ´,ğ‘–
ğ‘‹
ğ‘ğ‘™

,

ğ´,ğ‘–
ğ‘Œ
ğ‘ğ‘™

}ğ‘›ğ‘– =ğ‘1ğ‘™

between

the

two parties. Party A owns

the

partition

Dğ´
ğ‘ğ‘™

:=

{ğ‘‹ ğ´,ğ‘–
ğ‘ğ‘™

,

ğ´,ğ‘–
ğ‘Œ
ğ‘ğ‘™

}ğ‘›ğ‘– =ğ‘1ğ‘™

and

party B

owns

Dğµ
ğ‘ğ‘™

:=

{ğ‘‹ ğµ,ğ‘–
ğ‘ğ‘™

}ğ‘›ğ‘– =ğ‘1ğ‘™

,

where ğ‘›ğ‘ğ‘™

is

the

number

of aligned samples.

One can perform

the

alignment through encrypted entity matching techniques in a privacy-preserving setting [30]. Here,

we assume that party A and party B have already established the alignment between their samples.

Fig. 1. View of the virtual dataset in vertical federated learning. Each party owns a vertical partition (or a partial view) of this dataset.

Other than aligned samples, the rest samples of each party have no aligned counterparts from the

other

party.

We

call

them

non-aligned

samples

and

denote

them

as

Dğ´
ğ‘›ğ‘™

:=

{ğ‘‹ ğ´,ğ‘–
ğ‘›ğ‘™

,

ğ´,ğ‘–
ğ‘Œ
ğ‘›ğ‘™

}ğ‘›ğ‘– =ğ‘›ğ´1ğ‘™

for

party

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:4

Kang, et al.

A

and

Dğµ
ğ‘›ğ‘™

:=

{ğ‘‹ ğµ,ğ‘–
ğ‘›ğ‘™

}ğ‘›ğ‘– =ğ‘›ğµ1ğ‘™

for

party

B.

From

the

perspective

of

a

single

tabular

dataset

(see

Figure

1),

each party has no features (or labels) that correspond to non-aligned samples of the other party. We

treat those features (or labels) are missing.

The conventional VFL is trying to build a federated machine learning model utilizing only aligned

samples

ğ·ğ‘ğ‘™ ,

leaving

non-aligned

samples

ğ·ğ´
ğ‘›ğ‘™

and

ğ·ğµ
ğ‘›ğ‘™

unused.

We,

therefore,

propose

a

semi-

supervised VFL with Cross-View Training (FedCVT) approach that not only leverages aligned

samples but also takes full advantages of non-aligned samples (shown in Figure 2), aiming to boost

the performance of VFL, especially when the amount of aligned samples is small.

4 OVERVIEW
Figure 2 depicts the overall workflow of our proposed FedCVT approach, which involves five stages:
(1) Learn representations from raw input features using neural networks; (2) Estimate representations for missing features based on learned representations; (3) Apply three trained classifiers ğ‘“ ğ´, ğ‘“ ğµ, and ğ‘“ ğ´ğµ to predict three candidate pseudo-labels for
each unlabeled sample; (4) When all the three candidate pseudo-labels for a sample are equal, and their probabilities are
above a predefined threshold, retain this pseudo-labeled sample in the training set; (5) Train the three classifiers ğ‘“ ğ´, ğ‘“ ğµ, and ğ‘“ ğ´ğµ jointly through vertical federated learning that each
classifier takes as input a different view of the training dataset.

Fig. 2. Overview of FedCVT approach
5 THE PROPOSED APPROACH As depicted in Figure 2, our FedCVT approach involves the following stages: learn representations from raw features (section 5.1); estimate representations for missing features (section 5.2); estimate pseudo-labels for unlabeled samples and cherry-pick pseudo-labeled samples (section 5.3); finally, jointly train three classifiers through vertical federated learning (section 5.4). In this section, we describe each of these stages in detail.
5.1 Representation Learning Deep neural networks have been widely used to learn representations [31]. In this work, we utilize two neural networks for each party to learn representations from raw input data. One is to learn representations weakly shared between the two parties and the other is to learn representations unique to each party.
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:5

Fig. 3. Learn representations from raw input features. Each party ğ‘ âˆˆ {ğ´, ğµ} has two neural network models hup and hcp to learn unique and shared representations, respectively.

As shown in Figure 3, for each party ğ‘

âˆˆ

{ğ´,

ğµ},

we

denote

ğ‘
ğ‘…ğ‘¢

=

hup (X p)

as the unique

rep-

resentations

and

ğ‘
ğ‘…ğ‘

=

hcp (X p)

as the shared

representations that are learned from raw input ğ‘‹ ğ‘

through

neural

networks

hup

and

hcp ,

respectively.

ğ‘
ğ‘…ğ‘¢

âˆˆ

Rğ‘ ğ‘ Ã—ğ‘‘ğ‘

and

ğ‘
ğ‘…ğ‘

âˆˆ

Rğ‘ ğ‘ Ã—ğ‘‘ğ‘ , where dp is

the dimension of the top hidden representation layer of neural networks of party ğ‘. The complete

learned

representations

for ğ‘‹ ğ‘

is

denoted

as ğ‘…ğ‘

=

[ğ‘…ğ‘ğ‘

;

ğ‘
ğ‘…ğ‘¢

]

,

where

[; ]

denotes

the

concatenation

operator

that

concatenates

two

matrices

along

the

feature

axis.

Further,

we

denote

ğ‘
ğ‘…

= [ğ‘…ğ‘

;

ğ‘
ğ‘…

]

ğ‘ğ‘™

ğ‘,ğ‘ğ‘™ ğ‘¢,ğ‘ğ‘™

as

representations

learned

from

aligned

samples

ğ‘‹

ğ‘

,

while

ğ‘
ğ‘…

= [ğ‘…ğ‘

;

ğ‘
ğ‘…

] as representations

ğ‘ğ‘™

ğ‘›ğ‘™

ğ‘,ğ‘›ğ‘™ ğ‘¢,ğ‘›ğ‘™

learned from non-aligned samples ğ‘‹ ğ‘ .
ğ‘›ğ‘™
Intuitively, neural network hcp aims to capture domain-invariant representations between the two parties while hup helps learn domain-specific representations. We propose following three loss terms

to enforce neural networks to learn the desired representations.

ğ´ğµ
ğ¿diff

(ğ‘…ğ´
ğ‘,ğ‘ğ‘™

,

ğµ
ğ‘…
ğ‘,ğ‘ğ‘™

)

=

1 ğ‘›ğ‘ğ‘™

âˆ‘ï¸

ğ´,ğ‘–
ğ‘…

âˆ’

ğµ,ğ‘–
ğ‘…

ğ‘,ğ‘ğ‘™ ğ‘,ğ‘ğ‘™

2 ğ¹

(1)

ğ‘–

ğ´
ğ¿

(ğ‘…ğ´,

ğ‘ ğ‘–ğ‘š ğ‘¢

ğ‘…ğ´)
ğ‘

=

1 ğ‘›ğ´

âˆ‘ï¸

ğ´,ğ‘–
ğ‘…

âŠ—

ğ´,ğ‘–
ğ‘…

ğ‘¢

ğ‘

2 ğ¹

(2)

ğ‘–

ğµ
ğ¿

(ğ‘…ğµ,

ğµ
ğ‘…

)

=

1

âˆ‘ï¸

ğµ,ğ‘–
ğ‘…

âŠ—

ğµ,ğ‘–
ğ‘…

2

ğ‘ ğ‘–ğ‘š ğ‘¢ ğ‘ ğ‘›ğµ

ğ‘¢

ğ‘ğ¹

(3)

ğ‘–

where

âŠ—

denotes

matrix

multiplication

operator

and

âˆ¥Â·âˆ¥2
ğ¹

denotes

the

squared

Frobenius

norm.

By

minimizing

(1),

neural

networks

â„ğ´
ğ‘

and

â„ğµ
ğ‘

are

pushed

to

learn

shared

representations

from

the

raw

features of the two parties. While (2) and (3) are orthogonality constraints [2] that encourage neural

networks

ğ‘
â„ğ‘

and

ğ‘
â„ğ‘¢

belonging

to

party

ğ‘

âˆˆ

{ğ´, ğµ}

to

learn

distinct

representations.

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:6

Kang, et al.

5.2 Representation Estimation

Instead of directly imputing values of missing features, we estimate representations of missing

features (see Figure 1). In other words, we want to estimate missing representations of party A that

correspond to non-aligned representations ğ‘…ğµ of party B and estimate missing representations of
ğ‘›ğ‘™
party B that correspond to non-aligned representations ğ‘…ğ´ of party A.
ğ‘›ğ‘™
Each missing representation is estimated through the weighted sum over representations learned

from raw features. The weights reflect the similarity between the representation to be estimated and

the learned representations. To compute these weights, we leverage the scaled dot-product attention

(SDPA) function [37]:

ğ‘„ âŠ— ğ¾ğ‘‡

ğ‘”(ğ‘„, ğ¾, ğ‘‰ ) = softmax( âˆš ) âŠ— ğ‘‰

(4)

ğ‘‘ğ‘˜ where ğ‘„ is the query matrix, ğ¾ is the key matrix, ğ‘‰ is the value matrix, and ğ‘„ and ğ¾ have the

same dimension denoted as ğ‘‘ğ‘˜ .

In the rest of this section, we will elaborate on how the SDPA function ğ‘” is applied to performing

the representation estimation by giving an example of estimating party Aâ€™s missing representations,

denoted as ğ‘…Ëœğ´, that correspond to the representations ğ‘…ğµ learned from the non-aligned samples ğ‘‹ ğµ

ğ‘›ğ‘™

ğ‘›ğ‘™

of

party

B.

Because

ğ‘…Ëœğ´

=

[ğ‘…Ëœğ‘ğ´;

ğ‘…Ëœğ´
ğ‘¢

],

the

estimation

of

ğ‘…Ëœğ´

involves

two

parts:

one

is

computing

ğ‘…Ëœğ´
ğ‘

cor-

responding

to

shared

representation

ğ‘…ğµ
ğ‘›ğ‘™,ğ‘

(section

5.2.1)

and

another

is

computing

ğ‘…Ëœğ´
ğ‘¢

corresponding

to unique representation ğ‘…ğµ (section 5.2.2).
ğ‘›ğ‘™,ğ‘¢

Fig. 4. Estimate missing representations ğ‘…Ëœğ´ corresponds to the non-aligned representations ğ‘…ğµ of
ğ‘›ğ‘™

party

B.

(a)

estimates

missing

representations

ğ‘…Ëœğ‘ğ´

corresponding

to

shared

representations

ğ‘…ğµ
ğ‘›ğ‘™,ğ‘

while

(b)

estimates

missing

representations

ğ‘…Ëœğ‘¢ğ´

corresponding

to

unique

representations

ğ‘…ğµ .
ğ‘›ğ‘™,ğ‘¢

Then,

ğ‘…Ëœğ´

can be obtained by concatenating ğ‘…Ëœğ‘ğ´ and ğ‘…Ëœğ‘¢ğ´.

5.2.1

Shared

Representation

Estimation.

Formula

(5)

estimates

the

shared

representation

ğ‘…Ëœğ´
ğ‘

corresponding to ğ‘…ğµ :
ğ‘›ğ‘™,ğ‘

ğ‘…ğµ âŠ— (ğ‘…ğ´)ğ‘‡

ğ‘…Ëœğ´
ğ‘

=

ğ‘”(ğ‘…ğµ

,

ğ´
ğ‘…,

ğ‘…ğ´)

ğ‘›ğ‘™,ğ‘ ğ‘ ğ‘

=

softmax(

ğ‘›ğ‘™,ğ‘ âˆš

ğ‘

)

âŠ—

ğ´
ğ‘…

ğ‘

(5)

ğ‘‘

where

ğ‘…ğ´
ğ‘

denotes

the

shared

representations

of

ğ‘‹ğ´

and

ğ‘‘

denotes

the

dimension

of

ğ‘…ğ´
ğ‘

.

The

estimation procedure can be divided into the following two steps and it is pictorially described in

Figure 4(a).

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:7

â€¢

Step

1:

compute

similarities

between

each

representation

vector

of

ğ‘…ğµ
ğ‘›ğ‘™,ğ‘

and

that

of

ğ‘…ğ´
ğ‘

.

The

result is a similarity matrix, denoted as ğ‘†ğ‘ ;

â€¢

Step

2:

compute

ğ‘…Ëœğ´
ğ‘

by

softmax( âˆšğ‘†ğ‘

)

âŠ—

ğ‘…ğ‘ğ´ .

ğ‘‘

5.2.2

Unique

Representation

Estimation.

Formula

(6)

estimates

the

unique

representation

ğ‘…Ëœğ´
ğ‘¢

corresponding to ğ‘…ğµ .
ğ‘›ğ‘™,ğ‘¢

ğ‘…ğµ âŠ— (ğ‘…ğµ )ğ‘‡

ğ‘…Ëœğ´
ğ‘¢

=

ğ‘”(ğ‘…ğµ

ğµ
,ğ‘…

ğ´
,ğ‘…

)

ğ‘›ğ‘™,ğ‘¢ ğ‘ğ‘™,ğ‘¢ ğ‘ğ‘™,ğ‘¢

=

softmax(

ğ‘›ğ‘™,ğ‘¢

âˆš

ğ‘ğ‘™,ğ‘¢

)

âŠ—

ğ´
ğ‘…

ğ‘ğ‘™,ğ‘¢

(6)

ğ‘‘

where ğ‘…ğ´ and ğ‘…ğµ denotes the unique representations of ğ‘‹ ğ´ and ğ‘‹ ğµ respectively, and ğ‘‘ denotes

ğ‘ğ‘™,ğ‘¢

ğ‘ğ‘™,ğ‘¢

ğ‘ğ‘™

ğ‘ğ‘™

the dimension of ğ‘…ğµ . The estimation procedure can be divided into the following two steps and it is
ğ‘ğ‘™,ğ‘¢

pictorially described in Figure 4(b).

â€¢ Step 1: compute similarities between each representation vector of ğ‘…ğµ and that of ğ‘…ğµ . This

ğ‘›ğ‘™,ğ‘¢

ğ‘ğ‘™,ğ‘¢

result is a similarity matrix, denoted as ğ‘†ğ‘¢;

â€¢

Step

2:

compute

ğ‘…Ëœğ´
ğ‘¢

by

softmax( âˆšğ‘†ğ‘¢ )
ğ‘‘

âŠ—

ğ‘…ğ´ .
ğ‘ğ‘™,ğ‘¢

Note

that

the

similarity

matrix

ğ‘†ğ‘¢

is

calculated

based

on

ğ‘…ğµ
ğ‘ğ‘™,ğ‘¢

in

the

representation

space

of

party

B

and

then

applied

to

ğ‘…ğ´
ğ‘ğ‘™,ğ‘¢

in

the

representation

space

of

party

A.

We

assume

ğ‘†ğ‘¢

calculated

based

on ğ‘…ğµ can be applied to ğ‘…ğ´ because ğ‘…ğµ and ğ‘…ğ´ are aligned indicating that they are different

ğ‘ğ‘™,ğ‘¢

ğ‘ğ‘™,ğ‘¢

ğ‘ğ‘™,ğ‘¢

ğ‘ğ‘™,ğ‘¢

views of the same samples.

Fig. 5. View of the training set ğœ’, each sample of which consists of the representation of a raw input feature and its corresponding label. (a) The view of ğœ’ with missing representations estimated. (b) The view of ğœ’ with cherry-picked pseudo-labeled samples.

We

obtain

estimated

representations

ğ‘…Ëœğ´

=

[ğ‘…Ëœğ´
ğ‘

;

ğ‘…Ëœğ´]
ğ‘¢

corresponds

to

the

non-aligned

representation

ğ‘…ğµ
ğ‘›ğ‘™

of

party

B.

Following

the

same

logic,

we

estimate

the

missing

representations

ğ‘…Ëœğµ

=

[ğ‘…Ëœğµ
ğ‘

;

ğ‘…Ëœğµ
ğ‘¢

]

corresponds

to

non-aligned

representation

ğ‘…ğ´
ğ‘›ğ‘™

of

party

A,

where

ğ‘…Ëœğµ
ğ‘

=

ğ‘”(ğ‘…ğ´ , ğ‘…ğµ, ğ‘…ğµ))
ğ‘›ğ‘™,ğ‘ ğ‘ ğ‘

is

shared

representations

and

ğ‘…Ëœğµ
ğ‘¢

=

ğ‘”(ğ‘…ğ´ , ğ‘…ğ´ , ğ‘…ğµ )
ğ‘›ğ‘™,ğ‘¢ ğ‘ğ‘™,ğ‘¢ ğ‘ğ‘™,ğ‘¢

is

unique

representations.

As

a

result,

we

obtain

a

expanded training set ğœ’ with all missing representations estimated, as depicted in Figure 5(a).

However, samples {ğ‘…ğµ , ğ‘…Ëœğ´} in ğœ’ are unlabeled. In section 5.3, we will elaborate on the procedure of
ğ‘›ğ‘™
estimating pseudo-labels for these unlabeled samples.

To force the estimated representations to approximate the representations learned from input raw

features, we add following two loss terms:

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:8

Kang, et al.

ğ´
ğ¿diff

(ğ‘…Ëœğ´
ğ‘ğ‘™

,

ğ´
ğ‘…
ğ‘ğ‘™

)

=

1 ğ‘›ğ‘ğ‘™

âˆ‘ï¸

ğ‘…Ëœğ´,ğ‘–

âˆ’

ğ´,ğ‘–
ğ‘…

ğ‘ğ‘™ ğ‘ğ‘™

2 ğ¹

(7)

ğ‘–

ğµ
ğ¿diff

(ğ‘…Ëœğµ
ğ‘ğ‘™

,

ğµ
ğ‘…
ğ‘ğ‘™

)

=

1 ğ‘›ğ‘ğ‘™

âˆ‘ï¸
ğ‘–

ğ‘…Ëœğµ,ğ‘–

âˆ’

ğµ,ğ‘–
ğ‘…

ğ‘ğ‘™ ğ‘ğ‘™

2
,
ğ¹

(8)

where ğ‘…Ëœğ‘ is the estimated representations of aligned samples in party ğ‘ âˆˆ {ğ´, ğµ} and it should be

ğ‘ğ‘™

as

close

to

ğ‘
ğ‘…

as possible.

ğ‘ğ‘™

5.3 Pseudo-Label Prediction

As discussed in section 5.2, samples {ğ‘…ğµ , ğ‘…Ëœğ´} in training set ğœ’ are unlabeled. For each unlabeled
ğ‘›ğ‘™

sample indexed by ğ‘–, we apply the three trained softmax classifiers ğ‘“ ğ´, ğ‘“ ğµ and ğ‘“ ğ´ğµ that take as input

ğ‘…Ëœğ´,ğ‘– ,

ğµ,ğ‘–
ğ‘…

and

{ğ‘… ğµ,ğ‘– ,

ğ‘…Ëœğ´,ğ‘–

}

respectively

to

produce

three

candidate

pseudo-labels

(Algorithm

1,

line

8).

ğ‘›ğ‘™

ğ‘›ğ‘™

Only when all the three candidate pseudo-labels are equal and their probabilities are higher than a

predefined threshold ğ‘¡ do we retain sample ğ‘– with the estimated pseudo-label (Algorithm 1, line 9).

Otherwise, we discard sample ğ‘– for the current iteration of training.

As a result, all samples in the training set ğœ’ have labels, as depicted in Figure 5(b). We further

define the training set ğœ’ = {ğ‘…ğµ, ğ‘…ğ´, ğ‘Œ ğ´}, where ğ‘…ğ´ denotes all samples of party A including estimated

ones, ğ‘Œ ğ´ denotes labels of ğ‘…ğ´, and ğ‘…ğµ denotes samples of party B that correspond to ğ‘…ğ´.

5.4 Cross-View Training
Inspired by cross-view learning [5] to share representations across models and improve modelsâ€™ representation learning, we train three softmax classifiers ğ‘“ ğ´, ğ‘“ ğµ and ğ‘“ ğ´ğµ jointly and each classifier takes as input a different view of the training dateset ğœ’ (Figure 5(b)) and outputs estimated class distributions.
We define the input to classifiers ğ‘“ ğ´, ğ‘“ ğµ and ğ‘“ ğ´ğµ as ğœ’ğ´ = {ğ‘…ğ´, ğ‘Œ ğ´}, ğœ’ğµ = {ğ‘…ğµ, ğ‘Œ ğ´}, and ğœ’ = {ğ‘…ğµ, ğ‘…ğ´, ğ‘Œ ğ´}, respectively. Then, the loss functions for the three classifiers are defined as follows:

ğ´
ğ¿

(

ğ´
ğœ’

)

ğ‘ğ‘’

=

1

âˆ‘ï¸ ğ¿ğ‘ğ‘’ ( ğ‘“ ğ´ (ğ‘…ğ´,ğ‘– ), ğ‘Œ ğ´,ğ‘– )

(9)

ğ‘›

ğ‘–

ğµ
ğ¿

(ğœ’ğµ)

ğ‘ğ‘’

=

1

âˆ‘ï¸ ğ¿ğ‘ğ‘’ ( ğ‘“ ğµ (ğ‘…ğµ,ğ‘– ), ğ‘Œ ğ´,ğ‘– )

(10)

ğ‘›

ğ‘–

ğ‘“ ğ‘’ğ‘‘
ğ¿ğ‘ğ‘’

(

ğœ’

)

=

1

âˆ‘ï¸

ğ¿ğ‘ğ‘’

(

ğ‘“

ğ´ğµ

(

{ğ‘…ğµ,ğ‘–

,

ğ´,ğ‘–
ğ‘…

}),

ğ‘Œ

ğ´,ğ‘–

)

(11)

ğ‘›

ğ‘–

where ğ‘› is the number of samples in ğœ’. Formulas (9), (10) and (11) compute the cross-entropy

between estimated class distributions and ground-truth labels. Finally, we have the overall loss

defined as Formula (12):

ğ¿ğ‘œğ‘ ğ‘—

=

ğ‘“ ğ‘’ğ‘‘
ğ¿ğ‘ğ‘’

+

ğ´
ğ¿

ğ‘ğ‘’

+

ğµ
ğ¿

ğ‘ğ‘’

+

ğ´ğµ
ğœ†1ğ¿diff

+

ğ´
ğœ†2ğ¿diff

+

ğµ
ğœ†3ğ¿diff

+

ğ´
ğœ†4ğ¿sim

+

ğµ
ğœ†5ğ¿sim

(12)

where ğœ†ğ‘  are loss weights. Algorithm 1 gives a full version of the FedCVT algorithm.

5.5 Security Analysis
FedCVT does not require participating parties to share their original data and models (including architecture and parameters) but only intermediate representations and gradients. The intermediate representations are results transformed from original features through deep neural networks with multiple layers of non-linear transformations. There is little chance for a malicious party to reconstruct the victim partyâ€™s original features from exposed intermediate representations without knowing the

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:9

Algorithm 1 FedCVT algorithm

1: Input:

Datasets ğ·ğ´ and ğ·ğµ;

Batch

index

sets

ğ‘‡ğ‘ğ‘™ ,

ğ‘‡ğ´
ğ‘›ğ‘™

and

ğ‘‡ğµ
ğ‘›ğ‘™

of

ğ·ğ‘ğ‘™ ,

ğ·ğ´
ğ‘›ğ‘™

and

ğ·ğµ ,
ğ‘›ğ‘™

respectively.

Neural

networks

â„ğµ, â„ğµ,
ğ‘¢ğ‘

â„ğ´
ğ‘¢

and

â„ğ‘ğ´ ;

Representation estimators ğ‘”;

Softmax classifiers ğ‘“ ğ´, ğ‘“ ğµ and ğ‘“ ğ´ğµ

Class probability threshold ğ‘¡ and the epoch number ğ¾

2: for ğ‘’ = 1, 2, ..., ğ¾ do

3:

for ğ‘1, ğ‘2, ğ‘3

in

ğ‘‡ğ‘ğ‘™

,

ğ‘‡ ğ´,
ğ‘›ğ‘™

ğ‘‡ğµ
ğ‘›ğ‘™

do

4: Select mini-batches:

{ğ‘‹ ğµ,ğ‘1,
ğ‘ğ‘™

ğ‘‹ ğ´,ğ‘1, ğ‘Œ ğ´,ğ‘1
ğ‘ğ‘™ ğ‘ğ‘™

}

from

ğ·ğ‘ğ‘™ ;

{ğ‘‹ ğ´,ğ‘2, ğ‘Œ ğ´,ğ‘2 } from ğ·ğ´ ;

ğ‘›ğ‘™ ğ‘›ğ‘™

ğ‘›ğ‘™

{ğ‘‹ ğµ,ğ‘3 } from ğ·ğµ ;

ğ‘›ğ‘™

ğ‘›ğ‘™

5:

Learn

ğ‘…ğµ,ğ‘1

,

ğµ,ğ‘
ğ‘…

3

,

ğ‘…ğ´,ğ‘1

,

ğ‘…ğ´,ğ‘2

ğ‘ğ‘™ ğ‘›ğ‘™ ğ‘ğ‘™ ğ‘›ğ‘™

from

ğ‘‹ ğµ,ğ‘1, ğ‘‹ ğµ,ğ‘3, ğ‘‹ ğ´,ğ‘1, ğ‘‹ ğ´,ğ‘2
ğ‘ğ‘™ ğ‘›ğ‘™ ğ‘ğ‘™ ğ‘›ğ‘™

through

â„ğµ
ğ‘¢

,

â„ğµ
ğ‘

,

â„ğ´,
ğ‘¢

â„ğ´
ğ‘

;

6:

Estimate missing representations ğ‘…Ëœğµ,ğ‘2 and ğ‘…Ëœğ´,ğ‘3 through ğ‘”;

7: Form training set ğœ’ of samples consisting of representations and corresponding labels;

8:

Estimate candidate pseudo-labels for unlabeled samples {ğ‘…ğµ,ğ‘3, ğ‘…Ëœğ´,ğ‘3 } of ğœ’:

ğ‘›ğ‘™

ğ‘ŒËœ1ğ‘3 = ğ‘“ ğ´ (ğ‘…Ëœğ´,ğ‘3 );

ğ‘ŒËœ2ğ‘3

=

ğ‘“ ğµ (ğ‘…ğµ,ğ‘3 );
ğ‘›ğ‘™

ğ‘ŒËœ3ğ‘3

=

ğ‘“ ğ´ğµ ( [ğ‘…ğµ,ğ‘3 ; ğ‘…Ëœğ´,ğ‘3 ]);
ğ‘›ğ‘™

9:

Retain

samples

of

{ğ‘… ğµ,ğ‘3 ,
ğ‘›ğ‘™

ğ‘…Ëœ ğ´,ğ‘3 ,

(ğ‘ŒËœ1ğ‘3 ,

ğ‘ŒËœ2ğ‘3 ,

ğ‘ŒËœ3ğ‘3 )}

that

satisfy

the

rule

defined

in

section

5.3;

10:

Feed ğœ’ğ´, ğœ’ğµ and ğœ’ to classifiers ğ‘“ ğ´, ğ‘“ ğµ and ğ‘“ ğ´ğµ respectively for cross-view training;

11:

Compute loss: ğ¿ğ‘œğ‘ ğ‘— = ğ¿ğ‘“ ğ‘’ğ‘‘ + ğ¿ğ´ + ğ¿ğµ + ğœ†1ğ¿dğ´iğµff + ğœ†2ğ¿dğ´iff + ğœ†3ğ¿dğµiff + ğœ†4ğ¿sğ´im + ğœ†5ğ¿sğµim;

12:

Compute gradients âˆ‡ğ¿ğ‘œğ‘ ğ‘— and update models.

13: end for

14: end for

victimâ€™s model [13]. In recent years, there has been a series of works on investigating data leakage through gradients [10, 16, 41, 42]. However, those research works focus on horizontal federated learning where the server is malicious. They assume that the malicious server can access the victim clientâ€™s model parameters and gradients. These assumptions are not held in the FedCVT setting. Research works [23, 27] investigate privacy leakage issues in the VFL setting and demonstrate that the malicious party could recover party Aâ€™s labels through gradients. To provide a strong privacy guarantee, FATE design and implement a privacy-preserving VFL-DNN (Deep Neural Network) framework that can efficiently perform encryption in both forward and backward stages of the VFL training procedure to prevent intermediate representations and gradients from being exposed. For experimental convenience, we did not implement FedCVT with FATE. However, FedCVT is compatible with the VFL-DNN framework and can be migrated to FATE straightforwardly.
6 EXPERIMENTAL EVALUATION We report experiments conducted on three public datasets, which are: 1) NUS-WIDE dataset [4]; 2) Vehicle dataset [7]; 3) CIFAR-10 [20] to validate our proposed FedCVT approach.
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:10

Kang, et al.

6.1 Hyperparameters
FedCVT-VFL combines multiple techniques to improve the performance of VFL and thus it introduces various hyperparameters - specifically, the loss weights ğœ†ğ‘  , the label probability threshold ğ‘¡, the sharpening temperature ğ‘‡ for estimating representation and learning rate. For experiments on NUS-WIDE, we set ğœ†1,2,3,4,5 = 0.1, ğ‘¡=0.5, ğ‘‡ =0.5 and learning rate = 0.005. For Vehicle, we set ğœ†1,2,3,4,5 = 0.1, ğ‘¡ =0.5, ğ‘‡ =1.0 and learning rate = 0.005. For CIFAR-10, we set ğœ†1,4,5 = 0.1, ğœ†2,3 = 0.01, ğ‘¡=0.6, ğ‘‡ =0.1 and learning rate = 0.001. For all experiments, we use Adam [19] optimizer.
6.2 Baseline Models
We consider three baseline models: (1) party Aâ€™s local model ğ‘“ ğ´, (2) vanilla-VFL and (3) FTL [26]. Party Aâ€™s local model is trained on all local samples of party A, while vanilla-VFL is trained on aligned samples between party A and party B (without using non-aligned samples). FTL leverages all samples of party A in addition to aligned samples to help party B build a classifier model. We consider party Aâ€™s local model as a baseline model because it is worth examining whether VFL performs better than the local model when the number of aligned samples is limited. Vanilla-VFL and FTL are adopted to verify whether our FedCVT-VFL approach is effective. Note that FTL aims to build a model only for party B, and thus it does not have a federated model that takes input from both parties. To ensure a fair comparison, we add a federated model on top of local models of FTL and denote this modified FTL as mFTL.
6.3 Experimental Results
6.3.1 NUS-WIDE. The NUS-WIDE dataset consists of 634 low-level image features extracted from Flickr images and their associated 1000 textual tags as well as 81 ground truth labels. In this work, we consider solving a 10-label classification problem with a data federation formed between party A and party B. In our setting, each party has 56000 training samples, 8000 aligned validation samples, and 10000 aligned testing samples. Each party utilizes two local neural networks that each has one hidden layer with 96 units to learn representations from local input samples. Then, each party feeds the learned representations into its local softmax classifier ğ‘“ ğ‘, ğ‘ âˆˆ {ğ´, ğµ} with the dimension of 192 (96x2) and the federated softmax layer ğ‘“ ğ´ğµ with the dimension of 384 (96x2x2), respectively, for federated training.
On NUS-WIDE, we run experiments with two scenarios. In the first scenario, denoted as scenario1, we put 634 image features on party A and 1000 textual features on party B. The second scenario, denoted as scenario-2, is the other way around. In both scenarios, party A owns the labels. We consider these two scenarios because image and text are different in modality and have different discriminative power, and we want to investigate how these differences would affect the performance of FedCVT-VFL.
Table 1 shows the accuracy comparison between FedCVT-VFL and baselines with a varying number of aligned samples from 250 to 8000. FedCVT-VFL outperforms mFTL [26] noticeably and outperforms vanilla-VFL by a large margin (see Figure 6) for both scenarios. For example, with 250 aligned samples, FedCVT-VFL improves 13.4 and 17.0 points in accuracy over vanilla-VFL in scenario-1 and scenario-2, respectively. With 8000 aligned samples, FedCVT-VFL enhances the accuracy by 1.8 and 3.8 points, respectively. In addition, FedCVT-VFL outperforms the local model with only a small amount of aligned samples. For example, FedCVT-VFL outperforms the local model with only 250 aligned samples in scenario-1 and with 500 aligned samples in scenario-2.
As shown in Table 1, the local model of scenario-2 significantly outperforms that of scenario-1 (by 11.4 points), which demonstrates that text features have much more discriminative power than image features on the NUS-WIDE dataset. Therefore, in scenario-2, leveraging complementary

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:11

Model

scenario-1: Party A with image

250

500

1000

2000

4000

8000

Local Model 58.92

58.92

58.92

58.92

58.92

58.92

Vanilla-VFL 51.79Â±0.21 56.58Â±0.28 64.23Â±0.09 68.05Â±0.36 71.07Â±0.14 73.69Â±0.12

mFTL

62.43Â±0.46 65.07Â±0.29 68.49Â±0.36 71.19Â±0.26 73.10Â±0.27 74.32Â±0.36

FedCVT-VFL 65.16Â±0.21 68.37Â±0.22 68.93Â±0.20 71.75Â±0.14 74.27Â±0.24 75.45Â±0.11

Î” FedCVT-VFL â†‘ 13.37

â†‘ 11.79

â†‘ 4.7

â†‘ 3.7

â†‘ 3.2

â†‘ 1.76

scenario-2: Party A with text

Model

250

500

1000

2000

4000

8000

Local Model 70.31

70.31

70.31

70.31

70.31

70.31

Vanilla-VFL 51.79Â±0.21 56.58Â±0.28 64.23Â±0.09 68.05Â±0.36 71.07Â±0.14 73.69Â±0.12

mFTL

63.95Â±0.26 67.17Â±0.28 70.34Â±0.19 72.59Â±0.20 73.79Â±0.18 74.51Â±0.18

FedCVT-VFL 68.76Â±0.08 71.51Â±0.27 72.77Â±0.21 73.95Â±0.06 75.92Â±0.24 77.45Â±0.06

Î” FedCVT-VFL â†‘ 16.97

â†‘ 14.93

â†‘ 8.54

â†‘ 5.9

â†‘ 4.85

â†‘ 3.76

Table 1. Test accuracy (%) comparison of FedCVT-VFL to party Aâ€™s local model, mFTL and vanilla-VFL

on NUS-WIDE for a varying number of aligned samples. Note that the local model is trained based on 56000 local samples of party A and thus it is not affected by the number of aligned samples. Î” is the performance gain of FedCVT-VFL compared to Vanilla-VFL.

(a)

(b)

Fig. 6. Test accuracy (%) comparison of FedCVT-VFL model to vanilla-VFL model on NUS-WIDE for a varying number of aligned samples. Exact numbers are provided in Table 1. (a) shows results when party A holds image data, while (b) shows results when party A hold text data.

image features contributes little to vanilla-VFL when the number of aligned samples is small. This partially explains why vanilla-VFL models do not outperform the local model (solid-blue line v.s. red line in Figure 6(b)) in scenario-2 when the size of aligned samples is not sufficiently large enough (i.e., â‰¤ 2000). While in scenario-1, party A benefits significantly from leveraging complementary text features of party B through VFL (solid-blue line v.s. red line in Figure 6(a)). This suggests that, in practice, when the size of the aligned samples is limited, the party owning labels may be better off using its local model (with much more training samples) if it has much stronger discriminative features than other parties. Otherwise, it may benefit from leveraging the features of other parties.
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:12

Kang, et al.

6.3.2 Vehicle. The vehicle2 [7] dataset is for classifying the types of moving vehicles in a distributed sensor network. There are 3 types of vehicles. Each sample in this dataset has 100 features, the first 50 of which are acoustic features, while the rest are seismic features. Thus, it is natural to split the two modalities of features into two parties. In our setting, each party has 40000 training samples, 5000 validation samples, and 5000 testing samples. Each party utilizes two local neural networks that each has one hidden layer with 32 units to learn representations from local input samples. Then, each party feeds the learned representations into its local softmax classifier ğ‘“ ğ‘, ğ‘ âˆˆ {ğ´, ğµ} with the dimension of 64 (32x2) and the federated softmax layer ğ‘“ ğ´ğµ with the dimension of 128 (32x2x2), respectively, for federated training.
Similar to NUS-WIDE, we consider two scenarios on the Vehicle dataset. In scenario-1, we put acoustic features on party A. While in scenario-2, we put seismic features on party A. In both scenarios, party A owns the labels.

(a)

(b)

Fig. 7. Test accuracy (%) comparison of FedCVT-VFL model to vanilla-VFL on Vehicle dataset for a varying number of aligned samples. Exact numbers are provided in Table 2. (a) shows results when party A holds acoustic data, while (b) shows results when party A hold seismic data.

From Table 2 and Figure 7, we have the following observations. For both scenarios, FedCVT and vanilla-VFL outperform the local model by a large margin, even with only 250 aligned samples. This demonstrates that leveraging more complementary features brings great benefits for improving the accuracy of machine learning models on the Vehicle dataset. Compared to vanilla-VFL and mFTL, FedCVT achieves better performance with much less aligned samples. For example, in both scenarios, with 500 aligned samples, FedCVT reaches an accuracy comparable to the performance of vanilla-VFL and mFTL using 4000 aligned samples. It is worth noting from Figure 7 that mFTL becomes less accurate than vanilla-VFL when the number of aligned samples reaches 8000. We conjecture that this is because mFTL mainly leverages two partiesâ€™ shared representations that are learned based on aligned samples to boost performance, and thus it lost information unique to each party. With more aligned samples are added for training, more unique information would be lost.
6.3.3 CIFAR-10. The CIFAR-10 dataset consists of color images with shape of 32 Ã— 32 Ã— 3 in 10 classes. To simulate the two-party vertical federated learning setting, we partition each CIFAR-10 image with shape 32 Ã— 32 Ã— 3 vertically into two parts (each part has shape 32 Ã— 16 Ã— 3). Each party has 25000 training samples, 5000 aligned validation samples, and 10000 aligned testing samples. Each
2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:13

Model

scenario-1: Party A with acoustic features

250

500

1000

2000

4000

8000

Local Model 69.91

69.91

69.91

69.91

69.91

69.91

Vanilla-VFL 76.44Â±0.64 77.86Â±0.83 78.83Â±0.53 79.28Â±0.75 80.24Â±0.60 80.82Â±0.29

mFTL

76.74Â±0.19 79.07Â±0.17 79.22Â±0.18 79.49Â±0.23 80.01Â±0.21 80.49Â±0.20

FedCVT-VFL 79.82Â±0.27 80.42Â±0.28 80.58Â±0.07 80.86Â±0.09 81.05Â±0.17 81.14Â±0.06

Î” FedCVT-VFL â†‘ 3.38

â†‘ 2.56

â†‘ 1.75

â†‘ 1.58

â†‘ 0.81

â†‘ 0.32

scenario-2: Party A with seismic features

Model

250

500

1000

2000

4000

8000

Local Model 70.31

70.31

70.31

70.31

70.31

70.31

Vanilla-VFL 76.44Â±0.64 77.86Â±0.83 78.83Â±0.53 79.28Â±0.75 80.24Â±0.60 80.82Â±0.29

mFTL

77.35Â±0.39 78.62Â±0.35 79.16Â±0.18 80.21Â±0.19 80.45Â±0.23 80.59Â±0.17

FedCVT-VFL 79.67Â±0.35 80.13Â±0.25 80.44Â±0.16 80.67Â±0.16 80.90Â±0.15 81.02Â±0.05

Î” FedCVT-VFL â†‘ 3.23

â†‘ 2.27

â†‘ 1.61

â†‘ 1.39

â†‘ 0.66

â†‘ 0.20

Table 2. Test accuracy (%) comparison of FedCVT-VFL to party Aâ€™s local model, mFTL and vanilla-VFL

model on Vehicle dataset for a varying number of aligned samples. Note that the local model is trained

based on 40000 local samples of party A and thus it is not affected by the number of aligned samples. Î” is the performance gain of FedCVT-VFL compared to Vanilla-VFL.

party uses two local VGG-like CNN models to learn representations from input image samples. Then it feeds the learned representations into its local softmax classifier ğ‘“ ğ‘, ğ‘ âˆˆ {ğ´, ğµ} and the federated softmax classifier ğ‘“ ğ´ğµ, respectively, for jointly training. The VGG-like CNN model consists of 2x2 max-pooling layers, 3x3 convolutional layers with stride 1 and padding 1, and fully connected layers. The architecture is conv32-conv32-pool-conv64-conv64-pool-conv128-conv128-pool-fc64. Thus, the federated softmax classifier has the dimension of 256 (64x2x2). For CIFAR10, we do not swap image partitions between the two parties because they are cut from the same images, and thus we assume they have similar discriminative power.

Fig. 8. Test accuracy (%) comparison of FedCVT-VFL model to vanilla-VFL model on CIFAR10 for a varying number of aligned samples. Exact numbers are provided in Table 3.
As shown in Table 3, FedCVT-VFL significantly outperforms vanilla-VFL for CIFAR10 (dashedblue line v.s. solid-blue line in Figure 8). For example, FedCVT-VFL improves accuracy by 22.7 points with 500 aligned samples and 7.1 points with 10000 aligned samples compared with vanillaVFL. Figure 8 (solid-red line v.s. solid-blue line) depicts that the vanilla-VFL cannot beat the
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:14

Kang, et al.

Model

500

1000

2000

4000

8000

10000

Local Model 67.74

67.74

67.74

67.74

67.74

67.74

Vanilla-VFL 40.02Â±0.13 44.20Â±0.12 50.38Â±0.14 56.46Â±0.06 62.87Â±0.08 65.01Â±0.11

FedCVT-VFL 62.68Â±0.21 65.04Â±0.26 66.02Â±0.31 67.84Â±0.23 71.44Â±0.11 72.06Â±0.16

Î” FedCVT-VFL â†‘ 22.66

â†‘ 20.84

â†‘ 15.64

â†‘ 11.38

â†‘ 8.57

â†‘ 7.05

Table 3. Test accuracy (%) comparison of FedCVT-VFL model to vanilla-VFL model on CIFAR-10

for a varying number of aligned samples. Note that the local model is trained based on 25000 local samples of party A and thus it is not affected by the number of aligned samples. Î” is the performance gain of FedCVT-VFL compared to Vanilla-VFL.

local model with even 10000 aligned samples (over 1/3 of local samples). FedCVT-VFL boosts the performance of vanilla-VFL so that it outperforms the local model using 4000 aligned samples (less than 1/6 of local samples). This manifests that FedCVT equips VFL with an effective way of leveraging unlabeled data to improving model performance.
6.4 Ablation Study
FedCVT exploits several techniques to improve performance. It leverages unique and shared representations learned from raw features, uses both types of representations to estimate missing representations, and then performs cross-view training with additional pseudo-labeled samples. In this section, we study how each of those techniques contributes to the performance of FedCVT. Specifically, we measure effects of:
â€¢ Use both unique and shared representations without representation estimation and cross-view training.
â€¢ Use only unique representations, only shared representations, or both unique and shared representations for estimating missing representations but without cross-view training.
â€¢ All techniques applied.

Ablation

500 4000

(1) Vanilla VFL (equals to using only unique representations w/o RE and CVT) (2) FedCVT-VFL uses unique and shared representations w/o RE and CVT (3) FedCVT-VFL uses only unique representations with RE w/o CVT (4) FedCVT-VFL uses only shared representations with RE w/o CVT (5) FedCVT-VFL uses unique and shared representations with RE w/o CVT (6) FedCVT-VFL with all techniques applied

56.58 63.10 66.12 65.68 67.66 68.37

71.07 72.70 73.31 72.89 73.69 74.27

Table 4. Ablation study results. All values are accuracy (%) on NUS-WIDE (scenario 1) with 500 or 4000 aligned samples.

We conducted the ablation study on NUS-WIDE (scenario-1) with 500 and 4000 aligned samples. For conciseness, we denote representation estimation as RE and cross-view training as CVT. The results are reported in Table 4. It shows that every technique applied to FedCVT-VFL contributes to the performance boost. We obtain the most dramatic performance gain when applying representation estimation. For example, comparing (5) and (1), FedCVT-VFL outperforms vanilla-VFL by 11% and 2.6%, respectively, using 500 samples and 4000 samples. Comparing (6) and (5), cross-view training also helps boost the performance of FedCVT-VFL.
J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:15

7 CONCLUSION
We propose Federated Cross-View Training (FedCVT), a semi-supervised learning approach that improves the performance of VFL using limited aligned samples. FedCVT leverages representation estimation and pseudo-labels prediction to expand the training set and trains three classifiers jointly through vertical federated learning to improve modelsâ€™ representation learning. FedCVT-VFL significantly enhances the performance of vanilla VFL. The ablation study shows that each technique used in FedCVT-VFL contributes to the final performance boost.
8 ACKNOWLEDGMENT
This work was partially supported by the National Key Research and Development Program of China under Grant No. 2018AAA0101100.
REFERENCES
[1] David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. 2019. MixMatch: A Holistic Approach to Semi-Supervised Learning. CoRR abs/1905.02249 (2019). arXiv:1905.02249 http://arxiv.org/ abs/1905.02249
[2] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain Separation Networks. In Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Eds.). Curran Associates, Inc., 343â€“351. http://papers.nips.cc/paper/6254-domainseparation-networks.pdf
[3] Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang. 2021. SecureBoost: A Lossless Federated Learning Framework. IEEE Intelligent Systems 01 (may 2021), 1â€“1. https://doi.org/10.1109/MIS.2021.3082561
[4] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. 2009. NUS-WIDE: A real-world web image database from National University of Singapore. In CIVR.
[5] Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc Le. 2018. Semi-Supervised Sequence Modeling with Cross-View Training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 1914â€“1925. https://doi.org/10.18653/v1/D18-1217
[6] DLA Piper. 2018. Data protection laws of the world: Full handbook. [7] Marco F Duarte and Yu Hen Hu. 2004. Vehicle classification in distributed sensor networks. J. Parallel and Distrib.
Comput. 64, 7 (2004), 826â€“838. https://doi.org/10.1016/j.jpdc.2004.03.020 Computing and Communication in Distributed Sensor Networks. [8] AdriÃ  GascÃ³n, Phillipp Schoppmann, Borja Balle, Mariana Raykova, Jack Doerner, Samee Zahur, and David Evans. 2016. Secure Linear Regression on Vertically Partitioned Datasets. IACR Cryptology ePrint Archive (2016), 892. [9] GDPR. 2018. General Data Protection Regulation. [10] Jonas Geiping, Hartmut Bauermeister, Hannah DrÃ¶ge, and Michael Moeller. 2020. Inverting Gradients - How easy is it to break privacy in federated learning?. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 16937â€“16947. https: //proceedings.neurips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf [11] Xuan Gong, Abhishek Sharma, Srikrishna Karanam, Ziyan Wu, Terrence Chen, David Doermann, and Arun Innanje. 2021. Ensemble Attention Distillation for Privacy-Preserving Federated Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 15076â€“15086. [12] Yves Grandvalet and Yoshua Bengio. 2004. Semi-Supervised Learning by Entropy Minimization. In Proceedings of the 17th International Conference on Neural Information Processing Systems (Vancouver, British Columbia, Canada) (NIPSâ€™04). MIT Press, Cambridge, MA, USA, 529â€“536. [13] Otkrist Gupta and Ramesh Raskar. 2018. Distributed learning of deep neural network over multiple agents. CoRR abs/1810.06060 (2018). arXiv:1810.06060 http://arxiv.org/abs/1810.06060 [14] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. 2017. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. CoRR abs/1711.10677 (2017). [15] Dong hyun Lee. 2013. Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks. In ICML Workshop on Challenges in Representation Learning. [16] Jiwnoo Jeon, jaechang Kim, Kangwook Lee, Sewoong Oh, and Jungseul Ok. 2021. Gradient Inversion with Generative Image Prior. In Advances in Neural Information Processing Systems, Vol. 34. Curran Associates, Inc. https://papers. nips.cc/paper/2021/file/fa84632d742f2729dc32ce8cb5d49733-Paper.pdf

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

1:16

Kang, et al.

[17] Peter Kairouz, H. Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. Dâ€™Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, AdriÃ  GascÃ³n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub KonecnÃ½, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, TancrÃ¨de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ã–zgÃ¼r, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian TramÃ¨r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. 2021. Advances and Open Problems in Federated Learning. Foundations and TrendsÂ® in Machine Learning 14, 1â€“2 (2021), 1â€“210. https://doi.org/10.1561/2200000083
[18] Yan Kang, Yang Liu, Yuezhou Wu, Guoqiang Ma, and Qiang Yang. 2021. Privacy-preserving Federated Adversarial Domain Adaption over Feature Groups for Interpretability. In International Workshop on Federated and Transfer Learning for Data Sparsity and Confidentiality in Conjunction with IJCAI. arXiv:2111.10934 [cs.LG]
[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980
[20] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical Report. University of Toronto. [21] Samuli Laine and Timo Aila. 2017. Temporal Ensembling for Semi-Supervised Learning.. In ICLR (Poster). OpenRe-
view.net. http://dblp.uni-trier.de/db/conf/iclr/iclr2017.html#LaineA17 [22] Daliang Li and Junpu Wang. 2019. FedMD: Heterogenous Federated Learning via Model Distillation. In NeurIPS
Workshop on Federated Learning for Data Privacy and Confidentiality. arXiv:1910.03581 [cs.LG] [23] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and Chong Wang. 2021.
Label Leakage and Protection in Two-party Split Learning. arXiv:2102.08504 [cs.LG] [24] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. 2021. FedDG: Federated Domain Generalization on
Medical Image Segmentation via Episodic Learning in Continuous Frequency Space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1013â€“1023. [25] Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang. 2021. FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection. Journal of Machine Learning Research 22, 226 (2021), 1â€“6. http://jmlr.org/papers/v22/ 20-815.html [26] Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang. 2020. A Secure Federated Transfer Learning Framework. IEEE Intelligent Systems 35, 4 (2020), 70â€“82. https://doi.org/10.1109/MIS.2020.2988525 [27] Yang Liu, Zhihao Yi, and Tianjian Chen. 2020. Backdoor Attacks and Defenses in Feature-partitioned Collaborative Learning. In ICML Workshop. arXiv:2007.03608 [cs.LG] [28] H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise AgÃ¼era y Arcas. 2016. Federated Learning of Deep Networks using Model Averaging. CoRR abs/1602.05629 (2016). arXiv:1602.05629 http://arxiv.org/abs/1602.05629 [29] Payman Mohassel and Yupeng Zhang. 2017. SecureML: A System for Scalable Privacy-Preserving Machine Learning. IACR Cryptology ePrint Archive (2017), 396. [30] Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini, Guillaume Smith, and Brian Thorne. 2018. Entity Resolution and Federated Learning get a Federated Resolution. CoRR abs/1803.04035 (2018). arXiv:1803.04035 http://arxiv.org/abs/1803.04035 [31] Maxime Oquab, Ivan Laptev, Josef Sivic, Maxime Oquab, Ivan Laptev, Josef Sivic Learning, Transferring Midlevel, Maxime Oquab, Leon Bottou Ivan Laptev, and Josef Sivic. 2014. Learning and transferring mid-level image representations using convolutional neural networks. In In CVPR. [32] Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain Sentiment Classification via Spectral Feature Alignment. In WWW. 751â€“760. [33] Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. 2019. Federated Adversarial Domain Adaptation. arXiv:1911.02054 [cs.CV] [34] Daniel Peterson, Pallika Kanani, and Virendra J. Marathe. 2019. Private Federated Learning with Domain Adaptation. In NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality. arXiv:1912.06733 [cs.LG] [35] R L Rivest, L Adleman, and M L Dertouzos. 1978. On Data Banks and Privacy Homomorphisms. Foundations of Secure Computation (1978), 169â€“179. [36] Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. V.

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

FedCVT: Semi-Supervised Vertical Federated Learning with Cross-View Training

1:17

Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf [38] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. 2018. Split learning for health: Distributed deep learning without sharing raw patient data. CoRR abs/1812.00564 (2018). arXiv:1812.00564 http://arxiv.org/abs/ 1812.00564 [39] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine Learning: Concept and Applications. ACM Transactions on Intelligent Systems and Technology 10, 2, Article 12 (2019), 12:1â€“12:19 pages. [40] Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu. 2019. Federated Learning. Morgan & Claypool. [41] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. idlg: Improved deep leakage from gradients. arXiv preprint arXiv:2001.02610 (2020). [42] Ligeng Zhu and Song Han. 2020. Deep leakage from gradients. In Federated Learning. Springer, 17â€“31.

J. ACM, Vol. 1, No. 1, Article 1. Publication date: January 2022.

