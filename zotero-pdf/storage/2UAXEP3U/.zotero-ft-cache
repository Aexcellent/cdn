分类号 UDC

TP391

密级 编号

10486

博士学位论文
面向小样本问题的主动学习理论及应用 研究
研 究 生 姓 名: 王增茂 指导教师姓名、职称: 杜博 教授、张良培 教授 学 科、 专 业 名 称: 计算机应用技术 研 究 方 向: 机器学习和图像处理
二〇一九年五月

A Dissertation Submitted to Wuhan University for the degree of
Doctor of Philosophy
A Study on the Theory and Application of Active Learning for Small Size Samples
By
Zengmao Wang
Supervised by Professor Bo Du and Professor Liangpei Zhang
School of Computer Science May, 2019

论文原创性声明
本人郑重声明: 所呈交的学位论文, 是本人在导师指导下, 独立进行 研究工作所取得的研究成果. 除文中已经标明引用的内容外, 本论文不包 含任何其他个人或集体已发表或撰写的研究成果. 对本文的研究做出贡献 的个人和集体, 均已在文中以明确方式标明. 本声明的法律结果由本人承 担.

学位论文作者 (签名):

年

月

日

论文创新点
本文针对标记数据稀缺的小样本问题，系统的研究了小样本问题下主动学习技术 面临的问题和解决办法，旨在在最少人工标注数据条件下，建立强泛化能力的分类模 型，取得的创新成果主要如下： (1) 本文提出了一种结合不确定性和代表性的启发式主动学习框架，并基于提出框架
设计了一个主动学习算法。在主动学习中，不确定性准则和代表性准则是主动学 习算法进行设计时的核心标准。为此，一些研究基于稀疏表达，半监督假设等形 式将不确定性和代表性进行结合建立联合查询准则。然而，这些准则的成立需要 数据满足其条件假设，缺乏算法设计和应用的灵活性。因此，建立一个启发式的 灵活主动学习框架，可以为主动学习算法的设计进行理想查询提供理论框架依 据。提出的框架中代表性衡量主要是基于双样本测试理论，而不确定性可以根据 具体的分类模型进行设计，具有很强的灵活性。同时，基于提出框架，提出了一 种主动学习算法，验证了框架和算法的有效性。 (2) 本文提出了一种鲁棒的多标签主动学习方法，将多标签学习和主动学习相结合， 根据多标签数据的特点，设计主动学习的不确定性准则和代表性准则，实现主动 学习在多标签学习中最具信息量样本的查询。由于多标签数据中存在标签相关 性，使得多标签中异常低的相关性标签对分类模型和样本间相似性衡量会产生巨 大影响，该方法的主要优势是采用了最大相关熵准则作为损失函数，最大相关熵 准则对异常点具有很强的鲁棒性，利用最大相关熵准则的有界性，可以有效抑制 低相关性标签的影响。在对目标函数最大化过程中，当出现低相关性标签的错误 预测时，其函数损失值最小趋近于零, 而对于分类准确的强相关标签其函数损失 值趋近于最大。同时，基于最大相关熵准则将标签空间的相似性引入主动学习的 代表性衡量，缓解异常低相关标签造成的基于特征衡量相似性不准问题。通过判 别性和代表性的增强，实现多标签中主动学习准则的准确建模。 (3) 本文提出了一种融合判别性和代表性的半监督主动学习方法。该方法的优势在于 通过半监督学习对未标记集中的判别性和代表性样本进行可靠性伪标签的标记， 而不能进行伪标签分配的样本则作为主动学习的候选样本集，事实上，候选样本 集中的样本既具有代表性又具有不确定性，通过主动学习的不确定性准则就可以 查询到既具有判别性又具有代表性的样本，不仅保证了主动学习的效率同时又保 证了其的性能。反过来，主动学习对判别模型的增强，也使得伪标签的可靠性逐 步提升。方法中通过分类模型获得未标记样本的分类结果，同时对未标记样本和 标记样本进行聚类，依据近邻原则，利用标记样本获取部分未标记样本的聚类标

签。根据聚类结果和分类结果的一致性，获取可靠性的伪标签，而其余不能分配 伪标签的样本，则是聚类结果和分类结果都不能确定的样本，因此既含有不确定 性又具有代表性。 (4) 本文提出了一种领域适配协同的多域主动学习方法。该方法将领域适配中的实例 权重法和主动学习的不确定性与代表性准则进行结合，实现领域适配和主动学习 在统一框架下的学习。方法中通过最大均值差异最小化加权源域样本与少量标记 样本和目标域中未标记样本之间的边缘概率分布差异，同时为了实现目标域和源 域匹配中相应类别的判别性匹配，利用目标域上的分类模型对加权后的源域样本 进行结构风险最小化，使加权后的源域样本更加直接适用于目标域的分类任务。 在主动学习查询过程中，其不断改变目标域样本的分布，使得加权后的源域样本 和目标域中的标记样本一起与目标域未标记样本具有更加相似的分布。同时，源 域样本的不断改变，也促使主动学习查询能够最大化标记集信息的样本，实现领 域适配和主动学习相辅相成的学习，达到跨领域分类问题中主动学习性能提升的 目的。

目录

要

V

ABSTRACT

VII

1论

1

1.1 研究背景与意义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 国内外研究现状 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2.1 主动学习准则 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2.1.1 不确定性准则 . . . . . . . . . . . . . . . . . . . . . . . 4

1.2.1.2 代表性准则 . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.2.1.3 不确定性准则和代表性准则结合 . . . . . . . . . . . . . 6

1.2.2 半监督协同主动学习 . . . . . . . . . . . . . . . . . . . . . . . . . 6

1.2.3 领域适配协同主动学习 . . . . . . . . . . . . . . . . . . . . . . . 8

1.3 尚待解决的问题 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

1.4 本文的研究方法与研究内容 . . . . . . . . . . . . . . . . . . . . . . . . . 10

1.5 全文的组织结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2 小样本学习基础理论

15

2.1 主动学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.1.1 不确定性采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.1.2 专家委员会 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.1.3 期望模型变化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.1.4 期望误差减小 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.2 半监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2.2.1 自训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2.2.2 协同训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2.2.3 直推式 SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

I

2.2.4 生成模型算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.2.5 基于图的半监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.6 流形正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.3 迁移学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.3.1 基于实例的迁移学习方法 . . . . . . . . . . . . . . . . . . . . . . 23 2.3.2 基于特征空间的迁移学习方法 . . . . . . . . . . . . . . . . . . . . 24 2.3.3 基于模型的迁移学习方法 . . . . . . . . . . . . . . . . . . . . . . 25 2.4 学习模式对比分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

3 结合不确定性和代表性的启发式主动学习框架

28

3.1 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3.2 双样本测试问题 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.3 结合不确定性和代表性的启发式主动学习框架 . . . . . . . . . . . . . . . 33

3.3.1 不确定性和代表性联合框架 . . . . . . . . . . . . . . . . . . . . . 33

3.3.2 不确定性和代表性联合算法 . . . . . . . . . . . . . . . . . . . . . 36

3.4 实验及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

3.4.1 实验数据及实验设置 . . . . . . . . . . . . . . . . . . . . . . . . . 39

3.4.2 实验结果及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

3.4.3 参数敏感性分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4 鲁棒的多标签主动学习方法

46

4.1 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

4.2 最大相关熵准则理论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

4.3 鲁棒的多标签主动学习方法 . . . . . . . . . . . . . . . . . . . . . . . . . 50

4.3.1 多标签不确定性衡量 . . . . . . . . . . . . . . . . . . . . . . . . . 51

4.3.2 多标签中代表性衡量 . . . . . . . . . . . . . . . . . . . . . . . . . 52

4.3.3 多标签主动学习方法 . . . . . . . . . . . . . . . . . . . . . . . . . 54

4.3.4 模型优化求解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

4.4 实验及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

4.4.1 实验数据及实验设置 . . . . . . . . . . . . . . . . . . . . . . . . . 59

II

4.4.2 实验结果及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.4.3 参数分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.4.4 复杂度分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.4.5 损失函数分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

5 融合判别性和代表性的半监督主动学习方法

70

5.1 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

5.2 基于 K-均值的半监督聚类树算法 . . . . . . . . . . . . . . . . . . . . . . 74

5.3 融合判别性和代表性的半监督主动学习方法 . . . . . . . . . . . . . . . . 76

5.3.1 主动学习策略 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

5.3.2 半监督主动学习中判别性信息提取 . . . . . . . . . . . . . . . . . 77

5.3.3 半监督主动学习中代表性信息提取 . . . . . . . . . . . . . . . . . 78

5.3.4 融合判别性和代表性的半监督主动学习方法 . . . . . . . . . . . . 80

5.4 实验及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

5.4.1 实验数据及设置 . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

5.4.2 实验结果及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

5.4.3 模型简化分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

5.4.4 参数分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

5.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

6 领域适配协同的多域主动学习方法

96

6.1 引言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

6.2 理论基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

6.2.1 实例权重法实例权重法 . . . . . . . . . . . . . . . . . . . . . . . 100

6.2.2 最大均值差异理论 . . . . . . . . . . . . . . . . . . . . . . . . . . 102

6.3 领域适配协同的多域主动学习方法 . . . . . . . . . . . . . . . . . . . . . 103

6.3.1 领域适配协同的主动学习框架 . . . . . . . . . . . . . . . . . . . . 104

6.3.2 多域主动学习方法 . . . . . . . . . . . . . . . . . . . . . . . . . . 106

6.3.3 优化求解 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

6.4 实验及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

6.4.1 实验数据及设置 . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

III

6.4.2 实验结果及分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 6.4.2.1 数据集 20Newsgroups . . . . . . . . . . . . . . . . . . . 113 6.4.2.2 字符数据集 . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

7 总结与展望

118

7.1 全文总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

7.2 未来展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

博期间发表的科研成果目录

137

参与的科研项目

139

读期间所获奖励

140

致谢

141

IV

面向小样本问题的主动学习理论及应用研究
摘要
近年来，机器学习在大数据应用中取得了很大的成功，应用场景和范围日益丰 富，涵盖了计算机视觉，自然语言处理，智能医疗等众多领域。大数据为机器学习的 发展带来机遇的同时也带来了挑战，突出的表现为数据标记困难。数据类型多样、数 据产生环境复杂、数据更新速度快等因素，导致在实际应用中往往只有少量的标记数 据可以利用，大大降低了模型泛化能力。如何利用有限的人工标记提高数据的分类精 度，即小样本问题，是当前机器学习领域的前沿问题之一。主动学习技术是机器学习 领域解决小样本问题的基础方法之一，其利用人机交互的方式，从大规模数据中选取 最具有信息量的样本进行人工的标记，逐步的建立高质量小样本训练数据集，来提升 分类模型在小样本下的泛化能力。本文面向小样本下的数据分类和预测任务，系统地 研究了利用主动学习解决分类任务中数据匮乏的方法。不确定性和代表性是主动学习 查找最具信息量样本的两大核心准则。不确定性主要是为了查找数据中类别间界面附 近的数据点，代表性主要是为了挖掘数据集中的分布结构、减少冗余和加速学习。本 文为了提升主动学习的性能，设计了一系列不确定性信息和代表性信息统一结合的样 本学习模型，主要创新点包括：
1）针对单标签数据分类问题，本文提出了将双样本理论和不确定性结合的启发 式主动学习框架，框架中基本上涵盖了进行理想样本查询的所有条件，克服现有主动 学习方法框架通过约束假设表达数据结构造成算法适应性差的难题。依据此框架进行 主动学习算法设计，可以选择出即具有不确定性又具有代表性的样本。
2）针对多标签数据分类问题，本文提出了基于最大相关熵准则的鲁棒多标签主 动学习模型，解决异常低相关标签影响主动学习中不确定性和代表性准则准确表达的 难题. 提出的模型主要是利用最大相关熵准则的有界性，抑制低相关标签在两种准则 衡量中影响，发挥强相关标签在衡量中的主导作用。
3）针对半监督和主动联合学习中存在的半监督信息欠缺问题，本文提出了一种 融合代表性和判别性的半监督主动学习方法，主要是利用主动学习循环中更新前后的 标记集，建立多个分类模型和聚类模型，依据分类模型对未标记样本分配可靠性伪标 签获得判别性样本，依据聚类中的近邻原则进行未标记样本的伪标签标记获得代表性
-V-

武汉大学博士学位论文
样本，而不能进行伪标签分配的样本则作为主动学习样本查询的候选集。通过这种融 合方式，可以极大地丰富标记集中判别性和代表性信息，快速提升主动学习的性能。
4）针对领域适配和主动学习联合学习中不确定性与代表性准则框架不统一的难 题，本文提出了一种领域适配协同的多域主动学习方法，将目标域数据和源域数据嵌 入到一个不确定性和代表性联合的主动学习框架中进行目标域样本的查询，同时在主 动学习中不断调整源域数据分布，使其更加适合目标域分类任务，大大提高跨领域分 类问题精度。
关键词: 主动学习，半监督学习，领域适配，多标签学习，不确定性和代表性
- VI -

面向小样本问题的主动学习理论及应用研究
ABSTRACT
Machine learning has achieved great success in many ﬁelds with big data during the past decades, including computer vision, natural language processing, and smart medical and so on. The big data has brought opportunities as well as challenges for machine learning. The annotation of data is one of the challenges that widely exists. The data types are various, the environment of data generation is complex, and the speed of data updating is fast and so on, resulting in that only a few labeled data can be used in many real world applications and the generalization ability of learning model is poor. How to improve the classiﬁcation accuracy with limited labeled samples, namely small size samples problem, becomes one of the hot topics in machine learning ﬁeld. Active learning is an approach by the interaction between machine and human experts to address the small size samples problem. It selects the most informative samples for human labeling from large-scale data to build a high quality training data set. Then a strong generalization ability model can be trained with these high quality data. To improve the active learning performance, this thesis presents a systematic study on the models of samples selection with the combination of uncertain and representative information. The novel contributions are summarized as follows:
1) For the classiﬁcation task of single label data, this thesis proposes a heuristic active learning framework by combing the two-samples theory and uncertainty together. In this framework, it almost contains all the constraints that can be used to select the optimal samples. In this way, the poor adaptability of the related works is overcome, due to their data structure presentation with various assumptions. The active learning methods that are designed based on this framework can select the samples with uncertainty and representativeness simultaneously.
2) For the classiﬁcation of multi-label data, this thesis proposes a robust multi-label active learning model based on maximum correntropy criterion to address the problem that the abnormally low relevant label impacts on the accurate measure of uncertainty
- VII -

武汉大学博士学位论文
and representativeness. The proposed model mainly depends on the bounded property of the maximum correntropy criterion, which can restrain the inﬂuence of low relevant labels to measure the two criteria and make the strong relevant labels dominate the measures.
3) To address the shortage of semi-supervised information in the combination of semi-supervised learning and active learning, this thesis proposes a semi-supervised active learning method by integrating the discriminative and representative information together. It mainly uses the labeled data that is before and after updated in the active learning loop to build several classiﬁcation models and clustering models. Then the discriminative samples can be obtained by assigning pseudo labels with high conﬁdence based on the classiﬁcation models, and the representative samples can be obtained by assigning pseudo labels based on clustering models. While the samples that can not be assigned with pseudo under the mechanism are regarded as candidates for active learning. By this way, the discriminative and representative information in labeled data are greatly enhanced, and the active learning performance is eﬃciently improved.
4) For the problem that the uncertainty and representativeness criteria are usually not uniﬁed in the combination of domain adaptation and active learning, this thesis proposes multiple domains active learning with domain adaptation. It embeds the source data and target data in an active learning framework which combines the uncertainty and representativenss together to select the most informative samples from target domain. Meanwhile, the distribution of source domain can be constantly adjusted in active learning to make the source data be suitable for the target classiﬁcation task, and the accuracy is improved for the cross-domain classiﬁcation.
Key words: active learning, semi-supervised learning, domain adaptation, multilabel learning, uncertainty and representativeness
- VIII -

面向小样本问题的主动学习理论及应用研究
1 绪论
1.1 研究背景与意义
当前，网络上各类信息的数据量增长十分迅速，据工信部统计，数据总量每年呈 50% 增长，呈现出海量聚集爆发增长的态势。海量的网络数据已成为人工智能时代 机器学习发展的基石。由于数据来源广泛、数据类型多样、数据的更新速度快，迅速 对大量数据进行标记是对数据充分利用的前提。比如，深度学习的巨大成功得益于 ImageNet 数据中数以千万的标记图片 [45, 96]，利用 ImageNet 数据集训练得到的经 典网络模型，如 GoogleNet [158], AlexNet [96],VGG16 [152] 和 ResNet [73]，为解决 图像处理应用领域许多实际问题提供了主要技术支持。然而，为每一个具体的学习任 务，建立一个如此庞大的数据集是很困难的。在自然语言处理中，基础数据集宾州中 文树库中 4000 个句子的标注用时两年 [195]。此外，对于一些专业数据集的标注，更 是需要具备专业知识的技术人才才能进行数据集标注。比如医学影像数据集的标注， 需要资深的医生专家进行精准判读，但是资深医生是非常稀缺的，建立一个大的医学 影像数据集需要付出很大的代价 [164]，IBM 为打造 WaltsonHealth 平台，曾花费数 百亿美元收购健康大数据公司。数据标注问题一直以来是机器学习研究中应用的热点 和难点问题之一。在许多现实应用中，只有少量的标注数据可用，这就要求我们学会 用小数据去解决大数据的分析问题。对于监督分类模型来说，要想在未知数据集上取 得较好的结果，前提是假设训练数据集与测试数据集独立同分布，以此来保证训练模 型的泛化能力，但是小数据集的信息量有限，与测试数据集的分布往往差异非常大， 这就造成了小数据分析大数据在理论和实际应用中的障碍 [2]。如何利用有限的标记 数据训练在测试集上具有强泛化能力的分类模型即小样本问题，是机器学习中经典 的、具有挑战性的瓶颈难题。小样本问题在自然语言处理领域、医学影像处理领域， 城市安防、视频监控等领域广泛存在 [23, 167, 176]，解决小样本问题，对促进信息产 业发展和应用具有很高的价值和重要的意义。
主动学习是解决小样本问题的关键技术之一 [145]，其是相对于被动学习而言。 被动学习基于已经标注好的数据集上进行模型训练，这些标记数据集是人工随机选 取，造成有限标记训练样本中的信息量严重匮乏；主动学习通过人机交互方式，基于
-1-

武汉大学博士学位论文
图 1.1: 主动学习原理：查询函数从未标记集合中选取最具信息量样本，不断增强分类模型
查询函数，有选择性的从大量未标记数据中选取信息量最丰富的样本进行人工标记， 使得小样本数据中的信息尽可能的代表未标记数据中信息量，从而实现小样本下对大 数据的分析处理，同时降低耗费。主要流程如图 1.1 所示。
主动学习技术主要通过提升训练样本的质量来提升分类模型的性能。通过样本数 量进行训练集信息量的扩充，同样可以达到提升分类器性能的目的。因此，除了主动 学习外，在机器学习领域还提出了通过增加可用样本数量进行模型训练学习的技术， 主要是迁移学习和半监督学习，也可以缓解小样本问题。迁移学习的思想是通过从相 关邻域中提取已有数据，在放宽了训练数据和测试数据独立同分布的限制条件下，使 源域中数据与目标域中数据进行条件概率分布或者边缘概率分布匹配，获取能够适用 于目标域模型训练的数据 [128]。半监督学习则是通过充分挖掘未标记样本中的信息， 比如结构信息、分配可靠性伪标签增加标记信息，达到丰富训练数据中信息匮乏的目 的 [33]。与迁移学习、半监督学习等解决小样本问题的方法相比，主动学习的适用场 景更加灵活和宽泛，仅仅需要少量的标记样本，就可以逐步建立可靠数据集。
主动学习已在计算机视觉、数据挖掘和机器学习等领域的研究中取得了广泛的关 注 [52, 184]。在实际应用中，很多问题是动态变化的，需要不断修正数据，否则很难 保证训练模型的准确性。比如，在微博文本分类中，用户随着兴趣爱好的改变，微博 状态也会随之改变，以前用户关注的是生活、旅游和科技，可能在某个时期对武器、 军事等产生兴趣，微博转发内容发生改变，这时候就需要人工标记少许文本和更新模 型，从而提供更加个性化的服务。事实上，主动学习已经在电子商务，比如亚马逊中
-2-

面向小样本问题的主动学习理论及应用研究
的命名实体识别，地图服务，比如谷歌地图路线导航修正等领域取得了成功的应用， 并创造了不菲的价值。信息时代，数据已成为驱动人工智能发展的源动力。主动学习 作为建立高质量数据训练集的核心技术，其研究仍然具有很大的空间。因为数据类型 多样，不同的数据类型进行主动学习准则设计时要考虑的信息有很大差异。以多标签 分类问题为例，一个样本实例对应多个标记，标签与样本的关联性存在强弱问题，同 时标签之间也存在关联性 [3]。这不同于单标签数据，标签之间相互独立，而且一个 样本有且只有一个标签。在多标签和单标签数据的主动学习查询函数的建立过程中， 需要考虑的数据特性不同，而且多标签和单标签的标注空间耗费差异巨大。
本文为了提高主动学习的性能，研究了主动学习在单标签数据和多标签数据中的 准则制定，同时研究了基于半监督学习和领域适配协同下主动学习方法。
1.2 国内外研究现状
小样本问题引起了学术界和工业界的广泛关注。主动学习作为解决小样本问题的 核心技术之一，取得了一系列的成果 [71, 121, 161]。主动学习的核心思想是建立有效 的查询准则从大批量的样本中选取最具信息量的样本进行人工标记，而不是随机的 标注数据，这样可以去除随机标注中产生的冗余信息同时增加样本的信息量，从而 建立高质量的样本集。建立查询准则可以分为两类：一类是不确定性，主要是为了 查询分类过程中可能位于分类边界上的样本，逐步强化分类模型 [145]；另一类是多 样化性，主要是尽可能用查询的少量样本表达大量数据的信息，去除查询样本之间 的冗余 [36, 131]。类似于多样化性，近年来，为了挖掘数据的内部结构，在去除冗余 的同时，提出了利用代表性进行分布信息的查询 [36]。更进一步地，为了查询既具有 不确定性又具有代表性的样本，主动学习中不确定性和代表性的联合学习成为了主 动学习算法研究中受关注的问题 [52]。此外，为了实现信息互补提升训练数据中的信 息，不同学习模式的协同学习成为了机器学习领域的热门研究方向，比如多任务学 习和多视图学习 [101, 120]，多模态学习和迁移学习 [160, 191]，多模态学习和多核学 习 [179, 218] 等。在解决小样本问题中，为了从训练样本的数量和质量上增加训练数 据的信息量来提升分类模型性能，主动学习和半监督学习以及领域适配相协同成为了 主动学习的重点应用。为了系统地研究小样本问题，本文的工作集中在单标签和多标 签数据上主动学习不确定性和代表性准则的结合，主动学习和半监督学习的联合，以 及主动学习和领域适配的联合三个方面开展，本节也主要从这三个方面概述主动学习 的研究现状。
-3-

武汉大学博士学位论文
1.2.1 主动学习准则
不确定性和代表性作为主动学习的核心准则，是主动学习建立查询函数的重要驱 动力，目前已在计算机视觉、机器学习和数据挖掘等领域开展了丰富的研究。
1.2.1.1 不确定性准则
不确定性准则是主动学习中最受关注的准则 [145]，其研究比较宽泛，包括边缘 采样 [8, 146]，专家委员会 [14, 57]，期望误差减小 [59, 95, 140] 等。边缘采样的方法主 要是衡量样本相对于分类界面的位置，认为在分类过程中决策值最不可靠的样本可能 位于分类界面上，可以有效增强分类器的性能。文献 [90] 利用分类器计算单个样本 属于每个类别的概率，将最大概率值和第二大概率值作差，查询差值最小的样本作为 最具有不确定性的样本进行标注。在文献 [8] 中，其计算样本不确定性的准则更加直 接，通过点到分类界面的距离进行衡量。这两种方法是主动学习方法中经典的边缘 采样方法。[79] 提出了最小化期望信息熵，通过计算未标记样本在所有标签下的信息 熵，利用概率预测值进行所有信息熵的加权求和，选取期望信息熵最小的样本。专家 委员会不同于边缘采样的衡量方式，其主要是通过训练集构造多个小的训练样本集 合，利用每个小的训练集合进行训练，得到多个分类模型，认为投票结果最不一致的 样本是不确定性最大的样本，套袋法（Bagging）和提升法（Boosting）是常用的专 家委员会构造方式 [116]。文献 [162] 基于信息熵将 [116] 扩展到多类别的情况，通过 套代法和提升法构造多个训练集合，利用专家委员会的预测标签，计算标签分布概 率，来计算信息熵，选取信息熵最大的样本进行标记。期望误差减小是另外一种机制 的查询准则，其目的是挑选能够最大限度减小模型泛化误差的样本来增强分类器的性 能。文献 [59] 利用分层的锚图构建主动学习中的样本候选集，以及每个锚点的近邻 集，再使用所有数据的误差减小和每个锚点近邻集误差减小之间的比率来近似评估每 个锚点的误差减小，实现对大规模数据信息量的快速搜索。文献 [95] 提出了数据驱 动的主动学习方法，先基于蒙特卡罗在线学习在一个辅助数据源上模拟主动学习过程 中不同数据状态下的期望误差减小，并构建数据状态和期望误差减小值之间的回归模 型，然后将学习到回归模型应用到目标数据上，查询目标数据集上期望误差减小最大 的样本。文献 [213] 将主动学习的期望误差减小策略应用于代价敏感分类，提出了一 种快速更新逻辑回归模型和 C-SVM 的法则，避免了主动学习过程中的重复训练，提 升了算法效率。
虽然基于不确定准则的方法在主动学习的研究中，开展了大量的工作并且取得了
-4-

面向小样本问题的主动学习理论及应用研究
较好的效果。但是不确定性准则通常在一次循环中只选取一个样本，因为选取多个样 本时，可能造成选取样本之间信息的冗余，查询样本多时效率较慢 [77]。同时，不确 定性准则忽视了数据结构信息，可能造成查询结果的偏移 [36]。因此，需要多样化准 则或者代表性准则对不确定性准则进行完善和增强。 1.2.1.2 代表性准则
代表性准则是广义的多样化准则，是多样化准则的进一步信息挖掘，是对不确定 性准则的信息补充。多样化性准则一般是指对少量不确定性中的样本进行冗余信息去 除，其主要是针对批处理模式的主动学习方法，也就是每次循环中查询多个样本进行 标记。多样化性准则具体有两种策略，一种是从一批不确定性中先选择一定数目的 不确定样本，然后采用聚类等策略从预先选好的不确定性样本中筛选出代表性的样 本 [131]。文献 [94] 利用 K- 近邻进行样本密度估计并计算样本到标记样本集的最小 距离，将两者结合作为多样化性准则来提升专家委员会方法在批模式主动学习中的性 能。文献 [42] 利用核化 K 均值对筛选的大量不确定性样本进行聚类，选择聚类簇中 最不确定的样本进行标记。文献 [185] 提出了通过最大化朴素贝叶斯的子模函数或者 最近邻的子模函数来从不确定样本中选择子集。事实上，这些方法仍然选择的是不确 定性样本，因为它们的多样化性限定了样本只能从一个预筛选出的较大的不确定性子 集中进行选择，并未挖掘出数据集的结构信息。因此，提出了代表性准则，在去除冗 余的同时，通过分布估计使得标记数据集和未标记数据集之间具有相似的分布，挖掘 数据内部结构信息 [82, 204, 209]。不同于一般的多样化性策略，代表性准则不需要预 先依据不确定性准则进行样本的预筛选，而是可以直接看作是主动学习查询函数，其 目标是保证选择的样本与未标记数据集具有独立同分布的特性。文献 [125] 提出了一 种结合聚类的主动学习的方法，利用数据的先验概率分布，假设聚类簇中高密度的样 本为最具信息量样本，从而选择代表性的样本进行标注。文献 [77] 提出了边缘概率 分布匹配的主动学习方法，利用最大均值差异 [64] 在再生核希尔伯特空间中衡量未 标记样本和标记样本的边缘分布的一致性，选择能够使未标记样本集和标记样本集趋 近于一致分布的样本进行标记。
尽管多样化性和代表性在批模式下提升了主动学习的性能和效率，但是单一准则 相对衡量信息片面，例如代表性信息仅挖掘出了分布信息，因此准则的联合学习成了 进一步提升主动学习性能提升的关键。
-5-

武汉大学博士学位论文
1.2.1.3 不确定性准则和代表性准则结合
主动学习过程中准则的联合学习主要是将不确定性和代表性结合到同一个主动 学习框架中，查询既具有不确定性又具有代表性的样本。文献 [52] 提出了一种凸优 化的主动学习框架，利用稀疏表达进行样本相似性表达，将不确定性的衡量作为稀 疏表达中稀疏系数的权重，实现从不确定性高的样本中选择相似性表达能力强的样 本。文献 [184] 将最小化结构风险和最大均值差异风险通过权重相结合，查询出既 具有判别行又具有代表性的样本进行标注。文献 [86] 将半监督中的最小 -最大框架 引入主动学习中，基于未标记样本的伪标签衡量代表性信息，利用最小二乘损失约 束不确定性信息，实现目标函数最小时样本的查询。文献 [201] 提出了多类主动学 习方法，首先基于图的随机行走算法进行分布估计，并利用信息熵进行不确定性的 衡量，将两者进行加和查询具有不确定性和代表性的样本。文献 [87] 提出了结合不 确定性和多样化性的多标签主动学习方法，将标签基数不一致作为不确定性，同时 引入样本集缺少的标签数量作为多样化性，来查询带有需要标签信息的样本，没有 考虑多标签中低相关性标签的影响。关于主动学习准则的联合学习还可以参考文 献 [20, 21, 47, 56, 99, 103, 143, 171, 172, 182]。
综上，对于单标签数据的主动学习准则，一些方法是基于各种条件假设下的学 习，例如半监督假设 [86]，稀疏描述 [52] 等，缺少设计主动学习有效算法的灵活性。 而对于多标签，其忽略了多标签数据的特点，对异常低相关性标签的影响在准则建立 难以抑制。因此，单标签中建立灵活的主动学习框架，和建立对多标签数据有效的主 动学习准则是本文研究的重点。
1.2.2 半监督协同主动学习
主动学习和半监督学习的联合主要是为了增加对未标记样本的利用，实现主动学 习性能的进一步提升。半监督学习大致可以分为两类 [14]，一类是对未标记样本进行 统计学习来评估数据的分布，比如流形正则学习 [9]，一类是通过对未标记样本分配 可靠性的伪标签，扩充标记的样本集合 [7]。主动学习准则和半监督学习的联合也基 于这两方面进行了丰富的研究。首先是考虑未标记数据的分布情况，文献 [217] 提出 了高斯随机场中主动学习和半监督学习结合的方法。将半监督学习嵌入一个高斯随机 场的带权图中，并把节点的均值看作是一个调和函数，利用主动学习查询能够最小化 调和函数能量的样本。文献 [24] 中提出了流形自适应实验设计的主动学习方法。通过 拉普拉斯矩阵的嵌入建立一个流形自适应的核空间来反映数据的几何结构，最小化这
-6-

面向小样本问题的主动学习理论及应用研究
个核空间中分类器的期望误差来进行样本的选择。文献 [210] 提出了流形正则实验设 计的主动学习方法。不同于 [24]，其显式的表达了主动学习和半监督学习的关系，同 时此方法的选择不需要依赖训练样本集的标签信息。文献 [78] 提出了半监督 SVM 的 批处理主动学习方法。其将标记样本和未标记样本混合学习到一个核函数，然后将核 函数应用到最小 - 最大框架下的主动学习中，查询不确定性和多样化性的批量样本。 文献 [26] 提出了适用于支持向量机的半监督主动学习方法，其主要是通过生成概率 混合模型俘获数据中的结构信息，然后在 SVM 的核空间中，将混合模型融入到主动 学习的距离、密度、多样化和分布多种策略中来查询样本。
通过学习数据结构的半监督方式进行主动学习性能提升，主要是为了数据集中近 邻关系的保持，比如是图模型和流形结构，这些模式都是通过无监督方式进行数据中 结构的描述，而图的方法通常在大数据下具有很大的计算复杂度，因为关联矩阵过 大。
为了在大数据的处理中具有很好的效率，本文中半监督和主动学习联合的研究主 要是进行可靠性伪标签的标记，也就是通过扩充训练数据集来提升性能和效率。文 献 [174] 首先训练一个卷积神经网络对未标记样本进行预测，然后选择大多数决策值 高的的图片自动标记可靠性伪标签的，对于少数可靠性低的未标记样本通过主动学习 策略进行选择标记，将这些标记的样本对卷积神经网络进行微调，逐步提升卷积神经 网络的可靠性。文献 [114] 将主动学习前的标记样本集和伪标记样本混合，训练一个 分类模型，并以此建立主动学习准则选择不确定性高的的样本进行标记，获得新的标 记样本集，再新标记样本集上重新训练分类模型，依据前后两次的分类结果一致性， 对未标记样本进行重新伪标记分配，实现主动学习和半监督协同学习的快速提升。文 献 [170] 也采用了相似的策略。文献 [200] 不同于传统的主动学习方法通过人工标记， 而是通过随机标记的形式生成多个伪标记标注器，对主动学习策略查询到的样本进行 自行标记，进一步减少了主动学习的耗费。文献 [65] 通过选择目标域中有的类别而 源域中没有的类别但特征相近的样本进行伪标签的重分配，利用伪标记的源域样本和 目标域中的样本进行半监督学习，并以此建立主动学习查询函数从目标域中选择样本 进行标记。文献 [97] 提出了一种半监督主动学习方法用于软件瑕疵的预测，通过标 记样本和伪标记样本建立多个随机树，依据随机树选取最不确定的样本进行人工标 记，而对于集成结果一致的样本进行伪标记，不断更新多个随机树。
利用伪标记样本进行半监督学习和主动学习的结合，目标主要是保证伪标记样本 的伪标签的可靠性。如果没有很好的策略保证伪标签的可靠性，则很难保证主动学习
-7-

武汉大学博士学位论文
的有效性。因为在这个过程中，主动学习和半监督学习是相辅相成的，主动学习提升 伪标记样本的标签可靠性，而伪标记样本则提升主动学习对查询函数对样本信息的正 确衡量。如上所述，这些方法主要是保证伪标记标签的可靠性，信息挖掘相对片面， 忽略了伪标签样本信息的多样性，只有挖掘多样化信息，才能使得标记集合的信息更 全面的表达未知数据，而主动学习查询的样本也能促使标记数据集信息更加丰富，从 而提升分类器性能。因此，半监督下进行多样化信息的挖掘来提升主动学习的性能， 是本文研究的重点。
1.2.3 领域适配协同主动学习
当目标域上标记数据稀少，但是存在已有相关标注的源域数据时，主动学习和领 域适配的联合学习可以实现源域数据更好的适应目标域的分类任务，提升领域适配学 习的性能，同时也提升主动学习的性能。
领域适配是迁移学习中的一个方向，是满足领域间边缘概率分布的同构迁移学习 方法 [2, 128]。在主动学习和迁移学习结合的研究中，主动学习和领域适配联合学习 是研究的热点方向之一 [4, 44, 133, 190, 220]。领域适配主要有两种方法类型减小源域 和目标域的边缘概率分布差异，实例权重法和特征表示法 [2]。实例权重法主要是通 过对源域样本权重的学习，使源域与目标域尽可能具有相同的边缘概率分布，而特征 表示法则是把源域和目标域投影到一个公共子空间中，实现两个数据域在新空间中的 分布匹配。在与主动学习的结合过程中，无论特征表示法还是实例权重法，都需要样 本在特定的空间中能够显式表达，因为主动学习查询的样本是具体的。文献 [132] 提 出了基于特征表示法和主动学习结合的主动迁移方法，其利用投影矩阵将源域和目标 域投影到一个公共子空间中，在公共子空间中从源域中选择能够最小化目标域和源域 分布的样本。事实上，相对于特征表示法，主动学习与实例权重法结合的工作相对 更充分，由于实力权重法的空间一直都是显式的。文献 [35] 提出了一种基于源域加 权的批处理主动迁移学习方法。通过对源域的样本进行加权，并利用核最大均值差 异（Maixmum Mean Discrepancy, MMD）衡量加权后的源域和少量目标域上的标记 样本与目标域上未标记样本进行边缘概率分布匹配，期望主动学习查询的样本能够最 小化标记集和目标域上未标记集的边缘概率分布差异。文献 [180] 不同于 [35]，其提 出的模型偏移下的主动迁移学习主要是基于条件概率的分布匹配，通过对源域上的样 本条件概率加权，与目标域上的条件概率进行基于最大均值差异的匹配，利用主动学 习选择能最小化条件概率差异的样本。文献 [63] 提出了结合零样本先验模型的主动 迁移学习方法。首先从源域上学习零样本分类器，用作主动学习的暖启动模型，同时
-8-

面向小样本问题的主动学习理论及应用研究
对暖启动模型进行线性加权并和学习到的目标域上的分类器进行求和作为纠正的预测 函数，将预测函数转变为支持向量机的对偶方程，选择能够最大化对偶方程的的样 本。文献 [66] 提出了知识跨类迁移的主动学习方法。借助类别属性矩阵建立线性生 成函数，同时对目标域上的样本和类别标签进行加权，基于生成函数建立源域样本和 标签，以及加权的目标域样本和加权标签之间的对应关系损失，学习到生成函数后， 利用主动学习的多层次不确定性 [93] 策略选择最不确定性的样本。
尽管显式的空间定义可以从理论上最小化主动迁移过程中的误差泛化上界，较容 易进行理论上的分布适配误差分析，但是适配的最终目标是使得源域能够辅助目标域 的分类任务。而这些研究很少将源域的判别性和目标域进行统一关联。因此，实现判 别性适配，是本文研究的重点。
1.3 尚待解决的问题
综上所述，小样本问题研究中，主动学习在理论和应用上都进行了深入的研究和 验证。但是不确定性和代表性信息的有效充分准确挖掘，仍然是设计主动学习算法查 询最具有信息量样本的难点。在对问题的分析和方法设计中仍有很多的不确定因素尚 未考虑，本文从四个层面进行了分析：
1）数据结构表达受限于数据假设条件，缺乏设计有效主动学习算法的灵活框架: 主动学习的核心问题是设计能够反映标记数据集信息需求的查询函数，从大量未标记 样本中查询到最具信息量的样本。然而目前一些方法需要数据满足某些结构假设，才 能查询数据中最具信息量样本。比如，最小 - 最大主动学习框架 [86]，其建立是基于 半监督假设，如果数据不符合半监督假设，就会出现性能下降问题。文献 [52] 凸优化 的主动学习框架利用稀疏表达进行相似性的衡量，以不确定性作为稀疏权重系数，其 选择关注于不确定性高的样本，同时当数据集不适合用稀疏表达进行描述时，查询的 样本代表性不强。因此，需要建立一个可以进行算法设计灵活的主动学习框架。
2）多标签中异常低相关标签的存在致使分类模型判别性差，样本间特征相似性 偏差大，直接影响主动学习中不确定性和代表性的准确衡量：多标签是指一个数据示 例具有多个类别，而且这类数据在现实应用中广泛存在，例如一篇足球新闻报道可以 看作是体育新闻，也可以看作是关于球星的娱乐新闻。但是标签与示例之间存在强弱 相关性，如果新闻中大篇幅报道的是赛事情况，而只出现一次某个球星的名字，那么 这篇报道就和体育新闻呈强相关性，对于是否属于娱乐新闻，可以说具有异常低的关 联性，因为其与球星的娱乐新闻内容相差很大，只是少许关联，但是娱乐新闻仍然是
-9-

武汉大学博士学位论文
这篇报道的标签。在进行主动学习准则建立时，需要将这篇报道的特征和娱乐新闻建 立联系，但是由于特征对标签的判别性不强，这就会对准则的建立产生负影响。因 此，如何建立多标签中主动学习的查询函数，实现对强相关性标签的判别性识别，抑 制异常弱相关性标签对准则建立的影响，是建立多标签主动学习模型的难点问题。
3）半监督提供的伪标记样本信息欠缺，不能满足主动学习查询数据集不确定性 和代表性的需求：半监督学习技术是在标记样本少时，通过对大量未标记样本的利用 来增加训练数据的信息。半监督协同主动学习的联合学习方法无论是通过统计式的信 息挖掘还是通过对大量未标记样本进行伪标记都获得了很好的效果。但是对于通过统 计学习的方式进行数据信息的挖掘算法，计算复杂度一般比较高，例如被广泛采用的 流形正则化需要建立每个样本的近邻关系，当数据量特别大时，其时间耗费巨大，并 且相对于主动学习过程中对信息需求，结构信息的挖掘形式过于单一，不具备判别 性。同样地，通过对大量未标记样本进行伪标记，现有方法大多考虑通过分类模型选 取预测可靠性高的样本，未考虑结构信息的挖掘，造成提升主动学习性能过程中缺少 代表性信息。因此，目前半监督和主动学习协同方法中，半监督过程学习的信息欠 缺，不能和主动学习过程中不确定性和代表性的需求保持一致，如何使半监督学习和 主动学习查询的信息互补是研究中需要关注的问题。
4）领域适配和主动学习不确定性和代表性结合缺少统一框架，造成领域适配结 果判别性不强：领域适配的目的是在标记样本少时，通过利用已有标注并且与目标域 分类任务相似的源域数据来增加目标数据分类任务中可用的标记样本数量。领域适配 和主动学习的协同学习主要是通过概率分布相似度函数，从目标域中选取一定数量的 样本能够辅助学习源域的变换实现目标域和源域边缘分布概率差异最小，达到将源域 数据应用到目标域的目的。但是目前领域适配和主动学习的协同中，并没有联合统一 的框架，实现主动学习和领域适配的同时优化和学习，更多关注的是源域中的数据和 目标域中的数据整体分布的一致性，而未能保证相应类别之间仍然有很好的分布适 配，可能造成源域数据建立的分类模型在目标域上判别性差的问题。因此，如何建立 联合统一的学习框架，使领域适配和主动学习的准则相互补充，提高领域适配结果的 判别性是研究中的难点。
1.4 本文的研究方法与研究内容
为了克服小样本问题中主动学习在理论和应用中面临的困难，比如，主动学习框 架灵活性的欠缺，多标签学习中主动学习准则衡量困难，半监督学习和主动学习结合
- 10 -

面向小样本问题的主动学习理论及应用研究
中信息学习欠缺，以及领域适配和主动学习结合中缺少统一框架造成的适配结果判别 性不强等问题，本文系统的对小样本问题进行了深入分析，研究了不同问题下的解决 方案。本文大致可以分为两个部分，第一部分为根据单标签和多标签数据的特性建立 针对不同类型数据的主动学习准则，分别对应本文的第 3 章和第 4 章；第二部分为 主动学习与机器学习中另外两种解决小样本问题的核心机器学习方法半监督学习和领 域适配的联合学习，分别对应本文的第 5 章和第 6 章。可以看到本文以主动学习解 决小样本问题为主线，系统全面的研究了主动学习解决小样本问题在分类中的应用方 法。
第一部分对应的第 3 章，主要解决缺少灵活设计主动学习算法的框架问题。现有 主动学习框架虽然准则衡量过程中理论性很强，其应用往往受制于理论建立时数据假 设的前提条件，造成设计主动学习方法时缺少灵活性，而灵活性的框架对拓宽主动学 习的应用具有重要的作用。本章节为此建立了一种挖掘不确定性和代表性的主动学习 框架，这个框架是基于启发式的，对于每一部分都进行特定的定义，依据此框架可以 灵活的建立主动学习准则进行理想化的样本查询。在代表性准则中，基于双样本理 论，建立样本对于数据集分布衡量的函数，保证查询的样本与能够代表未标记集分 布，而与标记集中所有样本分布差异最大。对于不确定性，可以建立任何具有特定需 求的不确定性衡量，与代表性通过权重进行连接，同时为了查询样本的多样性，引入 了一个二次规划项。因此，此框架涵盖了主动学习进行理想查询的所有约束。基于 此框架，本章也提出了一种高效的主动学习算法，并与一些前沿算法相比，验证了 提出算法的优越性。本章的主要研究成果已发表于计算机和人工智能领域顶级期刊 IEEE Transactions on Cybernetics 2017。
第一部分对应的第 4 章，主要解决多标签中异常低相关标签造成主动学习联合 准则制定困难的问题。多标签中异常低相关标签严重影响了分类器的判别能力。并且 对于两个具有相同标签集合的样本，由于其异常低相关标签不同，会造成两个样本的 特征表达差异非常大。因此，异常低相关标签会造成主动学习判别性准则和代表性准 则中相似性的衡量偏差很大。但是目前的一些方法中，很多时候根据预测结果，不采 用低相关标签建模，但是并不是所有的低相关性标签与样本关联性都非常低，其低相 关性是相对的，因此直接舍弃会造成信息的丢失，而有些方法则直接忽略其影响。本 章为了抑制低相关标签的影响，基于最大相关熵准则，建立了具有鲁棒的多标签主动 学习方法。主要是利用最大相关熵准则的有界性，在建立模型时，最大化目标函数， 可以使异常低相关标签对模型的影响趋近于零，而强相关性标签的影响最大。同时将
- 11 -

武汉大学博士学位论文
标签集的相似性作为相似性衡量的策略之一结合到特征相似性衡量中。与若干前沿 的多标签主动学习方法相比，提出方法性能具有显著性的提升。本章的主要成果已 发表于计算机视觉领域国际顶级会议 ECCV 2016 和图像处理领域顶级期刊 IEEE Transactions on Image Processing 2017。
第二部分对应的第 5 章，主要是解决半监督学习和主动学习的联合学习中信息 互补欠缺的问题。针对现有的半监督主动学习方法，主要是通过挖掘未标记样本中的 结构信息或者伪标记一些可靠性高的未标记样本，虽然集合了半监督学习的优势，但 是信息挖掘的形式单一，无法满足主动学习过程中信息的多样化需求。本章提出了一 种新的半监督协同主动学习方法，主要是利用具有判别性的分类模型和具有挖掘数据 结构特性的聚类模型获得分类和聚类结果，依据分类和聚类结果的一致性对未标记样 本进行伪标签的分配，实现半监督学习中判别性信息和代表性信息的挖掘。将这些伪 标记样本加入标记集进行样本的扩充，达到对主动学习过程中不确定性和代表性准则 信息补充的目的，来提升主动学习的性能。与前沿方法相比，提出方法在四个基础 数据集上取得了较好的结果。本章的主要研究成果已发表于图像处理领域顶级期刊 IEEE Transactions on Geoscience and Remote Sensing 2017。
第二部分对应的第 6 章，主要是解决领域适配和主动学习结合中缺少领域适配和 主动学习不确定性及代表性的统一框架学习，导致领域适配结果判别性不强的问题。 现有的领域适配和主动学习结合，主要关注的是目标域和源域的边缘概率分布适配， 期望查询的样本能够最小化源域和目标域分布差异，忽视了源域样本对于主动学习过 程中不确定性的辅助。为了提升领域适配对主动学习的辅助作用，同时在主动学习过 程中，提升适配能力，本章提出了主动学习和领域适应在统一框架下学习的方法，其 思想主要是通过对源域样本进行加权，同时和少量目标域中的样本组成标记集，利用 最大均值差异最小化标记集和目标域中未标记样本的分布差异，再利用目标域样本构 建分类模型，对源域加权的样本进行判别性的约束，使源域样本加权后与目标域的样 本具有判别性的适配。将分类判别损失和最大均值差异通过权重进行结合，实现主动 学习对源域样本权重的辅助学习，同时源域样本辅助主动学习查询最具信息量的样 本。本章的主要成果已发表于人工智能领域国际顶级会议 IJCAI 2017。
1.5 全文的组织结构
本文的组织架构和章节安排如图 1.2 所以，按照研究基础和研究内容将本文分为 6 个章节；
- 12 -

面向小样本问题的主动学习理论及应用研究
图 1.2: 论文的组织架构和论文章节安排
第一章主要讲述主动学习的研究背景，其对于机器学习发展和应用实践的价值意 义。综合阐述了主动学习在解决小样本问题中的国内外研究现状，并概括了本文研究 的主要问题和研究内容，简述了本文的架构和章节安排。
第二章主要讲述机器学习中小样本学习的基础理论方法，包括主动学习、半监督 学习和迁移学习，对比了三种方法机制，简述了三种方法中每类方法的思想，以及每 类方法的原理和优劣性。
第三章提出了一种结合不确定性和代表性的启发式主动学习框架。框架中对主动 学习的信息需求进行了详细的定义。解决了主动学习在算法设计中的灵活性问题。并 基于框架提出了一种高效的主动学习方法，通过实验证明了提出框架的有效性。
第四章主要是提出了一种鲁棒的多标签主动学习方法。鉴于最大相关熵对异常点 具有很强的鲁棒性，将最大相关熵准则作为损失函数，最大化目标函数，利用最大相 关熵准则的有界性，保证在进行分类模型建模时异常低相关标签的最大相关熵准则损 失趋近于零，而对于强相关标签则使其最大相关熵准则的损失最大，从而在最大化目 标函数过程中，实现抑制异常低相关标签的影响，发挥强相关性标签对模型的主导作 用。同时，将标签基于最大相关熵准则进行相似性的衡量，避免由于异常低相关标签 的差异引起的特征相似性误差。解决了由于异常低相关标签的存在对主动学习准则衡 量不准确的问题。
第五章提出了一种融合判别性和代表性的半监督主动学习方法。通过主动学习过 程中使用的标记集和更新后标记集训练两个分类器，依据标记集对未标记样本进行快
- 13 -

武汉大学博士学位论文
速的 K 均值聚类和部分样本的聚类标记，依据聚类结果和分类结果的一致性，从判 别性和代表性两个层面进行伪标记样本中信息的挖掘，从而解决半监督协同主动学习 过程中半监督信息欠缺的问题。实验证明了提出方法的高性能以及方法思路的合理 性。
第六章提出了一种领域适配协同的多域主动学习方法。通过最大均值差异进行目 标域和源域样本的分布匹配，使得查询样本能够最小化分布差异。同时，基于现有的 目标域样本，建立分类模型，使其对加权后的源域样本具有很好的判别性。将两部分 通过权重连接，实现统一框架下联合学习。主动学习辅助源域权重学习提升源域对目 标域的判别性，解决了领域适配和主动学习联合学习过程中，由于过多关注于源域和 目标域分布适配，缺少领域适配和主动学习中不确定性和代表性结合的统一框架，可 能造成适配结果在目标域判别性不强的问题。
第七章主要是对本文的研究工作进行总结，并对未来将要开展的相关工作进行展 望，特别是对主动学习与深度学习以及主动学习与元学习的结合展开深入研究，实现 小样本下深度学习模型的有效训练以及主动学习模型的通用性。
- 14 -

面向小样本问题的主动学习理论及应用研究
2 小样本学习基础理论
小样本问题来源于统计领域，文献 [62] 中给出了小样本问题的定义，即由于样 本的数量小于数据的维度，造成散度矩阵的奇异性，这个问题被称为小样本问题。现 阶段随着数据量的增加，可用标记数据稀缺，使得训练集和测试集的分布差异较大， 导致训练模型在测试集上泛化能力差，形成了机器学习中的小样本问题。为了克服小 样本问题，提升模型的泛化能力，在机器学习领域发展了主动学习 [146]、半监督学 习 [33] 和迁移学习 [129] 三种核心方法。这三种方法从可用训练样本的数量和质量上 进行信息扩充，目的都是希望在最小耗费下，使得分类器性能具有显著提升。在小样 本问题的驱动下，三种机器学习方法已经形成了相对完善的理论体系。本文主要针对 主动学习方法的研究，同时也涉及半监督学习以及迁移学习的协同学习。本章节对三 种学习方法的基础理论以及背景进行概述。
2.1 主动学习
主动学习是三种核心技术中唯一从样本质量角度进行训练集提升的方法，由训练 集建立机器学习查询函数，不断从未标记样本中挑选高质量样本交给人工专家进行 标记，逐步建立高质量样本集。因此，主动学习主要涉及五大要素，即标记集 L，未 标记集 U，查询函数 F 、高质量样本集 Q 和人工专家 S。主动学习流程可以表述为 基于标记集 L 建立查询函数 F ，利用 F 从未标记集 U 中选取最具信息量的样本 Q， 交由人工专家 S 进行标记，更新标记集 L = L ∪ Q，同时将 Q 从未标记集 U 中移除 U = U /Q，重复这个过程，不断地查询新的高质量样本集 Q，直至建立满足要求的 高质量训练集。
从主动学习的核心过程可以发现，主动学习过程中最重要的部分是查询函数 F ， 因此主动学习算法的发展主要是查询函数的建立，也就是不确定性和多样化性 [201]。 尽管主动学习近年来在理论和方法上取得了长足的发展，但是其方法的设计和提出， 都离不开基础方法的激励，以下对主动学习的基础算法进行简单概述，主要包括不确 定性采样，专家委员会、期望模型变化、以及期望误差减小等方法 [146]。
- 15 -

武汉大学博士学位论文

2.1.1 不确定性采样

不确定性采样是主动学习中最常用和最经典的方法，其查询的是在分类过程中分 类结果可靠性最低的样本。比如，边缘采样策略 [40, 161]，通过衡量点到面的距离， 选择离分类平面最近的点作为查询的最具信息量的样本

x∗ = arg min |f (x)|
x∈U

(2.1)

边缘采样对于样本不确定性的描述较为直接，衡量值越小，说明样本可靠性越低。目

前很多主动学习方法都是基于此选择策略进行主动学习算法的进一步优化和提升，例

如文献 [86] 和文献 [183] 将边缘采样策略融合到代表性的准则中，实现既具有不确定

性又具有多样性的样本查询。

式 (2.1) 中边缘采样仅考虑了一种标签的可能性，文献 [93] 针对多类别问题采用

最大的类别预测概率和第二大的类别预测概率进行不确定性的衡量，称为 BvSB(Best

vs Second Best)，表达式为

x∗ = arg min P (y1|x) − P (y2|x)
x∈U

(2.2)

P (y1|x) 表示 x 类别归属概率最大值，P (y2|x) 表示类别归属第二大概率值。式 (2.2)

越小则表明样本 x 被判断为类别 y1 和 y2 的概率越相同，其主要查找位于类别 y1 和

y2 边界上的样本。但是当类别数较多时，BvSB 则会忽视其它类别的分类信息，为此

对所有的分类损失进行综合考虑，提出了基于熵的主动学习方法 [92]，表示为

∑C x∗ = arg max −pi(yi|x)logp(yi|x)
x∈U i=1

(2.3)

式（2.3）选取的是最大值，因为当样本对于所有类别的概率相等时，熵最大，此时

样本的具体类别在所有分类平面上最难确定。

不确定性采样的方法有很多，但是上述三种方法是目前主动学习算法中较为常用

的策略。更多的主动学习不确定性采样可以参考文献 [61, 146]。

2.1.2 专家委员会

专家委员会是主动学习中另一种样本选择机制 [147]，主要利用有限的训练样本

构造多个弱分类模型，将这多个分类模型看成是专家委员会对未标记样本进行预测，

选取委员会预测结果最不一致的样本, 作为最具有信息量的样本。常用的机制有投票

熵

x∗

=

arg max

−

∑

V

(y) V log

(y)

x∈U

N

N

y

(2.4)

- 16 -

面向小样本问题的主动学习理论及应用研究

其中 N 表示的是专家委员会的个数，V (y) 表示专家委员会中对 x 预测标签为 y 的个

数。此外，平均 KL(Kullback-Leibler) 散度也是常用的投票不一致性指标，表达式为

x∗

=

arg max
x∈U

1 N

∑ N D(pc||PN )
c=1

(2.5)

其中

D(pc||PN

)

=

∑C
i=1

pc(yi|x)log

pc(yi|x) , PN (yi|x)

pc(yi|x) 表示专家委员会中第 c 个分类模型将 x 判属于 yi 的概率，PN (yi|x) 表示专家

委员会中所有模型将 x 判属于 yi 的概率和的平均值。专家会员会相当于利用多个模

型进行集成，具有集成学习的优点，在专家委员会个数较少时便可以查询到最具信息

量的样本，比如文献 [43] 增强的专家委员会方法。对于专家委员会的不一致性度量，

还有很多指标比如 Jensen-Shannon 散度 [122] 等。

2.1.3 期望模型变化

期望模型变化策略与不确定性采样和专家委员会不同，其主要是选取能够使当前

模型产生最大变化的样本。就是当选取的样本 x 标记以后加入训练集时，训练模型能

够产生最大变化。定义 ∇ℓθ(L) 表示目标函数中模型参数 θ 的梯度，使 ∇ℓθ(L) ∪ ⟨x, y⟩ 表示训练集 L 加入训练样本 ⟨x, y⟩ 以后模型参数 θ 的新梯度。由于 x 是待查的样本，

其标签 y 是未知的，因此，必须用对所有标签进行梯度的计算作为期望来代替真实的

模型变化衡量，其目标函数表达式为

∑C x∗ = arg max Pθ(yi|x) ∥∇ℓθ(L∪ < x, yi >)∥
x∈U i=1

(2.6)

其中 ∥·∥ 表示加入样本梯度向量的欧式范数。应该注意到，相对于加入的单个样本，

训练集 L 的数量在梯度中作用更大，因此为了加快学习效率，可以在训练模型梯度

∇ℓθ(L) 上直接进行梯度求解，即 ∇ℓθ(L∪ < x, yi >) ≈ ∇ℓθ(< x, yi >)，这样对于数目 多的数据集可以进行并行计算。但是如果要真实的计算其期望模型变化，样本数量和

标签数量大时，此方法的计算时间复杂度较高。事实上，模型期望变化就是衡量样本

在无标签下对于模型改变的影响。文献 [25, 58, 203] 中，都是根据期望模型变化的思

想进行主动学习方法的开发。

2.1.4 期望误差减小

期望误差减小不同于期望模型改变，其主要查询能够使模型泛化误差最大限度减 小的样本。通过 L∪ < x, y > 进行模型训练，在剩余的未标记样本集 U 上进行模型
- 17 -

武汉大学博士学位论文

泛化误差的衡量，由于假定未标记集和测试集是独立同分布的，因此，在 U 上的测

试可以表示模型的泛化能力。但是由于样本的标签是未知的，因此仍然采用当前模型

参数 θ 对所有标签进行遍历，其目标函数就是期望减少预测错误标签的数量，可以表

示为最小化期望的 log-loss：

(

)

∑C

∑U ∑C

x∗ = arg min Pθ(yi, x) −

Pθ+<x,yi>(yj |x(u))logPθ+<x,yi>(yj |x(u))

x∈U i=1

u=1 j=1

(2.7)

式 (2.7) 等同于减小在未标记集 U 上的期望熵值。除了熵值形式，还可以通过最大 化样本的增益信息或者互信息 [39]，来选取最具信息量的样本。对于此类主动学习方 法，目前的研究中也有很多，可以参考文献 [59, 67, 80, 140, 213]。

2.2 半监督学习
半监督学习是机器学习中另一种解决小样本问题的技术，主要通过对未标记样 本中结构信息的利用，增加分类模型对未标记样本的可靠性分类。依据学习方式的 不同，半监督学习一般可以分为五种算法类型，分别为自训练、协同训练、直推式 SVM、生成模型算法、基于图的半监督学习以及流形正则化。下面对五种基础算法 类型进行概述。
2.2.1 自训练
自训练学习是半监督算法思想的首次体现，由文献 [144] 中提出。自训练的思想 相对简单，其主要就是利用所有可用的标记样本进行模型的训练和学习，再基于分类 模型对未标记样本进行类别预测，然后将可靠性高的预测标签分配给相应的未标记 样本并加入到训练集重新训练分类模型，不断地重复这个过程。由于自训练在学习 过程中不断将分类器自身标记的样本加入到训练集中进行重新训练，也被称为自学 习 [33]。自训练思想相对简单，目前很多算法基于自训练思想进行了应用和研究，比 如文献 [31] 采用自学习模型进行植物基因组中的基因预测。类似采用自学习进行半 监督学习的方法还可以参考文献 [107, 117, 118] 等。但是其有效性取决于对未标记样 本分配伪标签地可靠性，而这个可靠性是通过少量训练样本建立的分类模型获得的， 因此如果标签可靠性较低，就会造成分类结果不理想。
2.2.2 协同训练
协同训练算法是由文献 [16] 最初提出的，是建立在数据集中有两个视图或特征 子集基础上的半监督学习算法。相比自训练方法，其分配伪标签的可靠性更高。协同
- 18 -

面向小样本问题的主动学习理论及应用研究
训练假设数据集中有两个视图充分冗余，也就是说其特征空间条件独立，而且任何一 个视图的特征空间都可以描述目标问题，从而保证在任何视图下有充足数据集时都可 以训练获得强分类模型。协同训练的思想是使两个视图中的分类模型进行相互的学习 协同，具体策略为在两个视图中建立独立的分类模型，通过这两个分类模型对相应视 图中未标记样本进行分类预测，每个分类模型选择部分分类可靠性高的样本进行标 注，将标注的样本加入到另一个视图的训练集中，也就是作为对方分类器的训练样 本，不断地重复这个过程。
在协同训练中，未标记样本可以帮助减小版本空间。换句话说，两个视图的分类 模型在标记集和未标记集上具有判别一致性。协同训练从多视图增加了伪标记样本标 签的可靠性，文献 [127] 中从理论上分析了协同训练的有效性。协同训练也是半监督 学习中基础的技术方法之一，并且基于协同训练的思想，开展了很多的研究，可以参 考文献 [33, 70, 113, 136] 等。
2.2.3 直推式 SVM
直推式 SVM(TSVM, Transductive SVM) 是半监督学习和 SVM 分类器的结合， 主要针对的是二类问题。在学习过程中对未标记样本通过伪标签的分配更新，寻找能 够最大化正类和负类分类间隔的分类界面 [165]，算法可以描述如下：
假设有来自同一分布的标记训练集 L = {(x1, y1), (x2, y2), ..., (xn, yn)} 和未标记样 本集 U = {x∗1, x∗2, ..., x∗m}，其中 x ∈ Rd 为 d 维的向量，yi ∈ {1, −1} , i = 1, 2, ..., n 为 样本的标签，则有 TSVM 的目标函数表达式为

min
y∗,ξ∗,ω,ξ

1 ∥ω∥2 2

+

C

∑n

ξi

+

C∗

∑ m

ξj∗

i=1

j=1

s.t. yi [(ω · φ (xi)) + b] ≥ 1 − ξi, i = 1, 2, ..., n [( ( )) ]
yj∗ ω · φ x∗j + b ≥ 1 − ξj∗, j = 1, 2, ..., m

ξi ≥ 0, i = 1, 2, ..., n

ξj∗ ≥ 0, j = 1, 2, ..., m

(2.8)

其中 ξi 和 ξj∗ 表示标记样本和未标记样本的松弛变量，C 和 C∗ 表示惩罚系数。此外， y∗ = {y1∗, y2∗, ..., ym∗ } 为未标记样本集的标签预测向量和 ξ∗ = {ξ1∗, ξ2∗, ..., ξm∗ } 为未标记 集中样本的松弛向量。TSVM 作为与 SVM 分类器相结合的分类模型，具有坚实的理 论基础，在机器学习中取得了广泛应用，可参考文献 [28, 29, 34, 38]。
- 19 -

武汉大学博士学位论文

2.2.4 生成模型算法

生成模型算法是假设标记样本数据和未标记样本数据都由同一个潜在的模型生 成，并且这个生成模型的类型是先验已知的，比如高斯混合模型、隐性马尔可夫模型 等 [219]。因此，对于生成模型算法如果采用不同的生成模型类型，其结果会有很大 的差异，如果数据集的分布和生成模型类型相似或者一致，其结果会相对理想，如果 假设的生成模型与真实分布差异非常大，无论如何优化求解都不可能取得较好的结 果。因此，尽管生成模型算法相对简单，但是其方法需要准确的模型假设，这在现实 应用中相对困难。以高斯混合模型假设为例对生成模型算法进行阐述。
假设有标记样本 (x, y),y ∈ Y ，其中 Y = {y1, y2, ..., yC} 为数据集中所有类别的标 签集合，如果样本由高斯混合模型生成，不同的高斯混合分布成分生成不同类别的样 本，那么生成数据的概率密度可以表示为

∑C

∑

p(x) = wip(x|µi, ) i i=1

(2.9)

其中

wi

为第

i

个

高斯

混合

模型

的权重系数，µi

,

∑
i

则分别表示第

i

个高斯混合模

型的均值和协方差矩阵，p(x|µi,

∑ i)

表示

x

的概率。如果记

f (x)

为

x

的预测模型，

Θ = {Θ1, Θ2, ..., ΘC} 为高斯混合模型各个成分的参数，通过最大化后验概率可得

f (x) = arg max p (y = yj|x)
yj ∈Y
∑C = arg max p (y = yj, Θ = Θi|x)
yj ∈Y i=1
∑C = arg max p (y = yj|Θ = Θi, x) p(Θ = Θi|x)
yj ∈Y i=1

(2.10)

其中

∑

p(Θ = Θi|x)

=

wip(x|µi, ∑C

i) ∑

,

wjp(x|µj, j)

j=1

为生成 x 的第 i 个混合模型的后验概率，p (y = yj|Θ = Θi, x) 为第 i 个混合模型生成

样本 x 的标签为 yj 的概率。

由 于 假 设 每 个 类 别 对 应 一 个 独 立 的 高 斯 混 合 模 型，其 类 别 归 属 仅 与 样 本 x

所 属 的 高 斯 混 合 模 型 Θ 有 关，因 此 可 以 用 p (y = yj|Θ = Θi) 代 替。假 定 第 i 个 类别对应第 i 个高斯混合成分，即 p (y = yj|Θ = Θi) = 1 当且仅当 i = j，否则 p (y = yj|Θ = Θi) = 0。

- 20 -

面向小样本问题的主动学习理论及应用研究
通过式 (2.10) 可知，对 p (y = yj|Θ = Θi, x) 进行估计需要知道样本的类别标签， 仅能利用训练样本进行学习，而 p(Θ = Θi|x) 不需要真实标签，可以通过未标记样本 进行估计。因此，将所有数据进行利用，可以提升估计的准确性，达到半监督辅助提 升的目的。
2.2.5 基于图的半监督学习
基于图的半监督学习方法主要是为了保持数据集中的几何结构信息，将数据集中 的样本用图上的节点进行表示，图上的边表示相邻节点样本之间的相似性。对于图的 构建一般过程为
假设有数据集 T ，其包含有标记数据集 L = {(x1, y1), (x2, y2), ..., (xn, yn)} 和未标 记的数据集 U = {(x1, ?), (x2, ?), ..., (xm, ?)}。图的构建是在数据集 T 上即包含有标记 集又包含有未标记集。定义图 G = (E, V ), 其中 E 表示图中节点之间边的集合，V 表示图的顶点集合，对于边的定义一般采用高斯核进行相邻节点相似性的计算

  Wij = 

exp

( ∥xi−xj ∥22 − 2σ2

)

,

i

̸=

j

0, i = j

(2.11)

其中 i, j 为数据集 T 中的第 i 个样本和第 j 个样本的索引，σ 为核带宽参数。假设在 图 G 上，相似的节点具有相似的标签，定义其分类模型为 f (x)，则目标函数可以表 示为

L(f

)

=

min f

1 2

m∑+n

m∑+n

(f

(xi)

−

f

(xj ))2 Wij

( i=1 j=1

)

1 =
2

m∑+n

m∑+n

m∑+n m∑+n

dif 2 (xi) + djf 2 (xj) − 2

Wijf (xi)f (xj)

i=1

j=1

i=1 j=1

m∑+n

m∑+n m∑+n

= dif 2 (xi) −

Wijf (xi)f (xj)

i=1

i=1 j=1

= f T (D − W ) f

(2.12)

其中 W 为数据集 T = [L U ] 中两两样本之间的相似性矩阵，D 为对角矩阵且有 di = m∑+n Wij；f = [fn fm] 为分类模型对标记集和未标记集的预测标签。但是标记集
j=1
- 21 -

武汉大学博士学位论文

标签已知，定义为 Yn，代入到式 (2.12) 中，可得 
L(f ) = min (YnT fmT ) (D − W ) Yn  fm
= YnT (D1:n,1:n − W1:n,1:n)Yn − 2 fmT Wn+1:n+m,1:nYn + fmT (D1+n:n+m,1+n:n+m − W1+n:n+m,1+n:n+m)fm

(2.13)

其中 Kn1:n2,n3:n4 表示选取矩阵 K 中第 n1 行到 n2 行，第 n3 列到第 n4 列。通过对式 (2.13) 的 fm 求导有

fm = (Dn+1:n+m,n+1:n+m − Wn+1:n+m,n+1:n+m)−1Wn+1:n+m,1:nYn

(2.14)

fm 为所有未标记样本的预测标签。 可以看出，图的半监督学习方法在理论和概念上更容易理解和分析，但是其有两
个缺点：首先是在图的构建上，如果数据集足够大，图的存储开销大，计算耗时；其 次是其学习到的模型不能在线更新，对于新数据的加入，必须对图进行重构，才能获 取新加入样本的预测标签。由于图的方法理论充足，近年来受到广泛的重视，开展了 丰富的研究，可参考文献 [6, 91, 105, 215]。
2.2.6 流形正则化
正则化最初是用来解决数学领域的不适定问题，之后引入机器学习领域求解多元 函数的拟合问题 [1]。通过对正则化项的引入，可以保证分类模型的泛化能力。流形 正则化的半监督学习方法则是把保持数据内部几何结构的流形约束看作是正则项进行 调控。文献 [10] 给出了带正则化的学习框架，其表达式为

f∗

=

arg min
f ∈H

1 l

∑l
i=1

V

(xi,

yi,

f)

+

γA||f ||2K

+

γI ||f ||2I

(2.15)

其中 ||f ||2I 是一个近似的惩罚项，它反映了数据集内部的分布结构，是一个与数据集 分布相对应的平滑约束。比如，如果数据概率分布在低维空间的一个流形上，||f ||2I 则是对 f 在流形上的惩罚。γA 为控制函数在标记空间复杂度的系数，γI 为控制函数 在数据内部几何结构上的复杂度。通过加入流形正则化项，在模型训练时，可以要求 相似样本拥有相似的结构化标记输出。流形正则化在半监督学习中应用较多，比如降 维中的流形嵌入 [126]，特征选择 [193]，多标签分类 [112] 等。

- 22 -

面向小样本问题的主动学习理论及应用研究
图 2.1: 迁移学习原理：将源域中的先验知识迁移到目标领域的模型学习中
2.3 迁移学习
迁移学习解决小样本问题的原理主要是从与目标域相关的领域中获取知识，对已 有先验知识进行利用和分析，扩充目标域上可用的标注样本数量，达到分类模型性能 提升的目的，其原理如图 2.1 所示。在迁移学习中，将待进行分类任务的领域称为目 标域，而含有与目标域相关数据的领域称为源域。
按照迁移知识的方式不同，可以将迁移学习大致分为常见的三类方法，基于实例 的迁移学习方法、基于特征空间的迁移学习方法以及基于模型的迁移学习方法。
2.3.1 基于实例的迁移学习方法
图 2.2: 实例权重法工作原理
实例权重法主要是适用于两个领域之间差异不大的情况，其通过对源域中的样本 进行权重的分配，实现对源域数据的利用。实例权重法具有较强的理论基础，其权重
- 23 -

武汉大学博士学位论文

计算一般表示为样本在目标域和源域中概率密度比值即

w

(x)

=

pt(x) 。在迁移过程中，
ps(x)

对具有相同条件概率的样本赋予较大的权重值。图 2.2 展示了实例权重的学习原理。

基于实例的迁移学习方法在自然语言处理 [177]、生物信息提取 [169] 等领域取得了成

功的应用。

2.3.2 基于特征空间的迁移学习方法

图 2.3: 特征表示法原理：将源域和目标域投影到公共空间使其具有相同的分布

特征表示法主要是通过对目标域和源域样本进行变换，寻找一个能体现目标域和

源域分布一致性的空间，实现将源域的样本作用于目标域分类任务的目的。其成立的

假设条件是源域和目标域中可以共享一些特征，原理如图 2.3 所示。特征表示法又可

以分为隐含表征学习法和概率分布适配法两种学习形式。隐含特征表示法主要是通过

分析两个数据域中特征之间的共现关系，在一个隐式的空间进行特征聚合，达到减小

分布差异的目的。显然，如果特征关系不强，这类方法可能导致适配结果不理想。概

率分布适配法一般通过学习一个显式的特征空间 ϕ (x)，使得在新的特征空间中源域

的边缘概率分布和目标域的边缘概率分布一致 [2]。其中最大均值差异是常用的边缘

分布匹配函数，其主要通过最小化两个数据集期望差值距离进行特征空间学习，一般

表示为

dist(Xs′rc, Xt′ar) =

1 n1

∑ n1
i=1

ϕ(x′srci )

−

1 nT

∑ n2 ϕ(x′tari )
i=1

2 H

= dist(ψ(Xsrc), ψ(Xtar))

(2.16)

=

1 n1

∑ n1
i=1

ϕ

◦

ψ(x′srci )

−

1 n2

∑ n2
i=1

ϕ

◦

ψ(x′tari )

2 H

相对于隐式的特征表示法，概率分布适配法的显式表达更加容易进行理论分析，

- 24 -

面向小样本问题的主动学习理论及应用研究
但是由于缺少条件适配的约束，可能使得适配结果在目标域上判别性不强。总的来 说，特征表示在大多数场景中是适用的，并且取得了较好的结果，其相关研究工作可 以参考文献 [13, 134, 142, 218]。
2.3.3 基于模型的迁移学习方法
模型迁移学习方法主要是指寻找源域和目标域中的参数信息来实现迁移的一类方 法。类似于特征表示法需要两个领域中具有共享特征，模型迁移学习则需要两个领域 中的模型参数具有共享信息。图 2.4 展示了模型迁移学习的基本思想。

图 2.4: 基于模型的迁移学习方法

文献 [199]ASVM(Adaptve Support Vector Machine) 是模型迁移的经典方法模型。 假设有目标域标记数据 Dl 和源域数据 Ds，定义在源域 Ds 上训练的分类模型为 f a(x)，目标域上分类模型 f (x)，它们之间存在如下关系

f (x) = f a(x) + ∆f (x)

(2.17)

其中 ∆f (x) = wT ϕ(x) 为源域分类模型的偏置项，w 是需要从 Dl 中学习的参数。为 了对 w 进行学习，基于 SVM 的训练模型有

min w

1 2

∥w∥2

+

C

∑ N

ξi

i=1

s.t. ξi ≥ 0, yif a(xi) + yiwT ϕ (xi) ≥ 1 − ξi, ∀ (xi, yi) ∈ Dl

(2.18)

这里 w 为偏置项线性参数，并非 SVM 的分类模型参数。以此构建拉格朗日等式为

L

=

min w

1 2

∥w∥2

+

C

∑ N

ξi

−

∑ N

µiξi

−

∑ N

αi

( yif

a(xi)

+

yiwT

ϕ

(xi)

−

(1

−

) ξi)

(2.19)

i=1

i=1

i=1

- 25 -

武汉大学博士学位论文

求解 w，可以得到基于源域分类模型，目标域的预测函数为

∑ N f (x) = f a(x) + αiyiK(x, xi)
i=1

(2.20)

适配的决策函数 f (x) 可以看作是源域分类模型 f a(x) 的增强。基于模型迁移的

方法还有很多，比如文献 [181]。

2.4 学习模式对比分析
图 2.5 基于模型要素概括展示了主动学习、半监督学习和迁移学习的学习模式， 从模型要素可以看到，主动学习强调的是样本的查询，是一个人机交互的过程。半监 督学习则主要是在未标记数据上信息的挖掘，迁移学习则是通过源域数据学习到的知 识应用到目标域的分类任务中。它们利用的信息之间不存在冗余情况，因此，三种学 习的相互结合可以形成信息量的互补，对进一步提升解决小样本问题的算法性能具有 重要的作用和意义。

图 2.5: 主动学习、半监督学习和迁移学习流程
近年来，数据的爆发，使得小样本问题日益突出，而深度学习作为人工智能领域 的核心技术，其性能需要大量的标注数据进行保证。因此，既要顾及耗费问题，同时 又要弥补数据不足，满足当下技术发展的需求，这三种技术的研究仍会是未来机器学 习领域研究的热点。
2.5 本章小结
本章介绍了面向小样本问题的机器学习基础方法，包括主动学习、半监督学习和 迁移学习。并就三种学习中的每类方法的基础算法进行了回顾和介绍，同时对每个方 法的优劣性进行了分析。此外，分析对比了三种方法的学习机制。对于半监督学习和 迁移学习，要获得较好的结果，足够的先验知识是关键。而对于主动学习，要想提升
- 26 -

面向小样本问题的主动学习理论及应用研究
性能，更多的辅助信息是保证。因此，主动学习和半监督学习、以及主动学习和迁移 学习相结合，可以充分利用各自方法的优势，提升解决小样本问题的能力，这也是本 文中第五章和第六章研究的内容。
- 27 -

武汉大学博士学位论文
3 结合不确定性和代表性的启发式主动学习框架
不确定性和代表性是主动学习算法设计中的重要准则，而目前很多主动学习方法 往往假设数据满足某种结构，比如半监督假设中的流形结构，缺乏灵活设计有效主动 学习算法的框架。本章节为此建立了一种结合不确定性和代表性的启发式主动学习框 架，对于每一部分都进行特定含义的定义，依据此框架可以建立理想样本查询的主动 学习算法。具体地，在代表性准则中，基于双样本测试理论，利用样本估计数据集的 分布，使查询的样本尽可能代表未标记数据集的分布，而与标记集中的样本差异尽可 能大。对于不确定性，可以建立任何具有特定需求的不确定性衡量，与代表性通过权 重进行连接，同时为了保证查询样本的多样化性，引入一个二次规划项。因此，此框 架基本上涵盖了主动学习进行理想查询的所有条件。基于此框架，本章也设计了一个 主动学习算法，并与一些前沿算法相比，验证了提出框架的有效性。
3.1 引言
随着数据资源的日益增长，人工标记数据的数量有限，因为大量数据的标记需 要耗费巨大的人力、物力和财力，例如，ImageNet [45]。这造成了可用于监督训练 的数据稀缺，严重影响了监督学习方法的效果。但是对于监督学习来讲，标记样本 的数量和质量是制约分类模型学习能力的两大因素 [104]。比如，支持向量机分类器 (SVM) [157]，利用大量标记样本进行训练时，对分类器训练起决定性作用的只有若 干为支持向量的样本，因此，如果查找到 SVM 分类器中为支持向量的样本进行标 记，就可以达到相当于利用大量标记数据进行分类建模的目的，这里相对于大量的训 练样本，支持向量既为高质量的训练样本。对于最大似然分类器 [22]，由于其需要计 算方差，均值等统计量，此时对标记样本的数量需求较多。在机器学习领域，依据监 督学习的需求，发展了半监督学习 [219]、迁移学习 [128] 和主动学习 [145] 三种核心 技术来解决小样本问题。主动学习是三种核心技术中唯一一个构建高质量训练集的方 法，核心思想是以现有少量样本构建分类模型，并以此设计查询规则，从未标记样本 中查询最具信息量的样本进行人工标记，期望查询到的样本能最大限度的提升分类 模型性能。这是主动学习不同于被动学习进行随机样本标注的最大区别，如图 3.1 所 示。主动学习降低了人工随机标注中产生的冗余，提高了标注数据的信息量，在相
- 28 -

面向小样本问题的主动学习理论及应用研究
同人工标注数量下，通过主动学习进行标注的样本质量要显著高于人工随机标注的 样本质量。目前，主动学习已经在推荐系统 [51]、视频 [176]、图像 [11]、自然语言处 理 [151] 等领域中取得了广泛应用。
图 3.1: 主动学习和被动学习的差异：主动学习通过查询函数从未标记集中选取最具信息量样本进行标注，而被动 学习主要是利用人工随机标注的样本
从图 3.1 中也可以看出，主动学习的核心问题是如何从未标记样本中查找到最具 信息量的高质量样本。一般地，主动学习查询函数的设计主要基于两种准则: 不确定 性和多样化性 [42]。不确定性主要是衡量在分类过程中样本被正确分类的可靠程度， 可靠性越低，不确定性越大；多样化性主要是为了去除批处理主动学习中查询到的批 量不确定性样本中的冗余。近年来，一些研究中提出了代表性准则，主要是为了挖掘 数据结构中的分布信息 [36]。代表性准则可以看作是多样化性准则的延申，但是其与 一般的多样化性准则不同。在传统的多样化性定义中，其并不能挖掘未标记样本中的 代表性信息，主要是为了去除少量不确定性样本中的冗余，选取的样本为最不确定的 批量样本中的一个子集。比如文献 [42] 中提出的 MS-cSV 算法，首先根据边缘采样 策略从未标记样本中选取一批最不确定的样本，然后依据 SVM 分类器的支持向量进 行冗余去除，选取到不同支持向量最近的点中最不确定的样本进行标记。相当于是通 过支持向量进行聚类，选取聚类块中最不确定的样本，这个过程依据的仍然是不确定 性的排序。在文献 [178] 中提出了针对多示例的主动学习算法，其仍然是遵循主动学 习中不确定性和多样化性两个准则进行主动学习算法设计。这个方法中首先利用多示 例中的支持向量机模型进行每个多示例包的不确定性衡量，选取一批最具不确定性 的多示例包，然后提出了两种多样化策略进行去冗余，分别是核 K-均值和模糊粗糙 集 [48]。对于核 K-均值，其主要是对不确定性下选取的包进行核 K-均值聚类，再从 每个聚类簇中选取最不确定的包进行标记。这个方法中另一种多样化策略是基于模糊
- 29 -

武汉大学博士学位论文
粗糙集中的下近似，提出了一种新的相似度概念，它测量了一个示例在多示例包中的 唯一性。然后，通过聚合多示例包中所有示例的不相似度来计算每个包的多样性程 度。上述这些方法可以看作是主动学习中不确定和多样化性相结合的一类方法，主要 是通过两步选择的策略，这种策略在主动学习中经常被采用。然而这类主动学习方法 本质上仍然是依赖于少量的标记样本，选取不确定性的样本，可能造成主动学习过程 中样本查询的偏移，降低主动学习的效率和性能。
为了加速主动学习性能，一些主动学习方法用代表性取代多样化性，进行样本在 未标记数据集分布的衡量，促使选择的样本尽可能与未标记样本集具有相似分布，从 而实现训练集与测试集的分布趋于一致，提升分类模型的泛化能力和主动学习的性 能 [36, 194]。一般情况下，依据对数据结构的挖掘方式，可以将主动学习中代表性准 则概括为两种模式，一种是基于聚类的方法，主要是选取距离聚类中心最近的样本 点 [89, 100]。这类主动学习方法其性能的好坏直接受制于聚类的效果。另一种则是基 于优化实验设计的方法，主要通过直推的方式进行代表性样本的查询 [36, 60, 139]。尽 管代表性准则可以有效的挖掘数据内部结构信息，在查询过程中防止查询的样本偏 移，但是由于不确定性信息缺乏致使判别性信息不足，需要查询足够多的样本才能定 位到优化的分类边界。
综上，不确定性准则或代表性准则在进行样本的查询时都存在一定的缺陷，但是 不确定性的查询偏移问题可以通过代表性进行抑制，而代表性查询的样本中判别性不 足问题，可以通过不确定性进行增强，因此，不确定性和代表性的结合自然而然地成 为了主动学习研究中的关注点。目前，已有很多研究工作将不确定性和代表性融合到 同一个主动学习框架中 [20, 52, 81, 86, 184]，通过代表性来挖掘未标记数据中的分布信 息，促使主动学习建立的标记集尽可能与未标记数据具有相似分布，从而进一步提升 主动学习的性能。文献 [149] 提出了多准则的主动学习方法进行自然语言中命名实体 识别，其主要是通过两步走的策略，既先通过不确定性选取一定数量具有最不确定性 的样本，然后通过相似性衡量选取代表性的样本，在选取代表性样本后，再进行样本 的多样化性。这类方法的代表性仍然是基于少量不确定性数据的结构。文献 [86] 将 不确定性和代表性通过权重结合到同一个优化目标函数中，利用最不确定性作为查询 准则，然后基于半监督的设置对主动学习的性能进行提升，既基于未标记样本之间的 局部相似性。此方法在数据结构不满足半监督假设时，结果可能不理想。文献 [52] 提 出了一种凸优化的主动学习方法，主要是通过稀疏表达进行代表性信息的衡量，而把 边缘采样作为每个未标记样本不确定性的衡量，并将其看作是未标记样本稀疏表达中
- 30 -

面向小样本问题的主动学习理论及应用研究
系数的权重，期望不确定性样本加入稀疏表达词典后，能够最佳表达数据集。这个方 法将不确定性和代表性自适应地结合到同一个目标函数式中，但是由于不确定性是稀 疏表达系数的权重，其关注的是不确定性最大的样本，而忽略了那些不确定性低，代 表性信息高的样本，同时如果数据集稀疏表达效果不好，可能造成代表性准则衡量的 不准确。文献 [99] 提出自适应的主动学习方法，将不确定性和代表性通过权重结合 到同一个目标函数式中，并提出了一种自适应选择权重的策略。在文献 [99] 中，主要 基于高斯过程，通过互信息进行更加直接的代表性衡量，而不确定性则利用最大条件 熵，这个方法的计算复杂度较高。文献 [184] 提出了基于经验风险最小化的主动学习 方法进行既具有不确定性又具有代表性的样本查询，主要通过最小化结构风险作为不 确定性的样本查询准则，同时利用最大均值差异最小化标记样本和未标记样本边缘分 布的差异，查询既能进一步减小结构风险和两个数据集分布差异的样本。这个方法有 效的将不确定性和代表性进行了融合，但是由于求解问题非凸，容易获得次优的查询 结果。文献 [201] 提出了针对多类别的主动学习方法，主要是将不确定性和流形结构 进行结合，利用流形结构去除不确定性选择的样本之间的冗余信息。很显然，这个方 法对于流形结构明确的数据会有相对理想的结果。这些方法中，不确定性和代表性都 是通过某种形式连接在一个查询目标函数中，不同与两步选择的策略，其直接通过联 合的准则在一个框架中查询既具有不确定性又具有代表性的样本。代表性准则从一定 程度上依据分布信息去除了查询样本间的冗余，并且这些方法在一些基础数据集上都 取得了较好的结果。但是注意到，这些方法对数据结构有一定的要求或者约束，或者 存在优化过程中选择最优子集问题，缺乏进行有效主动学习算法设计的灵活性。那么 是否可以构建一个无特殊要求同时容易求解的并将不确定性和代表性进行融合的主动 学习框架呢？
为此，本章节提出了一种启发式的主动学习框架，这个框架主要由四部分组成。 第一部分是对要查询样本之间冗余性的定义，第二部分是查询样本对于未标记样本集 的分布情况描述，第三部分是查询样本对于标记集的分布情况描述，第二和第三部分 是确保查询样本能够尽可能实现标记集和未标记集的分布匹配，去除分布冗余。第四 部分则是不确定性的衡量，可以是边缘采样，也可以是专家委员会等其它不确定准 则。这四个方面基本上覆盖了主动学习查询理想样本的信息。在这个框架上，本章节 提出了一种主动学习方法，并在一些公开的常用数据集上进行测试，与一些基础算法 和前沿算法相比，取得了相对较好的结果。
- 31 -

武汉大学博士学位论文

3.2 双样本测试问题

双样本测试问题是由文献 [69] 和文献 [5] 中提出的，其目的是测试两个 p-变量的

概率密度函数是否一致。假设 P = {p1, ..., pm}. 和 Z = {z1, ..., zn} 是从数据源 D 中 抽取的两个数据集，定义 p 和 z 为两个数据集的分布，双样本测试问题则是为了衡

量是否有 p = z。在文献 [69] 中利用多变量概率密度函数基于核的概率估计和真实概

率密度函数的积分平方误差进行分析，其表达式为 ∫
H = (fσ − f )2

(3.1)

其中 fσ 为估计的概率密度函数，σ 为核的相关带宽，以及 f 为真实的概率密度函数。 依据文献 [69]，文献 [5] 中派生出了一个关于 H 的中心极限定理，这个定理和 H 中

fσ 的带宽有一定的条件联系，H 为在假设 f 是真实概率密度函数下进行置信测试时

的一个测试统计量，其将两个样本的分布测试问题表达为 ∫
Hσ1σ2 = (f1 − f2)2

(3.2)

式中，fj 是光滑参数为 σj 时第 j 个样本的密度估计，j = 1, 2。公式 (3.2) 中的 Hσ1σ2 可以作为对 f1 = f2 进行假设检验的统计量，这个检验即为双样本测试问题。通过双 样本的测试假设，可以检测两个样本相对于两个数据集分布的一致性，这和主动学习 中要求未标记集和标记集的分布一致的代表性准则目标相同。因此，可以将双样本测 试问题引入主动学习的代表性准则衡量中。依据文献 [5]，定理3.2.1中给出了概率密 度估计的形式。
定理 3.2.1 定义 {Xj1, ..., Xjnj }, j = 1, 2 是两组独立服从概率密度函数 fj 采样的 数据集，K 为球形对称 p-变量密度函数,σj 为带宽，假设

lim σ1 = 0, lim σ2 = 0, lim n1σ1 = ∞, lim n2σ2 = ∞

n1→∞

n2→∞

n1→∞

n2→∞

那么 p-变量的概率密度估计函数为

∑ ni fi = (niσip)−1 K {(x − Xij) /σi}
j=1

而两个分布之间的差异最小距离为 n−1/2σ−p/2, n = n1 + n2。 依据定理3.2.1，设想已有密度函数 f ，X 和 Z 是两个独立从 f 中采样的数据集，

参照文献 [64]，可以定义双样本问题的分布差异为

∑ m

∑n

f (x) − f (z) = sup (mσp)−1 f {(x − xi)/σ} − (nσp)−1 f {(z − zi)/σ}

i=1

i=1

(3.3)

- 32 -

面向小样本问题的主动学习理论及应用研究
式 (3.3) 中，对于给定的样本 x ∈ X 和 z ∈ Z，其分布差异可以在给定的概率密度估 计 f 下进行，并且具有最小下界。如果最小化这个上界，就可以使 X 和 Z 的分布尽 可能的相似，趋近于下界。如果定义其中的 X 为未标记集，Z 为标记集，就可以将 双样本问题引入到主动学习的代表性衡量中。如果一个样本在未标记集中有分布代 表性，同时又与标记集中的样本不同，那么就可以将其加入到标记集中，并从未标 记集中移除，实现标记集和未标记集分布趋于相似。此外，在主动学习中，标记样本 的数量非常少，因此在有限样本数量下的分布差异特性对于双样本问题应用到主动 学习中非常重要。依据文献 [5] 和文献 [119] 中有偏置的分布差异统计，基于收敛边 界证明了双样本差异在有限的样本数量下也可以有很好的性能保证。通过假设检验， 在有限样本数量下，可以获得两个双样本问题的性质。首先，如果定义 mσp = M 和 nσp = N ，依据 [64]，无论两个数据集的分布是否一样，经验分布差异的概率收敛速 率为 O((M + N )(−1/2))，这是一个与带宽有关的函数，它表明了统计检验的一致性。 其次，当两个数据集具有相同的分布时，在经验分布差异的大偏差中可以得到概率的 边界。通过定理 3.1，这些差异边界都收敛于某个阈值。依据文献 [5] 中 4.1 部分的定 理，收敛速度与带宽具有着一定的联系。更多详细的理论说明可以参考文献 [5]。总 之，双样本问题可以进行主动学习中两个数据集分布差异的衡量。
3.3 结合不确定性和代表性的启发式主动学习框架
假设存在一个数据集 D 含有 n 个样本，在小样本问题下，初始化时，随机的将 n 个样本划分为两部分，一部分为含有 nt 标记样本的训练集 Lt，其余 ut = n − nt 个 样本看作是大量的未标记数据集 Ut，nt ≪ ut。定义 ft 为在训练集 Lt 上获得的分类 器，主动学习的目标是不断从未标记集 Ut 中选取最具信息量的样本 xs 进行标记，使 新的训练集 Lt ∪ xs 具有更强的泛化能力。同时，定义 Y 为样本的标签集。在本章下 文的讨论中，主要采用以上定义的符号。
3.3.1 不确定性和代表性联合框架
本章节设计主动学习算法框架的目的是为了更加灵活的设计主动学习算法来选取 理想的样本。不仅要求选择的样本可以给分类器提供判别信息，而且可以查找到未标 记样本集的数据结构，加速主动学习的收敛，同时期望查询的样本与已有标记集信息 尽可能无冗余以及查询样本间无冗余。这就要求样本既具有不确定性又具有代表性。 如果没有代表性的约束，在主动学习的两次循环中，查询的两个样本对于分类器来讲 可能具有相似的监督信息，查询样本间容易产生信息冗余。同时由于监督信息比较
- 33 -

武汉大学博士学位论文

少，在查询过程中可能会产生偏移，降低主动学习的性能和效率。而代表性信息的加 入，有利于在查询过程中搜索到分布孤立的区域，而且不容易产生查询样本在分布上 的偏移，从而可以实现主动学习效率和性能的提升。如果没有不确定性信息，可能需 要查找到较多的代表性样本才能定位到准确的分类边界。为此，本章节通过权重将不 确定性准则和代表性准则进行结合，建立了一个启发式的的主动学习框架来查询既具 有不确定性又具有代表性的样本。
对于代表性，采用双样本差异进行衡量。双样本差异主要是为了在假设 fˆσ1 = fˆσ2 下测试 Hσ1σ2，以此证明两个分布的相似性。因此，分布差异主要是对概率密度的估 计。在定理3.2.1中，展示了如何通过一个样本 xj 查找一个 p-变量分布的概率密度估 计量 fj。如果利用 xj 查找在数据集 A 和 B 中两个 p-变量分布的概率密度估计量， 那么 A 和 B 的分布差异可以通过这两个估计量进行表示。定义 A 和 B 分别表示主 动学习中的标记集和未标记集，那么对于未标记集中的一个样本可以得到两个概率密 度估计量，进而可以衡量在这个样本下，两个数据集分布的一致性。对于每一个未标 记集中的样本，都可以得到样本在两个数据集上的分布估计量的差异。如果定义样本 xi 在标记集中的分布估计量为 M2(xi)，而在未标记集中的估计量为 M3(xi)，那么在 样本 xi 下，两个分布的差异为

M2(xi) − M3(xi)

(3.4)

在式 (3.4) 中，其值越小，则说明样本 xi 越能促使未标记样本集和标记样本集的分布 一致。如果将这样的样本加入 xi 中，可以有效减小未标记样本集和标记样本集的分 布差异。这和主动学习查询代表性的目的一致，因此可以促进主动学习性能的提升。 而在批量主动学习查询中，同时还要保证被查询样本间的信息无冗余。一般通过一个 相似矩阵对未标记样本的相似性进行两两约束，定义为 M1，其大小为 ut × ut，每个 元素 M1(i, j) 表示未标记样本集中样本 xi 和样本 xj 的相似性。然而在批量查询中， 样本的搜索空间是指数型的，为了能够通过数值进行快速求解，通常引入指示向量 α。α 为一个二值向量，既 αi ∈ {0, 1}，其长度为 ut。在样本查询过程中，如果样本 xi 为目标优化中的最佳查询样本，αi 的值为 1，否则 αi 的值为 0。代表性查询的目 标函数式可以表示为

min αT M1α + αT (M2 − M3) α
s.t. αT 1ut = b, αi ∈ {0, 1}

(3.5)

其中 1ut 是长度为 ut 的全一向量，b 为要查询的样本个数。通过式 (3.5) 可以查询到

- 34 -

面向小样本问题的主动学习理论及应用研究

有代表性的样本。 对于不确定性，一般通过标记样本训练的分类模型进行不确定性的设计，可以基
于边缘采样、专家委员会或者自定义的不确定性准则。这里用一个长度为 ut 的向量 C 表示未标记样本集中每个样本的不确定性，既 xi ∈ Ut, C (xi) ∈ C。将不确定性和 代表性通过一个权重 β 进行结合，得到不确定性和代表性结合的主动学习框架为

min αT M1α + αT (M2 − M3) + βαT C α
s.t. αT 1ut = b, αi ∈ {0, 1}

(3.6)

如果 S 是一个球型的对称密度函数，其值越大越说明两个样本间越具有相似性，

那么 M1, M2, M3 中的元素可以通过以下形式进行定义。对于 (3.6) 中的 M1，其是一 个方阵 M1 ∈ Rut×ut，每个元素定义为

1 M1(xi, xj) = 2 S(xi, xj)

(3.7)

M1 中元素主要定义两个样本的相似性。不同于 M1 中元素的定义，M2 中元素的定 义是衡量一个样本对标记数据集的分布表达情况，表示为

M2(xi)

=

nt + n

b

∑ nt S(xi, xj)

j=1

(3.8)

其中 (nt + b)/n 可以看作是权重，xi 为未标记集中样本，xj 为标记集中的样本。对 于未标记集中的每个样本，都可以衡量其对标记集的分布表达情况。M2(xi) 小，则 说明样本 xi 携带的分布信息可能与标记集中的信息无冗余。相反，如果 M2(xi) 相对 较大，表明 xi 与标记集中的样本具有很好的相似性，可以有效地表达标记集中样本 的分布，此时样本 xi 可能与标记集中的样本存在信息冗余。主动学习期望的是查询 样本与标记的样本之间具有最大差异，实现标记样本信息的多样化。因此，在提出

的主动学习框架中，将 M2 加和到最小化的目标函数中。与 M2 中元素的作用相似，

M3 中元素描述的是未标记样本对未标记集的分布表达，其元素的定义形式为

M3(xi)

=

ut − n

b

∑ ut S(xi, xj)

j=1

(3.9)

式中 (ut − b)/n 可以看作是在未标记集分布上的权重。在目标函数中，用 M2 和 M3 的差值是标记集和未标记集分布差异的衡量。因为 M3 的作用与 M2 相反，主动学习 期望查询的最优样本能够尽可能地表达未标记集上的分布情况，提升分类模型的泛化

能力。M3(xi) 值越大说明样本 xi 对未标记样本集的分布衡量越好，因此在最小化目 标函数时需要向 M3 的反方向进行优化。

- 35 -

武汉大学博士学位论文

综上，M2 和 M3 可以查找到未标记集中具有代表性的样本，实现标记集与未标 记集的分布匹配，而 M1 可以约束查询的批量样本中样本之间的冗余信息。不确定 性可以根据需求依据不同的分类模型进行设计，比如 SVM 或者逻辑回归模型等。因 此，在提出的框架中，只要找到一个合适地表达样本相似性的密度函数 S，以及一个 针对具体分类模型的不确定性准则，就可以建立一个有效的主动学习算法。但是不难

发现，框架中指示向量 α 是一个离散的向量，这是一个 NP-难的问题。为了进行凸 优化求解，将 α 中的每个元素都进行连续的松弛，变为 [0,1] 的连续取值，此时目标 函数式可以表示为

min αT M1α + αT (M2 − M3) + βαT C α
s.t. αT 1ut = b, αi ∈ [0, 1]

(3.10)

式 (3.10) 的求解就变成了一个标准的二次规划问题 (Quadratic Programming, QP)。

在求解得到 α 后，将 α 中最大的 b 个值设置为 1，而其它值设置为 0，贪婪的将 α

从连续值变为整型。α 中 1 值对应的索引即为要从未标记集中查询的样本的位置。

3.3.2 不确定性和代表性联合算法

基于提出的主动学习框架，本节利用样本的类别概率估计提出了一种主动学习算

法。为了得到未标记样本的类别归属概率，本节参照文献 [189] 和文献 [93] 中的估计

方法。最基本的想法就是采用 sigmod 函数进行类别概率的近似。假设 xi ∈ Rd 是一 个样本特征向量，而 yi ∈ {1, −1} 是样本 xi 对应的标签，f (x) 为 SVM 的决策函数， 那么类别的条件概率 P (y = 1|x) 可以近似的表达为

1 P (y = 1|x) = 1 + exp(Af (x) + B)

(3.11)

其中 A 和 B 是需要估计的参数，通常可以采用最大似然估计进行参数的优化求解，

表达式为 式中

∑l min − (tilog(pi) + (1 − ti)log(1 − pi))
(A,B) i=1

(3.12)

1

pi

= 1 +

exp(Af (xi)

+

B)

ti

=



Np+1 Np+2

if yi = 1

1
Nn+2

if

yi

=

−1

,

Np 和 Nn 分别表示训练集中的正样本数量和负样本数量。对于上式可以通过回溯线

搜索的牛顿法进行优化求解获得概率估计值 [102]。

- 36 -

面向小样本问题的主动学习理论及应用研究

注意到 SVM 主要是针对二分类情况，但是也可以通过一对多的形式拓展到多类

别情况，多类下其概率密度估计可以通过成对耦合获得 [189]。假设 rij 是二类下的概 率估值，在多类中，定义第 i 类的概率值为 pi，那么采用成对耦合的形式进行概率求 解可以表示为

1 ∑k min
P2

∑k

(rjipi − rijpj)2

i=1 j=1,j̸=i

∑k s.t. pi = 1, pi ≥ 0

i=1

(3.13)

其中 k 为类别数量。式 (3.13) 的优化问题是凸的，因此可以有全局最小值。采用高

斯消元法或者简单的迭代算法进行求解。在本节中，利用 LIBSVM 工具包进行分类

过程中的概率密度估计 [54]。在获得每个样本的概率密度估计后，进行主动学习中不

确定性和代表性的设计。

对于代表性的设计，提出方法是一个二次规划的凸优化问题，这要求未标记样本

之间的相似性矩阵 M1 是半正定的。而核矩阵通常被广泛用来进行数据间相似性的衡 量，并且保持了目标函数的凸性 [36, 37, 86]。核函数中最常用的高斯核满足概率密度

函数球型对称的要求，因此，这里采用高斯核函数进行两个样本间相似性的衡量。一

般来讲，高斯核函数进行相似度的计算是直接基于特征空间的。然而，在主动学习

中，未标记样本集中的分布和标记样本集的分布随着数据集中数量的改变是不同的。

但是，基于特征进行相似性的衡量，其相似性是基本保持不变的。因此，这样的衡量

可能并不合适。在本节提出方法中，利用样本的类别归属概率通过高斯核函数进行衡

量。类别概率更加直观地代表了样本对于分类模型的重要性，而类别概率的相似性也

更加直观地表现了两个样本对于分类模型的影响是否一致。这样可以促使代表性准则

选取的样本，对于分类模型的作用也更加明显。同时，类别概率来表达数据会使得样

本分布相对于特征表达数据的分布更加集中，促使选取具有信息量的样本更容易。对

于类别归属概率，采用上面描述的形式进行计算。定义样本 xi 属于类别 c 的概率为 pic，P i = {pi1, pi2, ..., pi|Y |} 为 xi 属于各类别的概率，并且有 ∑ |Y | pik = 1，|Y | 为数据集
k=1
上类别的个数。那么对于 xi 和 xj 的相似性 M1(i, j) 可以表达为

M1(xi, xj)

=

1 2 S(xi, xj)

=

1 exp(−γ
2

∗

∥P i

−

P j∥22)

(3.14)

其中 γ 为高斯核的参数。同理，M2 中的元素可以表达为

M2(xi)

=

nt + n

b

∑ nt exp(−γ

∗

∥P i

−

P j∥22)

j=1

(3.15)

- 37 -

武汉大学博士学位论文

M3 中的元素可以表达为

M3(xi)

=

ut − n

b

∑ uu exp(−γ

∗

∥P i

−

P j∥22)

j=1

(3.16)

计算相似性衡量后，对不确定性进行设计和计算。不确定性反映了分类模型对

于未标记样本分类的可靠程度。对于不确定性，本节提出了一种基于 BvSB(Best vs

Second Best) [93] 进行改进的策略。BvSB 是将每个样本类别归属概率最大的两个概

率进行相减，值越小说明样本不确定性越高，归属不同类的概率越相近，那么在进行

分类决策时这样的样本也越容易错分。对于一个样本 xi，定义其在 P i 中最大的类别 概率值为 pih，第二个最大的概率值为 pig，那么 BvSB 可以表达为

diBvSB = pih − pig

(3.17)

BvSB 的主要任务是选取分类界面附近的样本。而本节中采用的分类模型为 SVM，为了增强模型泛化能力，希望训练样本能够使类别间的间隔最大化。因此，需 要查询的样本不仅靠近分类平面，同时也应该靠近支持向量，将靠近支持向量的信息 要求称为位置衡量。基于此，对 BvSB 进行修改以查询更具有信息量的样本。定义 SVM 分类器的支持向量为 SV = {SV1, SV2, ..., SVm}，m 为支持向量的个数，同时定 义 P = {P1, P2, ..., Pm} 为支持向量的类别概率集合。对于未标记集中的每一个样本 xi，构建一个相似性的函数计算样本点到支持向量的距离进行位置衡量

f (xi, SVj) = exp(∥P i − Pj∥22)

(3.18)

如果样本与支持向量越接近，那么 f (xi, SVj) 的值就越小。因此，需要选择式 (3.18) 中最小值既点到所有支持向量中最近的距离作为样本在 SVM 分类间隔中的位 置衡量。此支持向量可以通过下式进行查找

SVc = arg min (f (xi, SVj))
SVj ∈SV

(3.19)

由于对 BvSB 改进的目标是不仅要增强分类器的分类平面同时也要增大分类器的

分类间隔，因此，这里将 BvSB 的衡量和位置衡量进行结合作为新的不确定性准则

C(xi) = diBvSB ∗ f (xi, SVc)

(3.20)

通过对 C (xi) 最小化，可以增强 BvSB 的不确定性。图 3.2 中展示了 BvSB 和提 出的不确定性准则选择样本的区别。从图中可以看出改进的 BvSB 方法主要是为了尽 可能扩大分类平面之间的间隔，使得类别之间分离性更强。

- 38 -

面向小样本问题的主动学习理论及应用研究
算法 1: 不确定性和代表性结合算法 (UR, Uncertainty and Representativeness) 输入: 标记样本集 Lt，未标记样本集 Ut，查询样本数量 b，权重系数 β，停止条件 δ. 输出: 标记样本集 L for 未满足停止条件 do 采用 LIBSVM 计算 Lt 和 Ut 中的样本类别归属概率; 通过类别归属概率基于 (3.14),(3.15),(3.16) 计算 M1, M2, M3; 依据 (3.17) 计算未标记样本集中每个样本的 BvSB 不确定性; 基于 (3.19) 查找未标记样本集中每个样本的最近支持向量, 并依据 (3.20) 计算提出的 不确定性准则; 利用 QP 工具包，求解主动学习目标函数得到指示向量 α，进而得到相应的查询样本; 更新标记样本集 Lt 和未标记样本集 Ut;
图 3.2: 红色的三角形和蓝色的四边形代表两个类别，绿色的点代表未标记样本，黑色矩形框表示查找的样本点。 左侧图表示原始的训练模型，中间图表示 BvSB 查找的样本点，右侧图表示改进的不确定性方法查找的样本点。
为了对提出算法的流程更加清晰，在算法 1 中对本章提出得算法主要步骤进行描 述。
3.4 实验及分析
本节对实验过程中参数设置和实验数据划分等进行详细的介绍，并对实验结果进 行分析对比。通过系统的实验设计，验证了提出的框架和方法的有效性。
3.4.1 实验数据及实验设置
本节对提出方法在 15 个 UCI 数据库的数据集上进行实验和验证。UCI 数据库 是机器学习中被广泛采用的基础数据库，包含了图片、文本、生物、医疗等众多领 域数据 [90]。15 个数据集主要包括 australian, sonar, diabetis, german, heart, splice, image, iris, monk1, vote, wine, ionosphere, twonorm, waveform, ringnorm。在这 15 个 数据集中 sonar、iris、wine 和 waveform 为含有 3 中类别的数据集，而其它数据集为
- 39 -

武汉大学博士学位论文

二类数据集。这 15 个数据集的特征数、类型等信息如表 3.1 所示。

表 3.1: UCI 数据集信息统计

数据集名称 australian
sonar diabetis german
heart splice image
iris monk1
vote wine ionosphere twonorm waveform ringnorm

特征数量 14 60 8 20 13 60 18 5 6 16 13 34 20 21 20

样本数量 690 208 768 1000 270 2991 2086 150 432 435 178 351 7400 5000 7400

类别数量 2 3 2 2 2 2 2 3 2 2 3 2 2 3 2

同时，本章节选择了几种基础算法和前沿算法进行对比，列表如下： 1) 随机策略 (Random): 随机选择策略是主动学习算法进行验证最有效的策略，是对
比算法中最基础的算法； 2) 文献 [183] 中基于经验风险最小化提出的 BMDR 主动学习算法，通过最小化结构
风险进行样本不确定性的衡量，同时最小化最大均值差异的上界查询独立同分布 的样本实现标记样本集和未标记集的分布相似性，两者通过权重进行结合来查询 既具有判别性又具有代表性的样本； 3) 文献 [86] 中基于主动学习的最小 -最大框架 [68] 提出了 QUIRE 主动学习算法，主 要是通过预测标签进行结构信息的保持和衡量，通过最小二乘法进行不确定性的 衡量，从而在半监督假设下查询既具有不确定性又具有代表性的样本； 4) 文献 [36] 中基于最大均值差异提出的 MP 主动学习算法，其主要是通过最小化标 记样本集和未标记样本集之间的分布差异，选择能够进一步减小两个数据集分布 差异的样本加入到标记集中，MP 算法是仅通过代表性进行查询的主动学习算法； 5) 文献 [161] 中提出的 MARGIN 主动学习算法，通过点到分类平面的距离进行不确
- 40 -

面向小样本问题的主动学习理论及应用研究
定性的衡量，查询到分类面距离最近的样本进行标记，MARGIN 方法是主动学习 中常用的经典策略； 6) 本章节提出的主动学习算法，表示为 UR(Uncertainty and Representativeness)。
在实验过程中，对每个数据集都随机地划分为 60% 和 40% 的两部分，其中 40% 的部分用来测试，60% 的部分用来训练。而训练部分又分为两部分进行主动学习算法 的学习，即标记集和未标记集。在初始状态下，从每个类别中随机选取一个样本作为 标记集，而剩余的数据则作为主动学习查询的候选样本集。对于查询停止准则，当主 动学习算法在每个数据集上查询到一定的比例时停止。
对比算法中的参数，尽可能的采用算法在原始文献中的参数。在对比算法中， BMDR 和 QUIRE 方法是将不确定性和代表性通过权重形式进行结合的算法，而提 出算法也存在相同的权重参数。实验中，参照文献 [183]，权重参数的选择通过遍历 的形式进行，参数选择的范围为 [1, 2, 10, 100, 1000]。事实上，QUIRE 的参数在原文 中设置为 1，这里为了实现公平对比，参数选择形式与提出算法一致。对于分类模 型，所有的实验算法均采用基于 LIBSVM 工具包的 SVM 支持向量机分类器 [32]，核 函数为高斯核，其惩罚系数设置为 100，核函数参数为 0.05。在实验中，对于二次规 划的求解则使用 Mosek 工具包。此外，对于每种实验算法在每个数据集上都进行 10 次独立实验。
3.4.2 实验结果及分析
图 3.3 中展示了提出算法和对比算法在 15 个基础数据集上的平均结果。从实验 结果中可以看到，在所有数据集上，提出方法与对比方法相比在主动学习的大多数 循环中都获得了相对较好的结果，BMDR 次之。这可能是由于 BMDR 方法的不确 定性衡量是基于自定义的线性核分类模型，产生了衡量误差，同时其结果是次优查 询，而提出算法的不确定性是基于 SVM 分类模型的，对分类器更具有针对性。此外， BMDR 和 QUIRE 都是结合不确定性和代表性的主动学习方法。然而，BMDR 的结 果在所有数据集上相对于 QUIRE 的结果要更好，这可能是因为 QUIRE 进行样本查 询的前提条件是数据结构满足半监督假设，其通过未标记样本的预测标签进行流形结 构的表达来查询代表性的样本。因此，当数据结构不满足假设时，就会造成 QUIRE 性能的下降。MARGIN 和 MP 是单准则主动学习方法，其中 MARGIN 主要衡量不 确定性，而 MP 方法则主要是代表性信息的衡量。MP 方法和 MARGIN 方法的结果 相比，在 splice、image、waveform 和 ringnorm 上差异不大，在 sonar 和 ionosphere 上 MP 结果相对较好，在其它数据集上 MARGIN 相对较好。而两者结合的方法结果
- 41 -

武汉大学博士学位论文
图 3.3: 不同主动学习算法在 15 个基础数据集上的对比；每条曲线表示在主动学习中 10 次独立实验的平均结果。
- 42 -

面向小样本问题的主动学习理论及应用研究
在多数情况下要比单个准则结果好，这说明两者结合对于主动学习性能的提升非常重 要。同时，上述结果也证明了提出方法的有效性和提出框架的实用性。

表 3.2: 10 次独立实验中在 95% 置信区间下 UR 方法和对比算法配对 t-检验的显著性差异检测 Win/Tie/Loss 统计结果

数据集名称 Vs BMDR Vs QUIRE Vs MP MARGIN Vs RANDOM

australian 36/64/0 76/24/0 81/19/0 86/14/0

85/15/0

sonar

54/46/0 80/20/0 72/23/5 73/26/1

77/21/2

diabetis 74/21/5 95/5/0 97/3/0 97/3/0

97/3/0

german

31/67/2 61/39/0 74/26/0 85/15/0

81/17/2

heart

40/58/2 54/44/2 73/27/0 67/31/2

64/36/0

splice

73/24/3 100/0/0 100/0/0 100/0/0

100/0/0

image

60/38/2 81/15/4 100/0/0 98/2/0

95/5/0

iris

64/33/3 77/23/0 67/30/3 22/73/5

37/60/3

monk1

74/26/0 93/7/0 100/0/0 100/0/0

99/1/0

vote

50/48/2 84/14/3 75/25/0 55/42/3

80/19/1

wine

28/70/2 41/55/4 49/49/2 33/67/0

72/28/0

ionosphere 55/42/3 91/8/1 81/17/2 89/8/3

75/24/1

twonorm 0/96/4

94/6/0 97/3/0 85/15/0

100/0/0

waveform 36/44/20 100/0/0 100/0/0 100/0/0

100/0/0

ringnorm 83/17/0 100/0/0 100/0/0 100/0/0

100/0/0

3.4.3 参数敏感性分析
在不确定性和代表性结合过程中，权重系数是算法中重要的参数，其揭示了主动 学习过程中信息的需求。为了分析提出方法对权重系数的敏感性，在权重系数的选 择范围为 [1, 2, 10, 100, 1000] 时进行参数分析，并在三个 UCI 数据集 balance、breast cance 和字符数据集 semeion handwritten digit 上进行实验。在本节实验中，除了对 权重系数进行不同值的固定外，其余设置与上节实验相同。
从图 3.4 中可以看出，在不同数据集上权重系数敏感程度不同。在数据集 balance 和字符数据集上，其敏感度没有在 breast cancer 上明显。而且通过观察可以发现， 在三个数据集的结果中，balance 和字符数据集在权重系数较大时取得了较好的结果， 而在 breast cancer 数据集上则相反，在权重系数较小时其结果相对较为理想。这可 能是因为在 balance 数据集和字符数据集上，不同类别间数据分布差异相对较大，具
- 43 -

武汉大学博士学位论文
图 3.4: UR 方法在 3 个 UCI 数据集上不同权重参数下的结果。每条曲线都表示 10 次独立实验的平均结果。
有判别性的不确定性信息可能更能增强分类边界。而对于 breast cancer 数据集来讲， 其类别内的分布可能不规则，不确定性信息很难查找到准确的分类边界，因此代表性 信息是查询过程中的主导信息。综上，数据的分布形态对权重系数具有重要的影响， 当数据分布相对规则时，可以采用较大的权重系数。当数据分布相对离散时，可以采 用较小的权重系数。但是其值为 2 和 100 时，在三个数据集上综合结果最好，可以 在此范围设置权值。
3.5 本章小结
不确定性和代表性是主动学习中常用的两个准则，是设计主动学习算法进行高质 量样本查询的重要思想依据。而目前很多研究都验证了单一的主动学习准则，在不同 的数据集上都有其优缺点，本章节中的 MARGIN 和 MP 实验结果也表明了单一主 动学习准则的缺陷。因此，很多研究开始聚焦于不确定性准则和代表性准则的联合 学习。为了实现这两个准则的有效联合，提出了一系列准则表达形式，比如稀疏表 达 [52]，流形假设表达 [86]，进行两种准则的联合。然而当这些表达的前提假设条件 不满足时，势必会降低主动学习的性能。因此，如何建立有效的主动学习框架，进行 灵活有效的主动学习设计具有重要的意义。
为此，本章节中提出了一种结合不确定性和代表性准则的启发式主动学习框架， 可以实现主动学习算法在不同数据领域或者分类任务上的灵活设计。同时，基于提出 的主动学习框架，在基于 SVM 分类器的基础上，设计了一个主动学习算法。为了验 证提出算法的有效性，进而证明框架的实用性。在机器学习领域被广泛应用的 UCI 数据库中的 15 个基础数据集上进行了实验测试，与主动学习中基础的单准则算法和 两者结合的一些前沿算法相比，提出的方法在大多数情况下都取得了较好的结果。
然而，在框架中不确定性和代表性是通过固定设计的形式经权重连接在一起的，
- 44 -

面向小样本问题的主动学习理论及应用研究
权重的自适应调节将会是在未来工作中的一个重要研究方向。
- 45 -

武汉大学博士学位论文
4 鲁棒的多标签主动学习方法
多标签中异常低相关标签严重影响了分类模型的判别能力，并且对于两个具有相 同标签集合的样本，由于它们的异常低相关标签不同，会造成两个样本的特征表达差 异很大。因此，异常低相关标签的存在使得主动学习中不确定性准则和代表性准则很 难在多标签中进行准确衡量，直接忽略异常低相关标签可能会造成模型的失真。目前 的一些多标签主动学习方法，为了避免异常低相关标签的影响，只查询其中的一个最 强相关的样本 -标签对或者相关次序的标签对。但是并不是所有的低相关性标签与样 本关联性都非常低，其低相关性是相对的，只采部分相关标签会造成标签相关性信息 的丢失。为了抑制异常低相关标签对主动学习准则的影响，本章基于最大相关熵准则 (Maximum Correntropy Criterion, MCC) 提出了鲁棒的多标签主动学习方法，主要是 利用最大相关熵准则的有界性，在建立模型时，最大化目标函数，可以使异常低相关 标签对模型的影响趋近于零，而强相关标签的影响最大。同时将标签集的相似性作为 相似性衡量的策略之一结合到特征相似性衡量中。与若干前沿的多标签主动学习方法 相比，提出方法性能具有显著的提升。
4.1 引言
多标签的标注相对于单标签而言，其标注耗费呈指数型增长。因为在对单标签数 据的标注中，如果有 y 个类别，那么对一个样本的标注空间为 y。而对于多标签样 本，如果一个多标签数据集存在 y 个标签，那么对一个多标签样本的标注空间为 2y。 因此，主动学习在多标签中的应用也显得更加具有价值和意义。由于多标签中一个样 本对应多个标签，如果按照单标签的模式建立主动学习准则，必然导致不确定性和代 表性衡量的不准确。这是因为多标签中，样本与标签之间存在强弱关联性，样本与标 签之间不是一一对应关系，相同标签的样本，它们之间的特征差异可能大，因此，对 分类模型的判别能力和样本相似性都会产生影响，导致主动学习准则很难进行准确的 衡量。
为了更加了解多标签主动学习准则建立的困难，下面通过实例进行详细的阐述和 说明。图 4.1 展示了由于标签与样本关联性的强弱不同，每类标签建立独立的二分类 模型时，对模型判别性的影响。从图 4.1 中，可以直观地看出，训练图像中树与样本
- 46 -

面向小样本问题的主动学习理论及应用研究
图 4.1: 低相关性标签在建立分类器时的影响
的相关性比大象强，大象与图像的相关性比狮子强，其特征信息与标签的强弱成正相 关。当使用这幅图像去训练狮子的分类模型时，由于输入特征中大部分信息是树的特 征，这样的分类模型对狮子必然是弱判别能力。但是需要注意到标签与样本的相关性 是相对的，并不是弱相关性标签就一定会弱化模型的判断能力。例如，一幅图像上， 只有两个动物大象和狮子，大象在图像上的比例为 1/2，而狮子的比例为 1/3，这两 个动物相对于这副图像而言都具有显著的相关性，大象的相关性要比狮子的相关性 强，但是对于这幅图像来讲，相对于大象的强相关性，狮子是弱相关性标签，然而狮 子对于提升狮子分类器的判别仍然会有帮助。因此，多标签中对分类模型判别能力影 响的标签是相关性异常低的标签，本章将这种标签定义为异常标签。
为了避免混淆异常的概念，本章定义了多标签中异常标签的两个属性。假定有一 个多标签样本的标签对为 (x, y1, y2)，y1 是其中一个相关性标签，y2 是与样本 x 最强 相关标签, y2 与样本 x 相关性比 y1 强很多，那么就认为 y1 异常标签。这两个属性 中，第一个性质定义了异常标签与样本的关系，表明了异常标签与样本有一定的相 关性，第二个性质定义了异常标签与样本关联的相对性。异常标签示意图如图 4.2 所 示。事实上，这种异常标签的定义与文献 [85] 中的标签类型是相一致的。
图 4.2: 异常标签的两个性质。左：异常标签与图像相关性；右：异常标签与图像的相关性比最强相关性标签弱很 多的相对性。
- 47 -

武汉大学博士学位论文
异常标签不仅影响了分类模型的判别能力，同时对多标签样本相似性的衡量也有 很大影响。对于两幅具有相同标签的影像，当它们的异常标签不同，在特征提取过程 中，标签对应的特征信息也就会有很大差异。因此，基于特征相似性进行多标签样本 间的相似性衡量是不准确的。图 4.3 给出了一个简单的例子对异常标签影响相似性进 行更加形象的说明。图片 1 和图片 2 具有相同的标签，表达的是同一场景，图片 3 中含有图片 1 和图片 2 中的部分标签草地和大象。但是在图片 3 中，草地和大象都 是强相关性标签，图片 2 中草地和大象也是强相关性标签，图片 1 的强相关性标签 为草地，其明显的异常标签为狮子，图片 2 中显著的异常标签为树和狮子。因此，在 基于尺度不变特征（SIFT）[111] 进行相似性衡量时，尽管图片 1 和图片 2 之间的标 签完全一致，图片 2 和图片 3 的特征相似度要比图片 1 和图片 2 之间强。因此，异 常标签对于相似性的衡量是不容忽视的。
图 4.3: 异常标签对于相似性衡量的影响
综上，异常标签的存在，对多标签数据分类模型的判别能力和相似性衡量有着很 大的负作用。一般而言，解决多标签中这种标签的强弱相关性问题，主要是利用标签 之间的相关性，对标签进行加权来提升分类模型的判别能力 [211, 212]。然而，在小 样本条件下，多标签样本的数量有限，标签之间的相关性很难通过标签统计的方式学 习到。一些多标签主动学习方法中仍然将多标签问题看作是多个独立的二分类问题进 行不确定性的计算 [98, 190, 198]。这些方法在进行建模时，并没有考虑到异常标签的 影响。而有些研究为了避免查询的样本出现异常标签，则从多标签中选择样本 -标签 对或者具有相关次序的标签对进行标记 [85–87, 135, 207]。这些方法只标记部分标签，
- 48 -

面向小样本问题的主动学习理论及应用研究

虽然避免了异常标签的查询，但是标签的部分标记削弱了标签之间的关联性信息。比 如两个关联性很强的标签，在两个相似的样本上其相关性相近，但是次序相反，如果 只查询一个标签，那么这两个相似的样本就会被标记上不同的标签，这样就造成标签 信息学习的不稳定。因此，受异常标签的影响，多标签主动学习中不确定性和代表性 很难进行准确的衡量，造成查询函数不能查找到多标签中最具有信息量的样本进行标 记。
为了解决异常标签在主动学习不确定性和代表性准则的衡量中引起的偏差，本章 提出了鲁棒的多标签主动学习方法，主要是基于最大相关熵准则。利用最大相关熵准 则的有界性，对主动学习的查询函数优化过程中，发挥强相关性标签的在目标优化中 的主导作用，抑制异常低相关性标签的负面影响。本章的主要贡献如下：
1）基于最大相关熵准则，提出了一种鲁棒的多标签主动学习方法。由于最大相 关熵准则对异常数据具有很好的鲁棒性，将最大相关熵准则引入到多标签的模型中， 在考虑所有标签的情况下，对异常标签可以自适应的抑制；
2）基于最大相关熵准则衡量未标记样本之间的标签相似性可以对样本间的特征 相似性衡量进行纠正，缓解了异常标签在特征相似性衡量的影响，使多标签样本的相 似性衡量更加准确；
3）在分类模型的学习中增加了未标记样本的结构信息，基于最大相关熵准则的 有界性，使多标签不确定性的衡量更加的准确。

4.2 最大相关熵准则理论
由于本章中多标签主动学习方法主要是基于最大相关熵准则理论，因此本小节对 最大相关熵准则的理论进行详细介绍。最大相关熵准则最先是在信息论里面被提出的 重要理论，在信号处理和机器学习领域被广泛用来进行鲁棒分析，对处理异常点具有 很好的效果 [55, 76]。相关熵主要是用来定义两个随机变量 a 和 b 之间的相似性，其 表达形式为

Vσ (a, b) = E [Kσ (a, b)]

(4.1)

其中 Kσ(·) 是满足 Mercer 理论的核函数，E [·] 表示期望。由于相关熵定义在核函数 上，因此它与核方法具有相同的优势。然而，不同于传统基于核的方法，最大相关 熵准则保证了每个样本的相互独立特性，有坚实的理论基础 [106]。基于这样的定义， 可以保证相关熵具有对称、非负和有界等特性。
然而，在许多现实应用中，a 和 b 的联合概率密度函数是未知的，同时 a 和 b 中
- 49 -

武汉大学博士学位论文

的数据量是有限的。如果我们定义 a 和 b 中可用的数据量为 m，同时两个数据表示

为 {ai, bi}mi=1，那么 a 和 b 的相关熵估计可表示为

1 ∑ m

Vm,σ = m

Kσ(ai, bi)

i=1

(4.2)

其中 Kσ(ai, bi) 是高斯核函数 g (ai, bi) = exp(−∥ai − bi∥2/2σ2) 。依据 [88]，相关熵和

M-估计量具有更加密切的关系，M-估计量是文献 [88] 中提出的一种广义的最大似然

估计方法用于估计损失函数

min

∑N ρ (ai

− bi|θ)

中的参数

θ，其中

ρ

是满足一定条件

θ i=1

下的积分函数。高斯核符合 ρ 中所有的条件，a 和 b 的最大相关熵准则可以表示为

max p′

1 m

∑ m
i=1

g

(ai,

bi)

(4.3)

p′ 是最大似然中的辅助参数等同于 θ。最大相关熵准则与均方误差相比，均方误差表

示一个全局的测度，而相关熵表示的是一个局部的测度，这表示相关熵是沿着直线

a = b 计算的，并且属于 M-估计量，可以为噪点的处理提供有效的解决方案。

4.3 鲁棒的多标签主动学习方法

为了方便理解和表达本章节提出的方法，首先对本章中要使用的基本符号进行定

义。定义 D = {(x1, y1), (x2, y2), ..., (xn, yn)} 为含有 n 个样本和 C 个标签的多标签数 据集，yi = (yi1, yi2, ..., yiC) 为样本 xi 的标签，yik = {1, −1} , k = 1, 2, ..., C。在数据集 划分中，定义 L = {(x1, y1) , (x2, y2) , ..., (xl, yl)} 为 l 个标记样本，yL = {y1, y2, ..., yl} 为 L 中 所 有 样 本 的 标 签 矩 阵，大 小 为 {1, −1}l×C 。将 剩 余 的 u = n − l 个 样 本 U = {xl+1, xl+2, ..., xl+u} 定义为主动学习的候选集。此外，主动学习的目标是从 U 中 查询一个最具信息量的样本进行标记，定义为 xq。同时由于本章采用的最大相关熵 准则对异常标签有很好的抑制作用，因此分类模型采用多个二类进行建模，定义为

f = {f1, f2, ..., fC}。 主动学习中不确定性一般通过标记集进行表达，代表性信息主要是未标记集和标

记集的联合表达。本章基于最大相关熵准则提出了将不确定性和代表性融合的多标签

主动学习方法，其数学表达式可以表达为关于分类模型和查询样本 xq 的优化问题， 表示为

∑ ∑C

∑C

arg max

M CC (yik, fk(xi)) − λ ∥fk∥2H + βM CC (L ∪ xq, U /xq, yL, yU )

xq,f xi∈L∪xq k=1

k=1

(4.4)

其中 H 为再生核希尔伯特空间，∥fk∥2H 为约束分类器复杂度的正则项，M CC (·) 为

- 50 -

面向小样本问题的主动学习理论及应用研究

最大相关熵准则的损失函数，yU 为未标记样本的伪标签矩阵，β 为权重。由于待查 询样本 xq 的标签未知，其标签也需要通过伪标签进行表示，函数式（4.4）可以表示 为

∑ ∑C

∑C

∑C

arg max

M CC (yik, fk(xi)) + M CC (yqk, fk(xq)) − λ ∥fk∥2H

xq,f xi∈L k=1

k=1

k=1

+ βM CC (L ∪ xq, U /xq, yL, yU )

(4.5)

yqk 表示 xq 的第 k 个类别的伪标签。如果 xq 中可能有第 k 个类别，yqk 为 1，否则 其值为 -1。式 (4.5) 中前三项可以看作是不确定性的衡量，最后一项则为在标签空间 和样本空间代表性信息的衡量。最后一项不仅可以表达代表性信息，同时对不确定信

息的衡量也有很强的辅助作用。下面对 (4.5) 进行更加详细的阐述。

4.3.1 多标签不确定性衡量

最小边缘采样是主动学习方法中最常用的经典方法之一，其主要通过计算样本和 分类界面之间的距离来衡量不确定性 [8]，表达形式为

xq = arg min |f ∗(xi)|
xi∈U

(4.6)

其中 f ∗ 为分类模型，在二类问题中可以基于监督学习通过下列目标函数获得

∑ f ∗ = arg min ℓ(Yi, f (xi)) + λ ∥f ∥2H
f ∈H xi∈L

(4.7)

其中 ℓ(·) 为损失函数，Yi 属于 {1, −1}。根据文献 [78, 86]，命题4.3.1可以实现边缘采 样与主动学习的最小 -最大框架关联。

命题 4.3.1 最小边缘采样准则从 x ∈ U 中查找最具信息量样本可以表达为

∑

(

)

x∗

=

arg min
xj ∈U

max min
Yj ∈{1,−1} f ∈H

xi∈L

ℓ

(Yi,

f

(xi))

+

λ

∥f ∥2H

+

ℓ

Yj, f (xj)

(4.8)

其中 Yj ∈ {1, −1} 为 xj ∈ U 的伪标签。 一般情况下，损失函数 ℓ(·) 采用平方损失作为损失函数，但是平方损失对于产生

的异常值不具有鲁棒性。因此，本章采用了最大相关熵准则作为损失函数，(4.8) 可

以表示为

(

)



2

∑ xq = arg max max max exp
xj ∈U Yj ∈{1,−1} f ∈H xi∈L

−

∥Yi

− f (xi)∥2 2σ2

−λ ∥f ∥2H+exp −

Yj − f (xj) 2σ2



(4.9)

- 51 -

武汉大学博士学位论文

为了求解目标函数式 (4.9)，定义 Yj = −sign(f (xj))，则目标函数式 (4.9) 可以

转化为最大化其最小值下的目标函数

arg max
xj ∈U,f ∈H

∑ exp
xi∈L

( − ∥Yi

) − f (xi)∥2
2σ2

−

λ ∥f ∥2H

+

((

exp

1 −

+

2

|f

(xj )| 2σ2

+

f (xj)2) )

(4.10)

由于最大相关熵准则可以很好的抑制异常标签的影响，因此在把最小边缘采样拓

展到多标签的过程中，本章通过最简单的形式，对每个标签建立二类分类器，多标签

的不确定性表达为

∑C

arg min

fk* (xj)

xj ∈U k=1

(4.11)

其中 fk* 表示第 k 个标签的分类器，依据目标函数式 (4.10)，目标函数式 (4.11) 的多

标签学习表示为

(

)

arg max
xj ∈U fk ∈H:k={1,2,..C }

∑ ∑C exp

−

∥yik

− fk (xi)∥2 2σ2

xi∈L k=1

∑C − λ ∥fk∥2H
k=1

+

∑C

exp

( −

( 1

+

2

|fk

(xj )| 2σ2

+

fk

(xj

)2)

)

k=1

(4.12)

4.3.2 多标签中代表性衡量

代表性信息是对不确定性信息的重要补充，因为不确定性衡量过程中采用的标记

样本数量有限，使得样本中的结构信息对于提高主动学习具有重要的作用。在多标签

中，由于异常标签的影响，多标签数据中基于特征进行相似性的衡量，已经不足以作

为两个多标签样本具有相似性的标准。而且对于一个多标签样本，其标签数量越多，

异常标签同样也会越多，特征相似性的衡量也就越不准确。如果将特征作为相似性衡

量，可能会选择到那些异常标签多的样本，事实上，这些样本的信息量是有限的。为

了避免异常标签的影响，本章用未标记样本的预测标签，在标签空间和特征空间对多

标签样本之间的相似性进行协同表达，并且标签空间的特征通过最大相关熵准则进行

衡量，新的相似性衡量表示为

(

)

sij = exp

−

∥yi

− 2σ

yj
2

∥22

wij

(4.13)

其中 yi 和 yj 为样本 xi 和样本 xj 的预测标签，wij 表示两个样本之间的特征相似性， 用高斯核函数进行衡量 wij = exp(−||xi − xj||2/2σ2)。同时由于在相似性衡量中预测 标签需要用分类模型预测，因此，不确定性的衡量通过结构信息进一步增强。定义

- 52 -

面向小样本问题的主动学习理论及应用研究

S = [sij]u×u 表示未标记样本之间对称的相似矩阵，表示为

 



Su×u = ss...T1T2  = ss12...11

s12
s22 ...

...
... ...

s1u
s2u ...



→

Ru×u

sTu

su1 su2 . . . suu

(4.14)

对于相似性矩阵 S，如果样本 xi 和样本 xj 比较相似，而和样本 xt 不相似，则 会有 sij ≫ sit。因为最大相关熵准则的测量使得标签稍有差异，其标签相似性会有很 大差异，从而可以使得在新的相似性衡量下，相似性样本和非相似性样本之间的相似

度值差异很大。因此，代表性样本可以很好的表示未标记样本集。为了表示代表性，

参考文献 [52] 引入辅助变量 pij ∈ [0, 1] 表示样本 xi 能够表达样本 xj 的概率，从而可

以构建一个概率矩阵

 



Pu×u = pp...T1T2  = pp...1211

p12
p22 ...

...
... ...

p1u
p2u ...



→

Ru×u

pTu

pu1 pu2 . . . puu

(4.15)

由于相似性值的差异很大，定义 pi 为 1u，如果样本 xi 为要查询的代表性样本，

1u 表示 u 长度的全 1 向量；否则 pi 为 0u，0u 表示 u 长度的全零向量。显然，pi 和

si 的内积为样本 xi 和未标记样本集之间相似度的和。因此，查询样本 xq 和未标记集

的相似性可以表示为

∑

∑

sijpij =

sqj pqj

xi,xj ∈U

xq;xj ∈U

(4.16)

同理，可以定义 d = [dij]u×l 和 z = [zij]u×l 分别为未标记样本集和标记样本集之

间的相似性矩阵和概率矩阵，那么查询样本 xq 和标记集之间的相似性可以表示为

∑

∑

dijzij =

dqj zqj

xi∈L,xj ∈U

xq;xj ∈L

(4.17)

为了实现查询样本信息最大化，不仅要选择能够代表未标记样本集的样本，同时

要保证查询样本和标记集的样本差异最大。因此，查询样本在未标记集和标记集上的

代表性信息描述是相反的，通过最大化 (4.16) 和 (4.17) 之间的差异进行代表性的衡

量

∑

∑

max

sijpij −

dij zij

xi∈U

xj ∈U

xj ∈L

(4.18)

- 53 -

武汉大学博士学位论文

由于标记样本数量和未标记样本数量差异很大，采用两个样本集上的期望，并通

过权重进行两部分信息的调节







max E [xi ∈ U, U ] − β0E [xi
xi∈U

∈ U, L] = max xi∈U

1 u

∑

sijpij − β0

1 l

∑ dij zij 

(4.19)

xj ∈U

xj ∈L

4.3.3 多标签主动学习方法

为了实现对不确定性准则和代表性准则的增强，本章提出的多标签主动学习中将

不确定性和代表性通过权重进行结合，目标函数式为

(

)

arg max

∑ ∑C exp

xj ∈U,fk∈H:k={1,2,..C} xi∈L k=1

+

∑C

exp

( −

( 1

+

2

|fk

(xj )| 2σ2

+

−

∥yik

− fk (xi)∥2 2σ2

fk(xj

)2)

)

+

β1

 

1 u

∑C − λ ∥fk∥2H

k=1 



∑

sij pij 

−

β2

1 l

∑

 dij zij 

k=1

xj ∈U

xj ∈L

(4.20)

在代表性中，标签空间的相似性需要预测标签。因此定义未标记样本集中样本

xj 的预测标签为 f (xj) = [f1(xj), f2(xj), ..., fC(xj)]，将其代入代表性的衡量中，目标 函数可以转化为

(

)

arg max
xq ∈U,fk ∈H:k={1,2,..C }

∑ ∑C exp

− ∥yik

− fk (xi)∥2 2σ2

xi∈L k=1
∑C + exp

( −

( 1

+

2

|fk

(xq)| 2σ2

+

∑C − λ ∥fk∥2H fk (xq )2k)=)1

k=1
() 1
+ β1 u

∑

(

)

exp

∥f −

(xj) − f 2σ2

(xi)∥22

wij

−

β2

() 1 l

xj ;xi∈U
∑

exp

( ∥f
−

(xj) − 2σ2

yi∥22

) wij

xj ;xi∈L

(4.21)

在函数式 (4.21) 中，待查询的样本 xq 是未知的。为了便于搜索最具信息量的 xq，引入一个指示向量 α 采用数值优化的方式进行查找。定义 α 为一个二值向量， 其长度为未标记样本的数量 u，αj ∈ {0, 1}uj=1。在 α 中的每个元素与未标记样本集中 的样本一一对应，如果 αj 为 1，则表示未标记样本集中 αj 对应的 xj 为要查找的最 具信息量样本 xq；如果 αj 等于 0，则表示 xj 不是要查询样本。因此，指示向量可以
- 54 -

面向小样本问题的主动学习理论及应用研究

约束为

αT 1u

=

1，将指示向量引入目标函数式 (

(4.21)，优化问题可表达为 )

arg max
xj ∈U,αT 1u=1

∑ ∑C exp

− ∥yik

− fk (xi)∥2 2σ2

xi∈L k=1

∑C − λ ∥fk∥2H
k=1

fk ∈H:k={1,2,..C }

+

∑

αj

∑C

exp

(( 1
−

+

2

|fk

(xj )| 2σ2

+

fk(xj)2) )

xj ∈U k=1

(

)

+

β1 u

∑

αj

∑

exp

−

∥f

(xj) − f 2σ2

(xi)∥22

wji

xj ∈U xi∈U

(

)

−

β2 l

∑

αj

∑ exp

−

∥f

(xj) − 2σ2

yi∥22

wji

xj ∈U xi∈L

(4.22)

在 (4.22) 中，对于分类模型采用核空间的线性回归模型定义为 fk(x) = ωkT Φ(x)，

Φ(x) 表示 x 在核空间中的映射。样本 x 的多标签预测值 f (x) 可以表示为



T  T

f

(x)

=



f1 f2

(x)
(x) ...



=



ω1
ω2 ...



[R ⊗ Φ (x)]

(4.23)

fC (x)

ωC

其 中 R 表 示 的 是 C × C 大 小 的 单 位 矩 阵，也 可 以 是 标 签 的 关 联 矩 阵；⊗ 表 示

克罗内克积。如果定义 ω = [ω1, ω2, ..., ωC]T ，那么多标签分类器 f (x) 可以表示为

f (x) = ωT [R ⊗ Φ(x)]。此时，目标函数可以转化为

arg max
ω;αT 1u=1:αj ∈{0,1}

( ∑ ∑C
exp −

yik − ωkT Φ (xi) 2σ2

2)

∑C

− λ ∥ωk∥2

xi∈L k=1

(

∑ ∑C

1+2

+ αj exp −

ωkT Φ (xj)

k=1
( + ωkT

Φ

(xj

))2)

 

2σ2

xj ∈U k=1

(

+ β1 u

∑ αj exp

−

ωT [R ⊗ [Φ (xi) − Φ (xj)]] 2σ2

2) 2 wji

xj ,xi∈U

(

−

β2 l

∑

αj

∑

exp

−

ωT [R ⊗ Φ (xj)] − yi 2σ2

2) 2 wji

xj ∈U xi∈L

(4.24)

在信息论中，由于最大相关熵准则往往跟非线性核函数密切结合，造成在目标函

数优化时，函数的解也是非线性的。为了解决非线性目标函数的求解问题，泰勒展

开 [75]、期望最大化、子空间迭代 [74]、半二次优化技术 [76] 和共轭梯度法等求解方

法被引入到信息论学习中。本章采用交替优化的半二次优化技术，通过迭代的方式对

目标函数式进行快速求解 [12]。基于凸共轭函数理论，可以得到命题4.3.2 [19, 205]。

- 55 -

武汉大学博士学位论文

命题 4.3.2 存在一个凸共轭函数 φ 使得

g

(x)

=

exp

( x2 ) − 2σ2

=

max p′

(

p′

∥x∥2 σ2

−

φ

) (p′)

其中 p′ 为辅助变量。对于固定的 x，g(x) 在 p′ = −g(x) 有最大值。 根据命题4.3.2，目标函数式 (4.24) 可以转化为

arg min

∑ ∑C [ mik

yik − ωkT Φ (xi)

]
2

+

λ

∑C

∥ωk

∥2

ω;αT 1u=1,αi∈{0,1} xi∈L k=1

k=1

∑ ∑C [ (

+ αj

njk 1 + 2

ωkT Φ (xj)

+ (ωkT Φ (xj))2)]

xj ∈U k=1

∑∑

− β1

αj

hji

ωT [R ⊗ (Φ (xj) − Φ (xi))]

2 2

wji

xj ∈U xi∈U

∑∑

+ β2

αj

vji

ωT [R ⊗ Φ (xj)] − yi

2 2

wji

xj ∈U xi∈L

(4.25)

其中 mik, njk, hji 和 vji 为未标记样本集中 xi 和 xj 的辅助变量，它们的具体表达形 式为

( mik = exp −

yik − ωkT Φ (xi) 2σ2

2) , xi ∈ L, yik ∈ yi,

njk

( 1+2
= exp −

ωkT Φ (xj)

+

( ωkT

Φ

(xj

))2)

 

,

2σ2

(

hji

=

1 u

exp

−

ωT [R ⊗ Φ (xj)] − ωT [R ⊗ Φ (xi)] 2σ2

2) 2,

(

vji

=

1 l

exp

−

ωT [R ⊗ Φ (xj)] − yi 2σ2

2) 2 , yi ∈ yL.

经过变换后的目标函数 (4.25)，可以通过交替优化策略进行优化求解。大致可以 分为两步：第一步为固定 α，则目标函数式变为对多标签分类器参数 ω 的优化求解 问题，可以通过交替方向乘子算法进行优化求解（ADMM）[18]。第二步为固定 ω， 则目标函数式可以转化为线性规划的求解问题

arg max αT a + β1αT b − β2αT c
αT 1u=1:αi∈{0,1}
- 56 -

(4.26)

面向小样本问题的主动学习理论及应用研究
其中 a，b 和 c 的表达式为

aj

(

∑C

1+2

= exp −

ωkT Φ (xj)

+

( ωkT

Φ

(xj

))2)

 ,

2σ2

k=1

(

bj

=

1 u

∑

exp

−

ωT [R ⊗ (Φ (xj) − Φ (xi))] 2σ2

2) 2 wji,

xi∈U

(

cj

=

1 l

∑

exp

−

ωT [R ⊗ Φ (xj)] − yi 2σ2

2) 2 wji.

xi∈L

由于 α 为离散整数，(4.26) 为 NP-困难问题。为了实现快速求解，采用文献 [36] 策略，对 α 进行连续化的松弛，定义其变化范围为 [0,1], 此时目标函数式 (4.26) 为标 准的线性规划问题。将求得的 α 进行离散化，α 中最大值设为 1，其它元素设为 0,1 位置所对应的样本即为查询样本 xq。
4.3.4 模型优化求解
本节对目标函数式 (4.25) 的优化求解过程进行详细的描述。如上所述，其求解 过程分为两个步骤，第一步为固定 α，对 ω 进行求解。为了求解方便，ω 采用核 化形式，定义 ωk = ∑ θkiΦ(xi)，将 ω 的学习转化为对 θ = [θ1, θ2, ..., θC]T 的学习，
xi∈L
θk = [θk1, θk2, ..., θkl]T ，目标函数式 (4.26) 可以表达为

arg min

∑ ∑C [ mik

yik − θkT KL (xi)

] 2 + λθT (R ⊗ KLL) θ

θ

xi∈L k=1

∑C [ (

+

nqk 1 + 2

θkT KL (xq)

+

( θkT

KL

(xq

))2)]

k=1( 1 ) ∑

+ β1 u

hqi

θT [R ⊗ (KL (xi) − KL (xq))]

2 2

wqi

(

1

)

xi∈U
∑

− β2 l

vqi

θT (R ⊗ KL (xq)) − yi

2 2

wqi

xi∈L

(4.27)

定义 mi = [mi1, mi2, ..., miC],M = [m1, m2, ..., ml] → Rl×C,N = [nq1, nq2, ..., nqC].vq∗i = vqi ∗ wqi,h∗qi = hqi ∗ wqi,v∗ = [vq∗1, vq∗2, vq∗3, ..., vq∗l],h∗ = [h∗q1, h∗q2, h∗q3, ..., h∗qu]，同时引入辅
- 57 -

武汉大学博士学位论文

助变量 ek = θkT KL(xq)。目标函数式可以进一步表达为

arg min (M1) diag(vec(M ))(M1)T
θ

+ λθT (R ⊗ KLL) θ + ediag(N )eT + 2 |diag(N )e|

+

β1 θT u

( R

⊗

( KLU diag

(h∗)

(1Tu

⊗

KL

(xq))T ))

θ

−

β2 l

(M2) diag

( vec

(1Tl

⊗

v∗))

(M2)T

(4.28)

s.t. ek = θkT KL (xq) , ∀xq ∈ U, k = 1, 2, ..., C

式中

M1

=

vec(yL)T

−

θT

(R ⊗ KLL)，M2

=

[ θT

] (R ⊗ KLL) − vec (yL) ，vec(·)

表示

矩阵沿列方向进行拉伸的向量。在等式约束下，采用增广拉格朗日乘子法，(4.28) 的

增广拉格朗日函数可以表示为

Lρ = (M1) diag(vec(M ))(M1)T + λθT (R ⊗ KLL) θ + 2 |diag(N )e|

+ −

( (

β1 u β2 l

) )

(( θT R ⊗ KLU
( (M2) diag vec

diag

( (h∗) 1Tu

⊗

KL

(xq))T )) θ

(1Tl ⊗ v∗)) (M2)T + ediag(N )eT

+

( e

−

θT

(R

⊗

KL

(xq

) ))

ηT

+

ρ 2

e − θT (R ⊗ KL (xq))

2 2

式 (4.29) 采用正交方向乘子法进行求解，其更新规则为

(4.29)

θt+1 = B−1rT ,

et+1

=

arg min ediag(N )eT
e

+

2 |diag(N )e|

+

eηtT

+

ρ eeT 2

−

ρθT

(R

⊗

KL

(xq)) eT

= arg min 1 2

AeT − µ

2 2

+

2

|diag

(N

)

e|

ηt+1

=

ηt

+

ρ

[ e

−

θ(t+1)T

(R

⊗

KL

] (xq))

其中，

B

=

(R ⊗ KLL) diag(M t)(R ⊗ KLL)T + λ(R ⊗

+ −

β1 β2

( (

1 u 1 l

) )

( R

⊗

( KLU diag

(h∗t) (1Tu

⊗

((

( ) ))

R ⊗ KLLdiag v∗t KLL

KL

KLL)2 (xq ))T

ρ + ))2

( R,

KL

(xq )

) KL (xq )T

r

=vec(yL)T

diag

(M )

(R

⊗

KLL)

+

0.5

∗

( η

t

+

ρet)

(R

⊗

KL

(xq ))T

−

β2

(

1 l

)

[ vec(yL)T

diag

( v

ec

(1Tl

⊗

v∗t)) (R

⊗

] KLL)T

[

]

Υ =diag (2N + ρ) , A = Υ−1, µ = ηt − ρθtT (R ⊗ KL (xq)) A

- 58 -

面向小样本问题的主动学习理论及应用研究

由于 e 是稀疏向量，可以通过 SLPLA 工具包进行求解。通过更新规则不断更 新，直至满足迭代停止条件，得到优化分类模型 ω。在第二步中，固定第一步学习到 的 ω，求解与 xq 相关的 α，目标函数式如下

max αT H
αT 1u=1:αi∈[0,1]

(4.30)

其中 H = a + β1b − β2c。通过线性规划对 (4.30) 进行求解，将 α 中最大值对应的样

本作为查询样本。对第一步和第二部循环迭代，直至满足迭代条件。为了更加直观地

了解提出的算法，算法伪代码如下所示。

算法 2: 鲁棒的多标签主动学习算法 (RMLAL) 输入: 标记样本 L，未标记样本 U ，权重系数 β1 和 β2，以及其它辅助变量和参数的初始 化。 输出: α 中最大值对应的索引，即为查询样本 xq 在 U 中的位置。 while 未满足停止条件 do 固定 α，采用正交方向乘子方法对目标函数式 (4.28) 求解 ω; 固定 ω，采用线性规划对目标函数式 (4.30) 求解 α;

4.4 实验及分析
本节对实验过程中参数设置和实验数据划分等进行详细的介绍，并对实验结果进 行分析对比，通过系统性的实验设计证明提出方法的有效性。
4.4.1 实验数据及实验设置
为 了 证 明 提 出 实 验 方 法 的 有 效 性，本 章 采 用 了 12 个 在 多 标 签 主 动 学 习 文 献 [56, 85, 87, 211, 212] 中被广泛采用的多标签标准数据集。这些数据都是从现实应 用中收集获得，包括图像和视频的语义标注、网页分类、功能基因组学、音乐的类 型和情感类别等，主要是从网上下载，具体为 Corel16k，meidalmail，meidalmail， emotions，image，medical，scene，health，social，corel5k，genbase 和 CAL500。这 12 个数据集涵盖了图像、视频、文本、音乐和生物信息，数据集的样本数量从 500 到 43097，标签数量从 5 到 374，其中 corel16k、mediamill 和 corel5k 样本数量大于 5000，而且标签数量大于 100，可认为是多标签数据中的大数据。这 12 个数据的标 签基数即数据中每个样本的平均标签数量从 1 变化到 26。数据集的统计信息如表 4.1 所示。
对于对比算法，本章节对比了几种多标签主动学习中的基准和前沿方法：
- 59 -

武汉大学博士学位论文

表 4.1: 多标签数据集中样本数、类别及标签数等信息统计

数据集 领域 样例数 标签数 特征数 标签基数

Corel16k images 13,766 153

500

2.86

Mediamill video 43,097 101

120

4.37

Emotions music 593

6

72

1.87

Enron

text 1,702

53

1,001

3.38

Image images 2,000

5

294

1.24

Medical text

978

45

1,449

1.25

Scene images 2,407

6

294

1.07

Health

text 5,000

32

612

1.66

Social

text 5,000 39 1,047

1.28

Corel5k images 5000 374

499

3.52

Genbase biology 662

27

1,185

1.25

CAL500 music 502

174

68

26.04

1) 随机策略（Random）：主动学习算法有效性验证中被广泛采用的基准测试算法。 2) 文献 [87] 中提出的 AUDI 算法，主要是将标签排序和阈值学习进行结合，然后将
不确定性和代表性在样本空间和标签空间进行结合查询样本 - 标签对； 3) 文献 [86] 中提出的 QUIRE 算法，基于最小 -最大主动学习框架，系统的设计了通
过标签相关性将不确定性和代表性融合的主动学习算法进行样本 -标签对的查询； 4) 文献 [98] 中提出的 Adaptive 算法，通过最大边缘不确定性采样进行不确定性的衡
量，基于预测标签的标签基数和标记样本的标签不一致性进行多样化准则设计， 进而将两种准则结合，通过自适应的方式选择权重； 5) 文献 [30] 中提出的 Batchrank 算法，基于互信息通过对 NP-难问题优化得到最具 信息量样本。 6) 本文提出的鲁棒多标签主动学习算法 RMLAL。
在实验中，对于所有数据集划分都采用随机形式，将数据集随机分为相同的两部 分，一部分作为测试集，另一部分用于主动学习算法实验。从实验部分的数据集中随 机选取 4% 作为主动学习初始状态下的标记数据集，其余部分则作为主动学习过程中 查询最具信息量样本的候选集。在对比算法中，AUDI 和 QUIRE 是查询样本 -标签 对的算法，为了实现对比的公平性，在实验中对这两种方法查询样本的所有标签。对 于 QUIRE，Batchrank 方法，在原始文献中，其权值设置为 1，为了实现公平对比， 其权值设置和提出方法采用相同的权值遍历策略 [108]。对于其它的参数设置则和原
- 60 -

面向小样本问题的主动学习理论及应用研究

文保持一致。在提出方法中，正则项系数设置为 0.1，拉格朗日乘子设置为 2。对于

核函数，所有算法均采用高斯核，其核宽设置为数据维度的倒数。

对于分类器，同文献 [86, 87] 相同，所有算法均采用 LIBLINEAR [54]。对于多标

签分类结果评价则采用 micro − F 1 [112]。micro − F 1 是标签学习中经常被采用的评

价指标，其综合了所有多标签数据的召回率和准确率，计算公式为

∑C

recall × precision

micro − F 1

=

recall

, recall + precision

=

∑C

T Pi
i=1
∑C

,

T Pi + F Pi

i=1

i=1

∑C T Pi

presicion = ∑C

i=1
∑C

.

T Pi + F Ni

i=1

i=1

(4.31)

T Pi 为第 i 类被正确分类的样本个数，F Pi 为被错分为第 i 类的样本个数，F Ni 为第

i 类被错误分类的样本个数。此外，对于每个数据集，每种算法进行 5 次独立实验获

取其平均结果，同时每个数据集都在查询样本数为 100 时停止查询。

4.4.2 实验结果及分析

对于不同的多标签主动学习算法在 12 个标准数据集上的平均结果如图 4.4 所示。 同时主动学习是一个逐步学习的过程，需要对主动学习的整个过程性能进行评价。为 了定量验证提出方法的性能，在表 4.2 中展示了提出方法和所有对比方法在置信区间 为 95% 时 5 次独立实验过程中配对 t 检验的 Win/Tie/Loss 结果。
从图 4.4 和表 4.2 中可以看出，提出方法在所有数据集上都获得了最好的结 果，并且几乎在整个主动学习过程中都有最佳的表现。在对比方法中，AUDI 和 QUIRE 两种方法考虑了多标签的相关性问题，其中 AUDI 考虑了标签的排序问题， QUIRE 考虑了标签之间的相关性问题，而 Batchrank 和 Adaptive 并没有考虑到样 本与标签之间的强弱关联性，在实验结果中注意到 AUDI 和 QUIRE 两种方法相对 于 Batchrank 和 Adaptive 几乎在所有数据集上的都取得了较好的结果，这说明低 相关性标签对于提升多标签分类模型的判别性具有重大的影响。而在 Adaptive 和 Batchrank 的对比中，可以看到 Adaptive 的结果要优于 Batchrank，Adaptive 相对于 Batchrank 主要考虑了在标签基数上的多样化性，这说明在多标签学习中，标签信息 的考虑对于提升多标签学习具有重要的作用，同时作为信息进行多样化性的衡量是必 要的。同时也要注意到 Batchrank 在某些数据集上比 random 方法的表现要差，可能 原因是异常标签对降低了模型的判别能力，也说明了抑制异常标签的必要性。尽管

- 61 -

武汉大学博士学位论文

(a) corel16k

(b) mediamill

(c) enron

(d) image

(e) scene

(f) health

(g) emotions

(h) medical

(i) social

(j) corel5k

(k) genbase

(l) CAL500

图 4.4: 不同多标签主动学习在 12 个标准数据集上 5 次实验 micro-F1 的平均结果。

- 62 -

面向小样本问题的主动学习理论及应用研究

AUDI 和 QUIRE 考虑了标签的相关性信息并且取得了较好的结果，但是与提出方法 相比，可以直观地看出提出方法地提升在很多数据集上提升显著，这表明提出方法可 以很好的抑制异常标签的影响，发挥强相关性标签在分类模型中的作用，查询到异常 标签少的样本，提升分类模型的判别能力。从实验结果的验证中，总的来说，提出方 法基于相关熵进行不确定性和代表性的衡量，可以很好的抑制异常标签对于信息衡量 造成的偏差，从未标记样本中查询到真正有价值的多标签样本来提升主动学习的性 能。

表 4.2: 在置信区间 95% 下 RMLAL 与对比方法配对 t 检验中 Win/Tie/Loss 统计。

数据集 Vs QUIRE Vs AUDI Vs Adaptive Batchrank Vs Random

corel16k 25/0/0 25/0/0

25/0/0

25/0/0

25/0/0

mediamill 5/16/4 10/12/3

25/0/0

25/0/0

25/0/0

emotions 25/0/0 25/0/0

25/0/0

25/0/0

25/0/0

enron

19/5/1 25/0/0

25/0/0

25/0/0

25/0/0

image

15/10/0 17/8/0

25/0/0

25/0/0

25/0/0

medical 13/10/2 25/0/0

25/0/0

25/0/0

25/0/0

scene

15/5/5 25/0/0

25/0/0

25/0/0

25/0/0

health 13/10/2 18/5/2

25/0/0

25/0/0

25/0/0

social

25/0/0 25/0/0

25/0/0

25/0/0

25/0/0

corel5k

25/0/0

25/0/0

25/0/0

25/0/0

25/0/0

genbase 25/0/0 20/5/0

25/0/0

7/15/3

25/0/0

CAL500 25/0/0

25/0/0

25/0/0

25/0/0

25/0/0

4.4.3 参数分析
提出方法中，主要有两组参数。第一组为与最大相关熵准则相关的核参数 σ。主 要是用来控制最大相关熵准则的鲁棒性 [106]。第二组是权重系数，主要是用来控制 不确定性和代表性之间的信息。本节对这两组参数的敏感性做详细分析，两组参数 均在“emotions”和“scene”两个被广泛使用的数据集上进行实验 [30]，数据划分同 上节。首先是核参数 σ，为了表达方便，定义核的大小为 γ = 1/(2σ2)。同时在标签 空间中固定参数 γ 为 1/C，而在特征空间中固定核参数为 1/m，m 为数据的特征 维度。本节分析核大小对提出方法的影响主要是针对标签空间中最大相关熵准则的 核大小，实验中，设置核大小的变化范围为 {γ, 2γ, 4γ}，而对于权重系数，固定为 β1 = β2 = 1。
- 63 -

micro-F1 micro-F1

武汉大学博士学位论文

0.58 0.57 0.56 0.55 0.54 0.53 0.52 0.51 0.50
0

0.60

0.58

0.56

0.54

0.52

0.50

2

4

0.48

20

40

60

80

100

0

20

40

60

(a) scene

(b) emotions

图 4.5: 标签空间中核参数 γ 的影响

2 4

80

100

图 4.5 中展示了核大小变化时，提出方法进行 10 次独立实验的平均结果。从结 果中，可以看出 γ 的变化在 scene 数据集上的性能变化不大，而在 emotions 上 γ 比 较小时变化显著，可能是由于 emotions 上的标签基数比较大，而 scene 上标签基数 基本为 1，因此 scene 中异常标签比较少，所以随着核参数的变化，结果无明显的波 动。这点也可以从当核参数变大时，两个数据集上的结果都对 γ 不敏感得到验证。因 为随着 γ 的增大基于最大相关熵准则的损失值在异常标签和判别性标签之间差异增 加。因此，为了得到较为稳定的实验结果，可以在实验中设置较大的核大小。

micro-F1 micro-F1

0.57 0.56 0.55 0.54 0.53 0.52 0.51 0.50
0

0.60

0.58

0.56

0.54

1=1, 2=1

1=1, 2=10

1=1, 2=100 1=10, 2=1

0.52

1=10, 2=10

1=10, 2=100 1=100, 2=1

0.50

1=100, 2=10

1=100, 2=100

0.48

20

40

60

80

100

0

20

40

60

(a) scene

(b) emotions

图 4.6: 不同权重对的结果敏感性分析

- 64 -

1=1, 2=1 1=1, 2=10 1=1, 2=100 1=10, 2=1 1=10, 2=10 1=10, 2=100 1=100, 2=1 1=100, 2=10 1=100, 2=100

80

100

面向小样本问题的主动学习理论及应用研究

其次是权重系数，实验中设置两个权重系数 β1 和 β2 的变化范围均 1,10,100。图 4.6 中展示了不同权重下的 10 次独立实验平均结果。可以看到不确定性和代表性信 息的变化对于两组数据的结果都有很大影响，并且 emotions 上的结果要比 scene 上 的结果波动性大，可能是由于 emotions 的标签基数较大异常标签相对较多，使得不 确定性和代表性在查询过程中通过固定的权重系数难以控制。但是也要注意到，在两 个数据集上当 β1 较大时，β2 的值相对较小时，两组数据的结果相对稳定。尽管此时 结果不是最优的，但是此时提出方法相对稳定并且在两个数据集上的综合结果较好。 因此，对于权重系数可以在未标记数据集上关注更多信息，也说明了未标记数据中代 表性信息的重要性。
4.4.4 复杂度分析
算法复杂度是衡量算法效率的重要指标，本节对提出算法和对比算法的时间 复杂度进行对比和分析。表 4.3 中给出了本章所有对比算法的时间复杂度。对于提 出算法，根据提出的优化求解方法，其时间复杂度为 O(t1t2C(l + 1)3 + t2u2)，t1 是 优化求解中 ADMM 算法迭代循环的次数，t2 是优化求解中选择策略的循环迭代次 数。Adaptive 和 AUDI 的时间复杂度分别为 O(Cml2) 和 O((Cl)2)。而 Batchrank 和 QUIRE 时间复杂度相对较高均为 O(n3)。复杂度中 C 为多标签的个数，m 为数据 集的特征维度。可以看出，提出方法相对于 Adaptive 和 AUDI，其时间复杂度较高。 但是与 QUIRE 和 Batchrank 相比，提出方法相对高效。

表 4.3: 对比方法的时间复杂度

方法

RMLAL

Adaptive AUDI Batchrank QUIRE

时间复杂度 O(t1t2C(l + 1)3 + t2u2) O(Ctl2) O((Cl)2) O(u3)

O(u3)

4.4.5 损失函数分析
本节更加深入的对提出算法进行分析，将提出方法中的最大相关熵准则损失替换 成在一些前沿算法中被经常使用的均方损失 [50, 157]。同时采用 PASCAL VOC2007 图片数据集 [53]，以方便对查询样本的可视化。PASCAL 数据集中含有 20 个类 别，分别为 person，bird, cat, cow, dog, horse, sheep，aeroplane, bicycle, boat, bus, car, motorbike, train，bottle, chair, dining table, potted plant, sofa, tv/monitor。由于 PASCAL 数据集庞大，实验中从 20 个类别中选取一个包含 4666 幅图片的子集。对 每幅图片利用 VLfeat 工具包提取 PHOW 特征和空间直方图特征 [166]。
- 65 -

武汉大学博士学位论文

(a) cat, TV monitor

(b) dining table, chair, bottle

(c) horse, person

(d) motorbike

(e) bicycle, person

(f) boat

图 4.7: 基于最大相关熵准则损失查询到的图像：(a)1st 循环；(b) 20th 循环；(c)40th 循环；(d) 60th 循环；(e) 80th 循环；(f) 100th 循环

(a) aeroplane

(b) bird

(c) bird, person

(d) cow

(e) chair, dining table

(f) cat

图 4.8: 基于均方损失查询到的图像：(a)1st 循环；(b) 20th 循环；(c)40th 循环；(d) 60th 循环；(e) 80th 循环； (f) 100th 循环

- 66 -

面向小样本问题的主动学习理论及应用研究
由于主动学习过程中查询样本数目比较多，从结果中抽取一部分图片进行分析， 本节中抽取了在第 1 次循环，第 20 次循环，第 40 次循环，第 60 次循环，第 80 次 循环和第 100 次循环中查询到的图片。图 4.7 和图 4.8 中分别展示了基于最大相关熵 准则和基于均方损失查询到的图片。此外，还展示了基于最大相关熵准则和基于均方 损失在数据集 PASCAL、emotions 和 scene 上整个主动学习过程中的平均结果，如 图 4.9 所示。
从图 4.7 中可以看出，基于最大相关熵准则查询到的样本其标签与样本基本都表 现出很强的相关性，并且相对于图像上的背景标签目标更加显著。图 4.8 基于均方损 失查询到的样本中，部分标签与样本的关联性相对较弱，比如在 20th 和 60th 次循环 查询到的样本，图像的标签相对于背景不够凸显。同时也应注意到，图 4.7 中查询到 的样本大多为多标签图像，而图 4.8 中则相反，其查询到的样本大多为单标签图像。 而图 4.9 中无论是在 PASCAL 数据集上还是在 emotions 和 scene 数据集上，基于最 大相关熵准则的主动学习查询结果在整个主动学习过程中都明显优于基于均方损失的 结果。这些都表明最大相关熵准则查询的样本可以很好的提升多标签学习分类模型的 判别性。尽管均方损失查询的样本大多为单标签，但是其分类结果不理想，可能是由 于查询的单标签样本背景相对于标签目标较为明显，使得在特征提取中形成噪声，影 响了分类模型判别能力的提升，也可能是由于标签单一，并未使分类模型学习到标签 之间的关联性。综上，最大相关熵准则对于多标签问题具有很好的自适应学习能力， 可以抑制异常标签的影响，同时能够保证相对低相关的标签在分类模型训练中发挥作 用，从而加快多标签主动学习性能的提升。

(a) PASCOL

(b) emotions

(c) scene

图 4.9: 基于最大相关熵准则 (MCC) 和均方损失 (MSE) 的主动学习平均结果。

- 67 -

武汉大学博士学位论文
4.5 本章小结
本章基于最大相关熵准则提出了一种对多标签中异常标签具有鲁棒性的多标签主 动学习算法，并且基于半二次优化技术，开发了一种快速优化求解方法。多标签数据 由于其一个样本含有多个标签，在进行标记时对每个样本的标记要从指数的标记空间 中进行查找，因此，多标签数据的标记代价非常高，进行主动学习减少人工标记降低 耗费就显得非常重要。但是与单标签数据不同，在多标签数据中标签与样本之间存在 着强弱关联性，这种关联性一般通过标签之间的相关性进行表示，然而由于标记样本 比较少，难以通过标签统计的方式进行标签相关性的挖掘，主动学习的不确定性和代 表性在多标签中的衡量。比如，如果两个多标签图片具有相同的标签，但是其异常标 签不同，可能造成两个多标签样本特征差异非常大，使得建立的分类模型很难进行准 确的标签识别，同时由于特征差异大，也造成相似性衡量具有很大的差异。本章节中 的低相关性标签通过异常标签进行定义。因为，标签与样本的相关性是相对的，当一 个样本的最相关标签和次最相关标签，与样本关联程度差不多时，次最相关标签也是 强相关性的标签，只有相关性非常低时，才会对模型判别产生影响。因此，本章中用 异常标签进行异常低相关性标签的定义。而目前很多主动学习算法中，并未考虑这种 异常标签的影响，有少数方法为了考虑异常标签的影响，只查询最相关标签来提高模 型的可靠性，然而这会造成部分主要标签信息的丢失。因为，最相关标签可以保证模 型不会产生大的误差，但是弱化了标签之间的关联性衡量。针对这些问题，本章节基 于最大相关熵准则进行多标签中不确定性和代表性衡量，主要是利用最大相关熵准则 的有界性，通过最大化目标函数抑制异常标签在建模中的影响，提升强相关性标签在 模型中的判别作用。当利用样本对异常标签进行判断时，其预测值可能会产生较大的 误差，这会增大与真实标签之间的损失，利用最大相关熵准则进行损失衡量时，最大 相关熵准则的值可能趋近于 0，而优化过程中是最大化目标函数，因此异常标签的影 响会减弱。而如果标签预测正确，此时与真实标签之间的损失为 0，基于最大标签进 行衡量，其值最大为 1，这与目标函数最大化相一致，因此可以发挥强相关性预测准 确的标签在模型中的作用。
与其它多标签主动学习方法相比，本方法的优点可以概括为两方面： 1）充分考虑异常标签的影响，自适应建立多标签问题中主动学习不确定性和代 表性准则的衡量。在不确定性衡量中，通过最大相关熵准则的有界性进行衡量。由于 最大相关熵准则是连续的，并且具有有界性，使得分类决策函数在进行损失衡量时， 其误差尺度范围较小在 0 到 1 之间，由于最大相关熵准则是非线性的，进行半二次
- 68 -

面向小样本问题的主动学习理论及应用研究
优化转化为线性时，其值可以看作是每个标签分类损失的权重，从而间接的实现了对 标签相关性的学习。此外，由于特征差异较大，利用最大相关熵准则也进行标签空间 中相似性的衡量，通过标签空间和特征空间的结合，减弱异常标签对相似性衡量的影 响。
2）提出方法查询的多标签样本中异常标签相对较少，判别性信息全面。本章节 查询的多标签样本都是进行全部标签标注，通过与传统的二次损失函数相比，最大相 关熵准则能够查询到标签数多而异常标签少的样本，而传统的均方损失函数约束查询 的样本大都是多标签数据中的单标签样本，虽然均方损失约束查询得到的样本与标签 之间关系明确，但是在建立分类模型时，基于提出方法建立的样本集比基于均方损失 函数建立的样本集分类结果有明显提高。因此，提出方法的查询的多标签样本对多标 签数据判别性更强。
- 69 -

武汉大学博士学位论文
5 融合判别性和代表性的半监督主动学习方法
在很多应用中，相对于标记样本，未标记样本的数量一般非常的丰富充足。为了 充分利用未标记样本，增强分类模型的泛化能力，本章研究中将半监督学习和主动学 习结合，解决半监督学习和主动学习协同中信息互补欠缺的问题。现有的半监督主动 学习方法，主要是通过挖掘未标记样本中的结构信息或者伪标记一些可靠性高的未标 记样本，虽然集合了半监督学习的优势，但是信息挖掘的形式单一，无法满足主动学 习过程中信息的多样化需求。本章提出了一种新的半监督协同主动学习方法，主要是 利用具有判别性的分类模型和具有挖掘数据结构特性的聚类模型获得分类和聚类结 果，依据分类和聚类结果的一致性对未标记样本进行伪标签的分配，实现半监督学习 中判别性信息和代表性信息的挖掘。将这些伪标记样本加入标记集进行样本的扩充， 达到主动学习过程中不确定性和代表性准则对信息需求补充的目的。与前沿方法相 比，提出方法在四个基础数据集上取得了较好的结果。
5.1 引言
监督学习是分类中最常用和最有效的机器学习方法。一般地，强泛化能力分类模 型往往要求训练数据和待进行分类的未知数据之间独立同分布，这需要大量地标记数 据才能够保证，比如基于 ImageNet 数据集的深度学习网络 [41, 96]。然而许多现实应 用中，大多数数据为未标记数据，标记样本的数量远远少于未标记样本的数量，比如 自然预言处理 [151]，生物医疗信息处理 [20] 等, 这形成了机器学习中的小样本问题。 为了解决小样本问题，主要有两种途径，提升样本的数量和质量 [138]。
主动学习是机器学习中通过提升标记样本的质量来解决小样本问题最主要的 方法，利用标记的样本建立查询函数，查询到最利于提升当前分类器性能的样本交 给人工标注，不断的重复这个过程，逐步建立高质量的数据集。因此，主动学习是 一个数据筛选的过程，筛选出那些最具有信息量的样本，以免对大规模数据集进行 不必要的标注，降低人工耗费。主动学习的查询函数主要分为两类，一类为不确定 性 [171]，一类为多样化性或代表性 [21]。基于这两类准则，已经开展了大量的研 究 [8, 11, 14, 20, 145]。但是由于标记样本数量有限，主动学习查询样本的信息量是逐 步增强的过程，未标记样本数量越大，其需要查询的样本数目越多，使得性能提升相
- 70 -

面向小样本问题的主动学习理论及应用研究
对缓慢。因此，如何快速增加标记样本的数量，实现主动学习效率和性能的进一步提 升是主动学习在大数据分类应用中的难题。
为了解决标记样本少的问题，在机器学习中，还发展了一种半监督学习方法，主 要是利用未标记样本中的信息来建立泛化能力强的分类模型，实现标记数据和未标记 数据的综合利用 [33]。半监督学习也是机器学习中解决小样本问题的核心技术。但是 与主动学习通过人机交互对查询样本进行标注不同，半监督学习直接利用大量未标记 样本进行训练，不需要任何人工参与。因为半监督学习对未标记样本的利用不需要未 标记样本的标签是正确的，而是根据数据分布的假设，在模型训练中保持未标记样本 向标记正确的方向趋近。半监督学习主要是基于两种假设：聚类假设和流形假设。聚 类假设主要是假设处于同一个聚类簇中的样本可能具有相似的标签，描述的是数据结 构中的全局特性，其搜索的是数据中的密集区域和稀疏区域分布，进而实现对决策函 数的调整，保证决策界面在密集区域外部。而流形假设则是假设某个样本可能与它的 近邻具有相似标签，主要描述的是数据结构中的局部特性，其保证的是通过约束使得 数据的空间分布更加密集，实现决策函数对数据的拟合。根据上述假设，半监督学习 可以大致分为五类，自训练方法、协同训练方法、直推式支持向量机、生成模型的方 法和图的方法。而这五类方法主要有两种技术途径，一种是对未标记样本进行伪标签 的分配，另外一种是直接利用未标记样本进行半监督模型训练。文献 [27] 中基于图 的结构建立了 SVM 的半监督分类模型。文献 [15, 216] 通过从未标记样本中选取分类 可靠性高的样本进行伪标签分配，并加入训练集，大规模增加可用的训练样本。
与进行伪标签分配的半监督学习相比，更多研究关注于将流形结构作为正则项或 者基于图进行半监督学习，这是因为流形正则项和图更容易嵌入目标函数中和监督项 进行统一框架下的学习，并在很多领域实现了成功的应用。然而流形正则或者基于图 的方法需要对每个样本点建立近邻域，构造整个数据集的邻接矩阵，数据集越大，时 间复杂度越高，造成这类方法对于大数据的处理效率相对较慢。文献 [137] 将未标记 样本上的回归残差项和流形光滑项进行结合，从未标记数据中选取锚点实现对大规模 数据的快速处理。文献 [91] 提出了基于图的大规模半监督学习方法，通过一种稀疏 贝叶斯方法和增强策略从未标记样本中选取可能对模型有用的样本进行图的建立，进 而实现大规模数据在低空间和低时间复杂度下的学习。文献 [208] 从数据中获取一组 较小的稀疏原型对数据的结构信息进行描述，并利用稀疏原型近似基于图的正则项， 从而实现对大规模未标记样本中信息的快速提取。文献 [175] 提出了锚点图正则的大 规模半监督学习方法，主要是首先基于局部线性重构方法从大规模的未标记样本中选
- 71 -

武汉大学博士学位论文
取锚点，再利用锚点进行图的邻接矩阵构建，从而实现效率的提升。从目前的研究中 发现，基于流形或者图的半监督学习方法在大规模数据处理中，主要是通过选取未标 记样本数据中的少量数据代表整个未标记集进行模型的构建。虽然这类方法的结果相 对较好，但是大量未标记样本的减少，必然造成信息的丢失。
对于伪标签分配方法，其空间和时间复杂度相对要低很多，这类方法的策略主要 是保证未标记样本被分配的伪标签的高可靠性。因为标记的训练样本数量有限，如果 伪标记的样本标签错误数量很大，会造成分类界面的严重偏移。文献 [197] 提出了标 签集成的方法进行伪标签的可靠性标记，其利用自助采样法（Bootstrap）从标记样 本中采样，建立多个弱分类模型，通过多核学习的形式对所有弱分类模型的预测标签 进行加权，实现对未标记样本的可靠性预测。文献 [115] 将半监督条件随机场方法用 于行为识别，主要是把标记样本的似然估计和标记条件伪似然的未标记条件熵结合来 达到半监督训练的目的。文献 [173] 提出了半监督哈希的大规模搜索方法，主要是最 小化标记数据集的监督信息和信息理论正则化项的经验误差，其中信息理论正则化项 通过标记集以及生成的一系列伪标签样本建立。文献 [188] 提出了一种带约束的狄利 克雷处理混合过程聚类方法，通过聚类给未标记样本进行伪标签分配，再利用伪标签 训练一个粗糙的卷积回归神经网络模型，最后基于标记样本进行精化，实现半监督辅 助学习的目的。尽管这些伪标签分配的半监督学习方法对于大规模数据处理的效率很 高，但是由于其效果对于伪标签可靠性非常依赖，而能够用于伪标签可靠分配的可标 记样本数量有限，会导致伪标签可靠性低，限制了方法的推广和应用。因此对于基于 伪标记的半监督学习的性能，效率已有保障，如何在少量标注样本下尽可能提升未标 记样本伪标签的可靠性，是确保这类方法在大数据分类中进一步被广泛应用的关键。
综上，主动学习和半监督学习的结合可以从样本的质量和数量上进行提升，实现 两种学习的优势互补来解决小样本问题。因为对于主动学习来讲，其缺少的是可用于 训练的标记样本数量，训练样本不足，使得查询最具信息量样本困难，只能逐步提升 分类模型性能，半监督学习可以通过利用未标记样本弥补主动学习中信息缺乏造成的 查询函数查询样本能力有限的问题。而半监督学习由于监督信息缺乏，可能导致伪标 签的可靠性不高，使训练模型产生大的误差偏移，主动学习可以通过增强标记样本质 量来提升分类模型分配伪标签的可靠性。因此，主动学习和半监督学习的协同学习可 以很好的解决小样本下训练样本质量和数量的问题，促进彼此学习的能力。事实上， 主动学习和半监督学习的结合已经在很多领域取得了成功的应用，例如，自然语言处 理 [163]，行为识别 [154]，图像检索 [78]，图像分类 [170] 等。为了提升半监督协同主
- 72 -

面向小样本问题的主动学习理论及应用研究
动学习方法在大数据处理中的效率和性能，本章节主要关注于分配伪标签的半监督学 习方法和主动学习方法的协同。而这类协同方式，也已进行了一系列的研究，主要是 基于半监督学习中的自训练模型和协同训练模式 [219]。文献 [114] 首先利用三重训练 算法获得伪标记样本，并和标记样本训练一个分类器，通过主动学习边缘采样策略选 取最不确定样本进行标记，再依据标记集训练一个新的分类器，根据两次未标记样本 集分类结果的一致性，对未标记样本进行伪标签的分配，将不能分配伪标签的样本作 为主动学习的候选池，从而实现半监督学习和主动学习的协同的训练学习，达到互相 提升的目的。文献 [68] 利用目标域中的标记文本和源域中的标记文本训练一个基础 分类器，通过基础分类器对目标域中的未标记样本进行分类，选取分类结果可靠性最 高的样本进行伪标签的分配，同时通过主动学习查询最具信息量的样本进行人工标 记，来实现半监督协同主动学习。文献 [214] 在一个主动学习循环中，首先利用不确 定性选取最具有信息量的样本进行人工标注，再利用标注数据集进行多视图的协同分 类器训练，选取未标记样本中可靠性最高的样本进行伪标签的分配，再重新训练分类 模型，然后再次选择可靠性最高的样本进行伪标记，直至满足要标记的伪标签数量， 进行下次主动学习的循环。文献 [46] 将自训练模型和主动学习相结合实现完全自动 标注，首先利用标记样本通过自训练模型对分类结果可靠性高的未标记样本进行伪标 记，将伪标记样本看作主动学习的候选池，从中选取最具信息量的样本加入训练集。 目前的这些方法中，主要是通过强化分类模型获得高可靠性的样本进行伪标记，通过 增加训练样本数量来增强主动学习性能，而缺少对数据内部信息的探索，比如伪标记 样本对数据结构信息的代表性。因为通过分类模型得到的样本说明其不确定性低，判 别能力强，而主动学习的另一准则代表性，并未通过半监督学习的伪标记样本进行增 强，这导致半监督学习和主动学习联合过程中信息互补欠缺。
为了使伪标记样本同时具有判别性信息和代表性信息，实现半监督学习标记的伪 标记样本与主动学习准则中的不确定性和代表性一致，本章节提出了融合判别性和代 表性的半监督主动学习方法。本章方法主要通过提出的一种 K-均值半监督聚类树方 法对数据的结构进行挖掘，利用少量标注的样本进行部分聚类簇的标记，得到聚类结 果。通过不确定性采样策略查询最具信息量样本标记，利用主动学习增强后的标记集 训练分类模型，同时也利用查询前的标记集和伪标记样本结合训练一个分类模型，依 据分类结果和聚类结果的一致性，进行未标记样本的伪标记，同时将不能进行伪标记 的样本作为下次循环中的候选样本集，可以发现，这个新的候选样本集本身即具有代 表性又具有判别性，因此提出方法可以很好的促进半监督学习和主动学习的信息互
- 73 -

武汉大学博士学位论文

补。

5.2 基于 K-均值的半监督聚类树算法

本章节中为了挖掘数据中的结构信息，增加伪标记样本的信息量，进一步提升 主动学习性能，拟采用 K-均值聚类方法 [72]。K- 均值聚类是一种非监督的聚类方 法，由于其聚类简单，是聚类算法在大数据处理中效率相对最高和最广泛使用的算 法。K-均值聚类算法是一种迭代的无监督聚类方法，只需要初始化类别数量和聚类中 心，通过点与聚类中心的距离判断样本点的归属类别。聚类算法的目标函数为最小化 点到各类别聚类中心的距离，一般采用欧式距离，通过最小二乘法和拉格朗日原理， 可以计算求得 K-均值的聚类中心为各聚类簇中所有点的平均值。假设 µk 为第 k 个 聚类簇的聚类中心, 在聚类过程中，对于一个样本 x, 其类别判别函数为

argmin ||x − µk||22
k:k∈{1,2,...,C}

(5.1)

K-均值算法的收敛，主要通过衡量聚类中心是否改变进行判断, 算法流程如下。

算法 3: K-均值聚类 输入: 需要聚类的类别个数 K，随机初始化 K 个聚类中心。 输出: 所有未标记样本的聚类标签 while 聚类中心在变化 do 计算所有数据点与 K 个聚类中心的距离; 依据式 (5.1) 判断每个数据点归属的类别; 计算每个聚类簇的样本均值作为新的聚类中心点;

在本章中主要解决的是小样本问题。尽管样本数很少，但是样本的类别个数是固 定已知的。因此，K-均值的类别个数可以根据标记样本含有的初始类别个数确定，同 时可以利用标记样本的标签，不断对含有不同类别标记样本的聚类簇进行聚类，建立 深度的聚类树。具体策略为，假如数据集 D 中有 C 个类别，初始情况下，将样本数 据 D 进行 C 个类别的聚类，得到 C 个聚类簇

D = D1 ∪ D2 ∪ ...DC

(5.2)

根据聚类簇内是否有标记样本，将每个聚类簇分为标记部分和未标记部分

D = P ∪ P i

i

i

unlabel

label

(5.3)

P i 表示第 i 个聚类簇中包含的标记样本，而 P i 表示第 i 个聚类簇中包含的未

label

unlabel

标记样本。比如对于一个样本 xi，如果 xi 在聚类过程中被分配到了 Di 这个聚类簇

- 74 -

面向小样本问题的主动学习理论及应用研究

内，并且是有标记的样本，那么

xi

属于

P i ，如果 label

xi

是未标记样本，则

xi

属于

P i 。统计 P i 中含有标记样本的类别数量，如果 P i 中只含有一种类别数量，

unlabel

label

label

则根据近邻原则，此聚类簇中的未标记样本可以赋予相同的标签，如果 P i 中含有 label

的类别数目大于 1，则无法根据近邻原则对聚类簇内的样本进行伪标签的分配。此时

将这个聚类簇继续进行聚类，其聚类数目则根据 P i 中含有的标签类别数目进行设 label

置。如果

Pi label

中有

k

个类别的标签

(k

>

2),

则对

Di

进行

k

个类别的聚类，同时对

这 k 个新产生的类别簇同样进行标记集和未标记集的统计。如果一个新聚类簇的标

记集为空，说明当前的标记集不足以对这个新的聚类簇进行标记，停止对将这个新的

聚类簇的聚类。如果这个新的聚类簇的标记集只含有一种类别，则对其进行伪标记的

分配。如果仍然含有多种类别，则继续依据上述规则进行聚类，直至所有含有标记样

本的簇，都只有一个类别，而将不含有标记样本的簇抽取出来。具体流程如下，

算法 4: 基于 K-均值的半监督聚类树算法 输入: 类别个数 C，随机初始化 C 个聚类中心，数据集 D 和 D 中的标记子数据集 L。 输出: 不含标记样本的聚类簇和只含有一种标签的聚类簇。 利用 K-均值对数据集 D 进行聚类, 得到不含有标记样本的聚类簇和含有标记样本的聚类 簇; for 每个含有标签的聚类簇 do if 聚类簇内的标记样本含有多个类别 then 判断混合聚类簇内标记样本的类别数目 c; 利用 K-均值聚为 c 个子簇; 对每个子聚类簇执行 if判断。

为了更加形象直观的了解 K-均值聚类树方法，图 5.1 中展示了聚类过程。从图

图 5.1: K-均值聚类树算法流程
- 75 -

武汉大学博士学位论文
5.1 可以看到，提出的聚类策略有部分样本不能进行伪标签的标记。但是本算法的主 要目的是用于主动学习，因此不需要对所有的样本进行伪标签的标记，不能标记的未 标记样本则可以作为主动学习的候选集。
5.3 融合判别性和代表性的半监督主动学习方法
本节对提出的半监督主动学习方法进行详细的介绍和陈述。首先对本章节使用的 符号进行定义和说明。定义 D = {x1, x2, ..., xn} 为含有 n 个样本的数据集，x ∈ Rd， d 为样本的特征维数。在初始状态下，从 D 中随机选择 l 个样本作为标记样本集，记 为 L = {(x1, y1), (x2, y2), ..., (xl, yl)}，其中 yi ∈ {1, 2, ..., C}，C 为数据集中的类别个 数，其余的 u = n − l 个样本为未标记样本，记为 U = {xl+1, xl+2, ..., xn}。同时本章 方法是结合半监督的方法，需要对部分未标记样本进行伪标签的分配来提升分类模 型，对于能够进行伪标签分配的样本，记为数据集 T ，而对于不能进行伪标签分配的 样本，则记为数据集 S，S 看作是主动学习过程中查询样本的候选集。同时记主动学 习查询的最具信息量样本为 Q，注意到 Q 为 S 的子集，同时记 Q 的大小为 h。
5.3.1 主动学习策略
主动学习策略是主动学习从未标记集中查询最具信息量样本的核心和关键。多层 次不确定性（Multiclass level uncertainty, MCLU）是不确定性衡量中经典的方法和 策略，在多类问题的主动学习方法中经常被采用，其与文献 [93] 提出的 BvSB(Best vs Second Best) 的思想类似。文献 [93] 通过对多分类中的标签概率进行排序，选取 最大的两个概率做差值，将此差值作为主动学习的查询函数，选取差值最小的样本进 行人工标记，因为差值越小说明两个样本被分到两个类别的概率越接近，这样的样 本即为不确定性最强的样本。多层次不确定性是由文献 [42] 提出，利用点到面的距 离代替条件概率，计算两个距离最大类别的差值。定义多层次不确定性的查询函数 为 c(x)，假如对于未标记样本集 U 中的样本 x，其分类过程中的决策函数为 fi(x)， {i = 1, 2, ..., C}，表示点到所有分类决策平面的距离，分类平面一般采用二分类的 SVM 通过一对多的形式进行建立，从而可以获得所用未标记样本到分类超平面的距 离，表示为 {f (xl+1), f (xl+2), ..., f (xn)}，其中 f (x) = {f1(x), f2(x), ..., fC(x)}。然后 对每个未标记样本点到分类面的距离进行排序，选取每个样本上决策函数中距离最大
- 76 -

面向小样本问题的主动学习理论及应用研究

的两个类别进行差值计算，表示为

r1 max = arg max {fi(x)} i=1,2,...,C

r2 max = arg

max

{fj (x)}

j=1,2,...,C̸=r1 max

cdiff (x) = r1 max − r2 max

依据 (5.4) 选取未标记样本中 cdiff (x) 中最小的 h 个样本进行标记。

5.3.2 半监督主动学习中判别性信息提取

(5.4)

图 5.2: 主动学习中半监督判别性信息提取流程
在主动学习过程中，不确定性信息也可以称为判别性信息 [184]，因为判别性主 要衡量监督信息学习到的分类模型对未标记样本分类的可靠程度。因此，本节中将依 据分类器获得的伪标签称为半监督学习中的判别性信息。为了提升主动学习过程中的 判别性信息，实现不确定性对最具信息量样本的正确和快速选择，本章节利用半监督 学习挖掘未标记样本中分类可靠性高的样本进行伪标记。在传统的主动学习过程中， 其主要通过少量的标记样本，训练一个分类模型，并以此建立主动学习的不确定性查 询函数，比如，多层次不确定性，通过查询函数查询一定数目的最不确定性样本进行 标记，然后加入标记集形成新的标记集，再进行分类模型的训练。重复这个过程，直 至查询样本的数目满足任务需求。从主动学习的过程上看，主动学习一次循环中可以 训练两个分类模型，一个为利用更新前的标记集训练的弱分类模型，另一个为利用主 动学习更新后的标记集训练的相对较强的分类模型。如果基于这两个分类模型对未标 记样本进行分类，则可以得到两个分类结果。本节中将两个分类结果一致的未标记样 本，分配预测标签，也就是伪标签。而对于两次分类结果中标签不一致的未标记样 本，其可能位于分类面附近，通过分类模型进行标记，可靠性不高，需要通过人工标
- 77 -

武汉大学博士学位论文
注，将这部分未标记样本作为主动学习的候选集。这个过程如图 5.2 所示。 在图 5.2 中，通过一个主动学习循环，将未标记样本分为两部分，一部分为含有
伪标签的未标记样本 T ，另一部分为不含有伪标签的未标记样本 S。将伪标记样本和 标记样本进行分类器的联合训练，设计查询函数，提升主动学习查询最具信息量样 本的能力。在这个过程中，分类模型采用支持向量机 (SVM) 进行训练。在一次循环 中，初始状态时，由于没有伪标记样本，因此在第一个循环中，标记集更新前的分类 模型只能利用标记样本进行训练，而在后续的循环中，由于获得了伪标记样本，第一 个分类模型则由标记样本和伪标记样本共同训练，将此分类模型记为 SV M1。基于 SV M1 对未标记样本集中无伪标签的样本进行多层次不确定性的计算，查询到最具信 息量的 h 个样本进行标注。将这 h 个样本进行人工标注，加入标记集，基于新的标 记集训练一个主动学习后的分类模型。对于这个分类模型，只利用标记集进行训练， 因为伪标记样本虽然具有一定的可靠性，但是仍然存在错误标签，因此利用主动学习 更新后的标记样本集训练分类模型可以建立一个相对可靠的强分类模型，将这个分类 模型定义为 SV M2。在主动学习的每次循环中对比 SV M1 和 SV M2 在所有未标记样 本上的分类结果，不断更新伪标记样本和主动学习的候选集，实现在增强主动学习查 询能力的同时，也增加了伪标签的可靠性。算法伪代码如下：
算法 5: 半监督主动学习中提取判别性信息方法 输入: 标记数据集 L，初始时带有伪标签的未标记样本集 T = ∅，未标记样本集 U ，查询 样本数 h，初始时不能够分配伪标签的未标记样本集 S = U 。 输出: 标记样本集 L 和伪标记样本集 T 。 while 未满足停止条件 do 利用标记集 L 和带有伪标签的未标记样本集 T 训练分类器 SV M1; 基于 SV M1 计算样本集 S 的多层次不确定性; 选取 S 中多层次不确定性最小的 h 个样本进行人工标记; 将标记的 h 个样本加入训练集 L，同时更新未标记集 U; 基于新的标记集 L 训练 SV M2; 利用 SV M1 和 SV M2 对未标记集 U 进行分类得到分类结果 V1 和 V2; 根据 V1 和 V2 分类结果的一致性获得伪标记样本集 T 和不能分配伪标签的样本集 S;
5.3.3 半监督主动学习中代表性信息提取
为了实现半监督学习和主动学习在提升分类器性能方向上的一致性。本节主要是 介绍半监督学习和主动学习结合中对数据中代表性信息的挖掘，弥补协同学习过程中
- 78 -

面向小样本问题的主动学习理论及应用研究

代表性信息的缺失。 本节基于少量的标注样本，利用提出的 K- 均值聚类树算法对未标记样本进行聚
类。通过 K-均值聚类树算法，可以快速的获得标记样本在未标记数据集中的分布， 以及类别间的分布情况。因为 K-均值聚类树是通过树形的方式进行扩展和延申，在 有少量标记样本的情况下，其聚类的个数是已知的，当聚类过程中某个聚类簇内含有 不止一种类别的标记样本，那么可以认为这几种类别的样本在空间分布上处于近邻关 系，而通过不断地聚类，可以很好的对这些类别混合区域的样本进行区分标注。在这 个过程中不能聚类的样本则可以作为主动学习的候选集，通过人工进行标注，在主动 学习下次循环时，实现对未标记簇中部分样本的标注，从而可以对上次循环中不能标 注的部分簇进行标注，随着主动学习的推进，更多的未标记簇被标记，实现代表性信 息的挖掘。同时数据中代表性信息的挖掘，可以使得主动学习查询的样本更快的查找 到准确的分类界面。
尽管 K-均值聚类具有很高的效率，但是相对于一些谱聚类算法，K-均值聚类结 果的可靠性相对较低。为了提高聚类过程中聚类结果的可靠性，本节依据分类模型得 到的分类结果和聚类模型得到的聚类结果的一致性进行伪标签的分配。因为分类模 型和聚类模型的原理机制不同，支持向量机分类模型的目的是使类别间隔最大，而 K -均值聚类依据的是近邻原则，通过两种形式的匹配可以提升伪标签的可靠性，这 类似于集成学习或者协同学习的模式。因此，这种提升伪标签样本可靠性的方法是可 行的。由于在主动学习过程中有两个分类器，代表性信息的挖掘分为两部分。第一部 分为标记样本更新前基于 SV M1 获得的分类结果和标记样本获得的聚类结果进行匹 配，第二部分为标记样本更新后基于 SV M2 获得的分类结果和更新后标记样本获得 的聚类结果。在聚类结果和分类结果的匹配过程中，将分类器结果和聚类结果一致而 分类结果不一致的样本作为具有代表性信息的样本。这是因为代表性信息的作用主要 是挖掘空间分布不规则的区域或者是空间分布相对孤立的区域，而对于分类器来讲， 密集区域和类别间差距较大区域相对容易被分类模型进行区分。为了表达的方便，将 主动学习循环中，标记集更新前获得的聚类结果记为 Z1，而将标记集更新后获得的 聚类结果记为 Z2，那么在主动学习的一次循环中伪标记的代表性样本为

T1 = (x, Z1(x))|(V1(x) = Z1(x), V1(x) ̸= V2(x)) T2 = (x, Z2(x))|(V2(x) = Z2(x), V1(x) ̸= V2(x))

(5.5)

T = T1 ∪ T2

其中 T1 表示标记样本更新前获得的具有代表性的样本，T2 表示的是标记样本更

- 79 -

武汉大学博士学位论文
新后获得的具有代表性的样本。而对于那些不能进行伪标签分配的未标记样本则作为 主动学习的候选集，记为 S。为了更加清楚地了解代表性信息提取过程，概括其算法 流程如下。
算法 6: 半监督主动学习中提取代表性信息方法 输入: 标记样本集 L；类别个数 C；未标记集 U ；伪标记样本集 T = ∅；未被伪标记的未 标记样本集 S。 输出: 标记样本集 L 和伪标记样本集 T 。 while 未满足停止条件 do 基于标记样本集 L 和伪标记样本集 T 训练算法 5 中的分类模型 SV M1 和 V1; 采用 K-均值聚类树方法，基于标记样本集 L 对数据集 D 进行聚类，获得聚类结果 Z1; 从 S 中选取多层次不确定性最小的 h 个样本进行标记; 更新标记样本集 L，在标记集 L 中加入 h 个查询到的样本，同时从未标记样本 U 中 移除; 基于更新的标记样本集 L 训练算法 5 中的分类模型 SV M2 和 V2; 采用 K-均值聚类树方法，基于更新的标记样本集 L 对数据集 D 进行聚类，获得聚类 结果 Z2; 依据公式（5.5）建立能够通过聚类进行伪标记的样本集 T; 将伪标记样本集 T 从未标记样本集 U 中移除，获得不能够进行伪标记的样本集 S;
5.3.4 融合判别性和代表性的半监督主动学习方法
本节将算法 5 和算法 6 进行融合，提出本章节新的半监督协同的主动学习方法。 融合算法通过把半监督学习中能够标记伪标签的判别性样本和代表性样本结合，进行 主动学习过程中的查询函数设计和候选集的更新，流程图如图 5.3 所示。
对于判别性样本和代表性的融合主要是对未标记样本进行伪标签分配策略的融 合。在初始状态下，记伪标记样本集 T 为空集。同时也注意到在算法 5 和算法 6 的 主动学习每次循环中，主动学习的候选集是变化的，其样本构成和大小在每次循环中 是不同的。因为在每次循环中，对每个样本进行伪标签分配时，相邻两次循环中分类 器性能不同，使得能够进行伪标签分配的样本不同，而伪标记样本集 T 和主动学习 候选集 S 的并集 U 是固定的。在初始状态下，主动学习的候选集 S 即为未标记样本 集 U 。对于主动学习的一次循环中，标记集更新前的分类模型 SV M1，通过标记集 L 和伪标记样本集 T 进行训练，同时获得在未标记集 U 上的分类结果 V1。通过算法 4，基于更新前的标记集 L 获得 U 上的聚类结果 Z1。然后通过 SV M1 计算 S 中每
- 80 -

面向小样本问题的主动学习理论及应用研究

个样本的多层次不确定性，选取多层次不确定性最小的 h 个样本作为查询样本集 Q， 将 Q 加入到标记集 L 中得到更新后的标记集 L，而同时将 Q 从未标记集合 U 中移 除。基于更新后的训练集 L 获得分类模型 SV M2，并获得 U 的分类结果 V2。同时通 过算法 4 获得在 U 上的聚类结果 Z2。此时，对于未标记集 U 中的样本，如果它们 的预测标签在 Z1 和 V1 相同，但是和 V2 不同，就将 Z1 中的标签分配给这些样本， 并记这些样本形成的集合为 T1。而如果这些样本的预测标签 Z2 和 V2 相同，而和 V1 不同，那么就将 Z2 的标签分配给这些样本，并记为 T2。由于这类样本的伪标签以聚 类结果为主，所以记为代表性的样本。对于判别性样本，则将 V1 和 V2 中具有相同标 签的样本进行分类预测标签的分配，记为 T3。在代表性样本和判别性样本中都不存 在伪标签的样本则作为主动学习的候选集 S。此时通过分类结果和聚类结果的相连， 将判别性和代表性的伪标记集结合得到样本集 T ，其表达形式为

T1 = (x, Z1(x))|(V1(x) = Z1(x), V1(x) ̸= V2(x));

T2 = (x, Z2(x))|(V2(x) = Z2(x), V1(x) ̸= V2(x)); T3 = (x, V1(x))|V1(x) = V2(x);

(5.6)

T = T1 ∪ T2 ∪ T3.

显然，公式（5.6）是将上述半监督主动学习提取的判别性信息和代表性信息融合， 主要通过聚类结果和分类结果连接，扩充伪标记集 T 的数据量和信息量，融合算法 的伪代码如下所示：

算法 7: 融合判别性和代表性的半监督主动学习方法 输入: 标记数据集 L，初始时带有伪标签的未标记样本集 T = ∅，未标记样本集 U ，查询 样本数 h，初始时不能够分配伪标签的未标记样本集 S = U 。 输出: 标记样本集 L 和伪标记样本集 T 。 while 未满足停止条件 do 基于标记样本集 L 和伪标记样本集 T 训练分类模型 SV M1 和和伪标记集 V1; 采用 K-均值聚类树方法，基于标记样本集 L 对数据集 D 进行聚类，获得聚类结果 Z1; 从 S 中选取多层次不确定性最小的 h 个样本进行标记; 更新标记样本集 L，加入 h 个查询到的样本，同时从未标记样本 U 中移除; 基于更新的标记样本集 L 训练获取分类模型 SV M2 和伪标记集 V2; 基于 K-均值聚类树方法和更新的标记样本集 L 对数据集 D 聚类，获得聚类结果 Z2; 依据公式（5.6）建立能够通过聚类进行伪标记的样本集 T ; 将伪标记样本集 T 从未标记样本集 U 中移除，获得不能够进行伪标记的样本集 S;

- 81 -

武汉大学博士学位论文
图 5.3: 融合判别性和代表性的半监督主动学习方法流程
5.4 实验及分析
5.4.1 实验数据及设置
为了验证提出方法的有效性，本节进行充分和系统的实验验证。本节采用的数据 是高光谱卫星遥感影像。卫星遥感数据是大数据中重要的组成部分，在实验中共使用 了 4 幅常用的高光谱遥感数据 [162, 170]，分别为 Botswana(BOT), Kenneedy Space Center(KSC), Pavia University 和 Indian Pines。这四幅影像都可以通过网站进行下 载，其中 Pavia University 也是高空间分辨率影像，而 BOT、KSC 和 Indian Pines 是中低空间分辨率的遥感影像，下面对着四幅遥感影像数据作简单的介绍。
第一幅影像 BOT（博茨瓦纳）影像是美国国家宇航局（NASA）于 2001 年通过 地球观察一号卫星以 30 米的空间分辨率在博茨瓦纳奥卡瓦格三角洲获得的高光谱遥 感影像。这幅影像的原始波段数为 242 个，覆盖了 400-2500 纳米的光谱范围，其光 谱分辨率为 10nm。但是在这 242 个波段中有一部分波段是噪声波段和没有标定的波 段。在去除了这部分波段后，剩余波段数为 145 个波段，即这幅影像上每个样本的 特征维数为 145。这幅影像的大小为 1476 × 256 个像元。但是在这个数据集上只有 3248 个像元被进行了标注，其标注的类别数为 14 个。
第二幅影像是 KSC（肯尼迪航天中心）高光谱遥感影像，是 NASA 通过机载 可见红外成像光谱仪（AVIRIS）于 1996 年在美国佛罗里达州肯尼迪航天中心获得 的。它的空间分辨率为 18 米。这幅影像的原始波段数为 224 个波段，在去除被水体 吸收的波段和噪声干扰波段后，剩余可用波段数为 176 个波段。这幅影像的大小为 512 × 614 个像元，人工标注像元总数为 5211 个像元，标注地物类别总数为 13 个。
- 82 -

面向小样本问题的主动学习理论及应用研究
图 5.4: 从左至右依次为 BOT 影像的假彩色影像、标注的真实地物分布、标注的地物类别。 图 5.5: 从左至右依次为 KSC 影像的假彩色影像、标注的真实地物分布、标注的地物类别。 图 5.6: 从左至右依次为 Indian Pines 影像的假彩色影像、标注的真实地物分布、标注的地物类别。
- 83 -

武汉大学博士学位论文

图 5.7: 从左至右依次为 Pavia Universit 影像的假彩色影像、标注的真实地物分布、标注的地物类别。

表 5.1: BOT 数据集地物类别样本数统计

表 5.2: KSC 数据集地物类别样本数统计

类别名称

样本数量

Water

270

Hippo grass

101

Floodplain grasses1 251

Floodplain grasses2 215

Reeds1

269

Riparian

269

Firescar2

259

Island interior

203

Acacia woodlands

314

Acacia shrublands

248

Acacia grasslands

305

Short mopane

181

Mixed mopane

268

Exposed soils

95

Total

3248

类别名称

样本数量

Scrub

761

Willow

243

CP Hammock

256

CP/Oak

252

Slash Pine

161

Oak/Broadleaf

229

Hardwood swamp 105

Graminoid

431

Spartina marsh

520

Cattail marsh

404

Salt marsh

419

Mud ﬂats

503

Water

927

Total

5211

- 84 -

面向小样本问题的主动学习理论及应用研究
第三幅影像是 Indian Pines（印第安纳）高光谱遥感影像。这幅影像是于 1992 年在美国印第安纳州的一块儿松树区通过机载可见红外成像光谱仪（AVIRSI）获得 的。这个类型的成像仪的波长范围为 0.375um 到 2.2um 之间，以 220 个连续波段对 地物进行成像。由于这幅遥感影像的成像区域是一片农场，其地物分布类型较多，这 个区域的地物类别被分为 16 类，共标注了 10249 个像元。
第四幅影像为 Pavia University（帕维亚大学）高光谱遥感影像。这幅影像既是 高光谱影像同时也是高空间分辨率影像，是由反射光学系统成像光谱仪（ROSIS） 于 2001 年在意大利帕维亚大学获得的。这幅影像的场景即为帕维亚大学，其大小为 610 × 340 个像元大小。在去除 12 个水体吸收波段和噪声强的波段后，这幅影像的可 用波段数为 103 个。这幅影像中共包含了 9 种地物类别。由于这幅影像的空间分辨 率相对较高，其地物细节信息，比如边界在影像上的反应相对清晰，人工共标注了 42776 个像元。
为了更加直观形象的观察四幅影像的地物分布类型，以及标注的地物像元分布， 图 5.4-图 5.7 展示了这四幅影像的假彩色图像以及标注的地物真实分布。而对于四幅 影像上每类地物的标注的数量在表 5.1-表 5.4 进行了展示。
本节采用了以下几种基准和前沿主动学习算法在上述数据集上进行了实验和对 比：
1) 文献 [170] 中提出的 CASSL 算法，主要是根据主动学习中分类模型的分类结 果一致性，进行未标记样本伪标签的分配，并将伪标记样本加入训练集，进行主动学 习查询函数的设计，最后查找到最具信息量的样本和得到最可靠的伪标记样本；
2) 文献 [42] 提出了 MCLU-ECBD 算法，主要是对主动学习多层次不确定性准则 进行增强。首先通过多层次不确定性查找到大于要查询数量的样本，然后对这些样本 通过核 K-均值进行聚类，聚类数量为待查询样本的数量，再从每个聚类簇中选取最 不确定的样本，从而实现批量查询中的冗余去除；
3) 文献 [162] 提出了 MS-cSV 算法，主要是采用主动学习中边缘采样的不确定 性准则进行批量的查询。首先依据 SVM 分类器设计边缘采样的查询准则，并记录 SVM 的支持向量，然后依据未标记样本的不确定性进行排序，依次计算到支持向量 的距离，选取到不同支持向量距离最近的最不确定样本进行标记；
4) 文献 [162] 提出了 EQB 的算法，这是一种基于专家委员会策略的主动学习不 确定性准则，其通过 bootstrap 算法进行重复采样，训练分类模型，从而使未标记样 本可以拥有多个预测标签，并统计其概率，计算每个样本的熵值，查询熵值最大的样
- 85 -

武汉大学博士学位论文

本进行标注；

5) 本章节提出的半监督协同的主动学习算法, 表示为 DRDbSSAL(Discovering

Representativeness and Discriminativeness by Semisupervised Active Learning)；

6) DRDbSSAL+Random 算法，主要是将提出算法中的主动学习策略替换为随机

选择策略，以验证主动学习的作用，和半监督学习中挖掘判别性和代表性的有效性。

表 5.3: Indian Pines 数据集地物类别样本数统计 表 5.4: Pavia University 数据集地物类别样本数统

类别名称

计
样本数量

Alfalfa

46

类别名称

样本数量

Corn-notill

1428

Asphalt

6631

Corn-mintill

830

Meadows

18649

Corn

237

Gravel

2099

Grass-pasture

483

Trees

3064

Grass-trees

730

Metal sheets

1345

Wheat

205

Bare soil

5029

Woods

1265

Bitumen

1330

Oats

20

Self-blocking bricks 3682

Soybean-notill

972

Shadows

947

Soybean-mintill

2455

Total

42776

Soybean-clean

593

Grass-pature-moved

28

Hay-windrowed

478

Buildings-Grass-Trees 386

Stone-Steel-Towers

93

Total

10249

对于实验中的参数设置问题，所有的对比方法，在主动学习的每次循环中都选

择 h=20 大小的样本进行人工标记。实验中所有的分类器都采用 LIBSVM 工具包中

的 SVM 分类器 [54]，而分类器中的核函数都使用高斯核。对于分类器的惩罚系数

和高斯核的参数，为了保证结果精度和效率以及避免参数选择的交叉验证，采用了

一些经验值 [54]，将惩罚系数设置为 100，而高斯核的参数设置都设为 0.05。在实验

中，DRDbSSAL，CASSL 以及 MS-cSV 只有分类器参数。而 MCLU-ECBD 和 EQB

还有附加的参数。在 MCLU-ECBD 中，其是先通过不确定性采样，然后进行多样性

- 86 -

面向小样本问题的主动学习理论及应用研究
去冗余。假设其首先选择 m=40 个样本，然后再从 m 个样本中选择 h 个样本。对于 EQB，构造 4 个标记集，每个标记集选取标记样本数量的 60% 进行模型训练。
对于数据集划分问题，每幅影像将标注的样本划分为训练集和测试集两部分，其 中训练集为 60%，测试集为 40%。初始时，在训练集上的每一类样本中随机选取 10 个作为主动学习中初始化的标记集，其余的训练集样本作为主动学习的候选集。每种 方法在每个数据集上在不同的随机初始化状态下进行 10 次独立实验。
5.4.2 实验结果及分析
在主动学习查询过程中，对于 BOT、KSC 和 Pavia University 影像，由于地位 类别分布差异性较大，可分性较强，当查询样本数为 1000 时则停止查询。而对于 Indian Pines 影像，其地物混合严重，相对复杂，类别特征相似性强，当查询样本数 为 2000 时则停止查询。同时为了更加充分的验证提出方法的有效性，实验中将提出 方法和对比方法的结果在置信区间为 95% 下进行了配对 t 检验，计算了检验的 p-值。 p-值小于 0.05，则说明提出方法和对比方法在主动学习过程中的结果有显著性的差 异，p-值越小，说明其差异越显著。而且配对 t-检验不仅在整个主动学习过程进行 了对比，同时在查询样本数为 200，400，600 和 800 时进行了对比。而对于 Pavia University 数据集则对比了查询样本为 400，800，1200 和 1600 时的结果。p-值只说 明了两个结果是否有显著性的差异，均值越大，则说明方法性能越优越，在主动学 习结果的曲线上主要体现为曲线越高。同时也计算了在不同样本数量查询阶段 10 次 独立实验的平均分类精度（Average Overall Accuracy, average OA）以及 10 次实验过 程中的实验结果的标准差（Standard deviation, STD），OA 越高说明方法性能越好， 而方差越小，说明方法的稳定性越好。表 5.5-表 5.8 展示了在四个影像数据集上提出 方法和对比方法的配对 t-检验 p-值，以及所有算法的平均 OA 和 10 次结果的 ST D。 图 5.8 展示了在四个数据集上所有对比算法在主动学习整个过程中的平均 OA。
从图 5.8 中可以看出，在 BOT 和 KSC 数据集上，分类精度收敛前，提出方法 比对比算法提升相对显著，而在收敛后，提出方法相对稳定，与对比算法相差不明 显。在 Pavaia University 数据集上，提出方法在查询样本达到 200 后，出现了效果 的显著提升。在 Indian Pines 数据集上提出方法在整个主动学习过程中相对于对比方 法都有显著的提升。同时也要注意到，相对于其它三个数据集，Indian Pines 的类别 复杂，类别分布非常不均衡，类别的稀疏分布较多，而提出方法在这个数据集上表现 效果突出，表明提出方法对于精细数据结构的挖掘具有很好的效果。总的来讲，在四 幅遥感影像上，提出的 DRDbSSAL 方法在主动学习过程中的大多数情况下都取得了
- 87 -

