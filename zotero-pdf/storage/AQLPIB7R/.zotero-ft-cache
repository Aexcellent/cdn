计算机研究与发展 Ｊｏｕｒｎａｌ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｒｅｓｅａｒｃｈ　ａｎｄ　Ｄｅｖｅｌｏｐｍｅｎｔ　

ＤＯＩ：１０．７５４４?ｉｓｓｎ１０００－１２３９．２０１９．２０１９０５４０ ５６（１０）：２０７１－２０９６，２０１９

机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述
纪 守 领１ 　 李 进 锋１ 　 杜 天 宇１ 　 李 　 博２
１（浙 江 大 学 计 算 机 科 学 与 技 术 学 院 网 络 空 间 安 全 研 究 中 心 　 杭 州 　３１００２７） ２（伊 利 诺 伊 大 学 香 槟 分 校 计 算 机 科 学 学 院 　 美 国 伊 利 诸 伊 州 厄 巴 纳 香 槟 　６１８２２）
（ｌｉｊｉｎｆｅｎｇ０７１３＠ｚｊｕ．ｅｄｕ．ｃｎ）
Ｓｕｒｖｅｙ　ｏｎ　Ｔｅｃｈｎｉｑｕｅｓ，Ａｐｐｌｉｃａｔｉｏｎｓ　ａｎｄ　Ｓｅｃｕｒｉｔｙ　ｏｆ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ
Ｊｉ　Ｓｈｏｕｌｉｎｇ１，Ｌｉ　Ｊｉｎｆｅｎｇ１，Ｄｕ　Ｔｉａｎｙｕ１，ａｎｄ　Ｌｉ　Ｂｏ２
１（Ｉｎｓｔｉｔｕｔｅ　ｏｆ　Ｃｙｂｅｒｓｐａｃｅ　Ｒｅｓｅａｒｃｈ　ａｎｄ　Ｃｏｌｌｅｇｅ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ　ａｎｄ　Ｔｅｃｈｎｏｌｏｇｙ，Ｚｈｅｊｉａｎｇ　Ｕｎｉｖｅｒｓｉｔｙ，Ｈａｎｇｚｈｏｕ ３１００２７）
２（Ｄｅｐａｒｔｍｅｎｔ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ，Ｕｎｉｖｅｒｓｉｔｙ　ｏｆ　Ｉｌｌｉｎｏｉｓ　ａｔ　Ｕｒｂａｎａ－Ｃｈａｍｐａｉｇｎ，Ｕｒｂａｎａ－Ｃｈａｍｐａｉｇｎ，ＩＬ，ＵＳＡ ６１８２２）
Ａｂｓｔｒａｃｔ　 Ｗｈｉｌｅ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｈａｓ　ａｃｈｉｅｖｅｄ　ｇｒｅａｔ　ｓｕｃｃｅｓｓ　ｉｎ　ｖａｒｉｏｕｓ　ｄｏｍａｉｎｓ，ｔｈｅ　ｌａｃｋ　ｏｆ ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ｈａｓ　ｌｉｍｉｔｅｄ　ｉｔｓ　ｗｉｄｅｓｐｒｅａｄ　ａｐｐｌｉｃａｔｉｏｎｓ　ｉｎ　ｒｅａｌ－ｗｏｒｌｄ　ｔａｓｋｓ，ｅｓｐｅｃｉａｌｌｙ　ｓｅｃｕｒｉｔｙ－ｃｒｉｔｉｃａｌ ｔａｓｋｓ．Ｔｏ　ｏｖｅｒｃｏｍｅ　ｔｈｉｓ　ｃｒｕｃｉａｌ　ｗｅａｋｎｅｓｓ，ｉｎｔｅｎｓｉｖｅ　ｒｅｓｅａｒｃｈ　ｏｎ　ｉｍｐｒｏｖｉｎｇ　ｔｈｅ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ｏｆ ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｍｏｄｅｌｓ　ｈａｓ　ｅｍｅｒｇｅｄ，ａｎｄ　ａ　ｐｌｅｔｈｏｒａ　ｏｆ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｍｅｔｈｏｄｓ　ｈａｖｅ　ｂｅｅｎ　ｐｒｏｐｏｓｅｄ　ｔｏ ｈｅｌｐ　ｅｎｄ　ｕｓｅｒｓ　ｕｎｄｅｒｓｔａｎｄ　ｉｔｓ　ｉｎｎｅｒ　ｗｏｒｋｉｎｇ　ｍｅｃｈａｎｉｓｍ． Ｈｏｗｅｖｅｒ，ｔｈｅ　ｒｅｓｅａｒｃｈ　ｏｎ　ｍｏｄｅｌ ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｉｓ　ｓｔｉｌｌ　ｉｎ　ｉｔｓ　ｉｎｆａｎｃｙ，ａｎｄ　ｔｈｅｒｅ　ａｒｅ　ａ　ｌａｒｇｅ　ａｍｏｕｎｔ　ｏｆ　ｓｃｉｅｎｔｉｆｉｃ　ｉｓｓｕｅｓ　ｔｏ　ｂｅ　ｒｅｓｏｌｖｅｄ． Ｆｕｒｔｈｅｒｍｏｒｅ，ｄｉｆｆｅｒｅｎｔ　ｒｅｓｅａｒｃｈｅｒｓ　ｈａｖｅ　ｄｉｆｆｅｒｅｎｔ　ｐｅｒｓｐｅｃｔｉｖｅｓ　ｏｎ　ｓｏｌｖｉｎｇ　ｔｈｅ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｐｒｏｂｌｅｍ ａｎｄ　ｇｉｖｅ　ｄｉｆｆｅｒｅｎｔ　ｄｅｆｉｎｉｔｉｏｎｓ　ｆｏｒ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ，ａｎｄ　ｔｈｅ　ｐｒｏｐｏｓｅｄ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｍｅｔｈｏｄｓ　ａｌｓｏ　ｈａｖｅ ｄｉｆｆｅｒｅｎｔ　ｅｍｐｈａｓｉｓ．Ｔｉｌｌ　ｎｏｗ，ｔｈｅ　ｒｅｓｅａｒｃｈ　ｃｏｍｍｕｎｉｔｙ　ｓｔｉｌｌ　ｌａｃｋｓ　ａ　ｃｏｍｐｒｅｈｅｎｓｉｖｅ　ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｏｆ ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ａｓ　ｗｅｌｌ　ａｓ　ａ　ｓｃｉｅｎｔｉｆｉｃ　ｇｕｉｄｅ　ｆｏｒ　ｔｈｅ　ｒｅｓｅａｒｃｈ　ｏｎ　ｍｏｄｅｌ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ．Ｉｎ　ｔｈｉｓ　ｓｕｒｖｅｙ，ｗｅ ｒｅｖｉｅｗ　ｔｈｅ　ｅｘｐｌａｎａｔｏｒｙ　ｐｒｏｂｌｅｍｓ　ｉｎ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ，ａｎｄ　ｍａｋｅ　ａ　ｓｙｓｔｅｍａｔｉｃ　ｓｕｍｍａｒｙ　ａｎｄ　ｓｃｉｅｎｔｉｆｉｃ ｃｌａｓｓｉｆｉｃａｔｉｏｎ　ｏｆ　ｔｈｅ　ｅｘｉｓｔｉｎｇ　ｒｅｓｅａｒｃｈ　ｗｏｒｋｓ．Ａｔ　ｔｈｅ　ｓａｍｅ　ｔｉｍｅ，ｗｅ　ｄｉｓｃｕｓｓ　ｔｈｅ　ｐｏｔｅｎｔｉａｌ　ａｐｐｌｉｃａｔｉｏｎｓ　ｏｆ ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｒｅｌａｔｅｄ　ｔｅｃｈｎｏｌｏｇｉｅｓ，ａｎａｌｙｚｅ　ｔｈｅ　ｒｅｌａｔｉｏｎｓｈｉｐ　ｂｅｔｗｅｅｎ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ａｎｄ　ｔｈｅ　ｓｅｃｕｒｉｔｙ ｏｆ　ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ，ａｎｄ　ｄｉｓｃｕｓｓ　ｔｈｅ　ｃｕｒｒｅｎｔ　ｒｅｓｅａｒｃｈ　ｃｈａｌｌｅｎｇｅｓ　ａｎｄ　ｐｏｔｅｎｔｉａｌ　ｆｕｔｕｒｅ ｒｅｓｅａｒｃｈ　ｄｉｒｅｃｔｉｏｎｓ，ａｉｍｉｎｇ　ａｔ　ｐｒｏｖｉｄｉｎｇ　ｎｅｃｅｓｓａｒｙ　ｈｅｌｐ　ｆｏｒ　ｆｕｔｕｒｅ　ｒｅｓｅａｒｃｈｅｒｓ　ｔｏ　ｆａｃｉｌｉｔａｔｅ　ｔｈｅ　ｒｅｓｅａｒｃｈ ａｎｄ　ａｐｐｌｉｃａｔｉｏｎ　ｏｆ　ｍｏｄｅｌ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ．
Ｋｅｙ　ｗｏｒｄｓ 　 ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ；ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ；ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｍｅｔｈｏｄ；ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｍａｃｈｉｎｅ ｌｅａｒｎｉｎｇ；ｓｅｃｕｒｉｔｙ
摘　要　尽管机器学习在许多领域取得了巨大的成功，但缺乏可解释性严重限制了其在现实任务 尤 其 是 安全敏感任务中的广泛应用．为了克服这一弱点，许多学者对如何提高机器学习模型可 解 释性 进 行 了深 入的研究，并提出了大量的解释方法以帮助 用 户 理 解 模 型 内 部 的 工 作 机 制．然 而，可 解 释 性 研 究 还 处 于
　 收 稿 日 期 ：２０１９－０６－１１；修 回 日 期 ：２０１９－０８－１６ 　基金项目：国 家 自 然 科 学 基 金 项 目 （６１７７２４６６，Ｕ１８３６２０２）；浙 江 省 自 然 科 学 基 金 杰 出 青 年 项 目 （ＬＲ１９Ｆ０２０００３）；浙 江 省 科 技 计 划 项 目
（２０１７Ｃ０１０５５） Ｔｈｉｓ　ｗｏｒｋ　ｗａｓ　ｓｕｐｐｏｒｔｅｄ　ｂｙ　ｔｈｅ　Ｎａｔｉｏｎａｌ　Ｎａｔｕｒａｌ　Ｓｃｉｅｎｃｅ　Ｆｏｕｎｄａｔｉｏｎ　ｏｆ　Ｃｈｉｎａ （６１７７２４６６，Ｕ１８３６２０２），ｔｈｅ　Ｎａｔｕｒａｌ　Ｓｃｉｅｎｃｅ Ｆｏｕｎｄａｔｉｏｎ　ｆｏｒ　Ｄｉｓｔｉｎｇｕｉｓｈｅｄ　Ｙｏｕｎｇ　Ｓｃｈｏｌａｒｓ　ｏｆ　Ｚｈｅｊｉａｎｇ　Ｐｒｏｖｉｎｃｅ （ＬＲ１９Ｆ０２０００３），ａｎｄ　ｔｈｅ　Ｐｒｏｖｉｎｃｉａｌ　Ｋｅｙ　Ｒｅｓｅａｒｃｈ　ａｎｄ Ｄｅｖｅｌｏｐｍｅｎｔ　Ｐｒｏｇｒａｍ　ｏｆ　Ｚｈｅｊｉａｎｇ （２０１７Ｃ０１０５５）．

２０７２

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

初级阶段，依然还有大量的科学问题尚待解决．并且，不同的学者解决问题的角度不 同，对可 解 释 性 赋予 的含义也不同，所提出的解释方法也各有侧重．迄今为止，学术界对模型可解释 性 仍缺乏 统一的 认 识，可 解释性研究的体系结构尚不明确．在综述中，回 顾 了 机 器 学 习 中 的 可 解 释 性 问 题，并 对 现 有 的 研 究 工 作 进行了系统的总结和科学的归类．同时，讨 论 了 可 解 释 性 相 关 技 术 的 潜 在 应 用，分 析 了 可 解 释 性 与 可 解 释 机 器 学 习 的 安 全 性 之 间 的 关 系 ，并 且 探 讨 了 可 解 释 性 研 究 当 前 面 临 的 挑 战 和 未 来 潜 在 的 研 究 方 向 ，以 期进一步推动可解释性研究的发展和应用．
关 键 词 　 机 器 学 习 ；可 解 释 性 ；解 释 方 法 ；可 解 释 机 器 学 习 ；安 全 性
中 图 法 分 类 号 　ＴＰ３９１

　 　 近 年 来 ，机 器 学 习 相 关 技 术 在 计 算 机 视 觉 、自 然 语 言 处 理 、语 音 识 别 等 多 个 领 域 取 得 了 巨 大 的 成 功 ， 机器学习模型也被广泛地应用到一些重要的现实任 务中，如人脸识别［１－３］、自动驾驶［４］、恶 意 软 件 检 测［５］ 和智慧医疗分析 等 ［６］ ．在某些 场 景 中，机 器 学 习 模 型 的表现甚至超过了人类．
尽管机器学习在许多有意义的任务中胜过人 类，但由于 缺 乏 可 解 释 性，其 表 现 和 应 用 也 饱 受 质 疑［７］．对 于 普 通 用 户 而 言 机 器 学 习 模 型 尤 其 是 深 度 神经网络 （ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ，ＤＮＮ）模 型 如 同 黑盒一般，给它一 个 输 入，其 反 馈 一 个 决 策 结 果，没 人能确切地知道它背后的决策依据以及它做出的决 策是否可靠．而 缺 乏 可 解 释 性 将 有 可 能 给 实 际 任 务 中尤其是安全敏感任务 中 的 许 多 基 于 ＤＮＮ 的 现 实 应用带来严重的威胁．比如 说，缺乏可 解释 性的自 动 医疗诊断模型可能 给 患 者 带 来 错 误 的 治 疗 方 案，甚 至严重威 胁 患 者 的 生 命 安 全．此 外，最 近 的 研 究 表 明，ＤＮＮ 本身也面临着多种安全威胁———恶意构 造 的对抗性样本可以轻易让 ＤＮＮ 模型分类 出 错 ， ［８－１０］ 而他们针 对 对 抗 样 本 的 脆 弱 性 同 样 也 缺 乏 可 解 释 性．因此，缺乏可解释性已经 成为机 器 学习 在现实 任 务中的进一步发展和应用的主要障碍之一．
为了提高机器学习模型的可解释性和透明性， 建立用户与决策模 型 之 间 的 信 任 关 系，消 除 模 型 在 实际部署应用中的 潜 在 威 胁，近 年 来 学 术 界 和 工 业 界进行了广泛和深入的研究并且提出了一系列的机 器学习模型可解释性方法．然而，由于不 同的研究 者 解决问题的角度不同，因 而 给 “可 解 释 性”赋 予 的 含 义也不同，所 提 出 的 可 解 释 性 方 法 也 各 有 侧 重．因 此 ，亟 需 对 现 有 工 作 进 行 系 统 的 整 理 和 科 学 的 总 结 、 归 类 ，以 促 进 该 领 域 的 研 究 ．
在 本 文 中，我 们 首 先 详 细 地 阐 述 可 解 释 性 的 定 义和所解决的问题．然后，我们 对现有 的可 解释性 方 法进行系统的总结 和 归 类，并 讨 论 相 关 方 法 的 局 限

性．接着，我们简单地介绍模型 可 解 释性 相关 技术 的 实际应用场景，同 时 详 细 地 分 析 可 解 释 性 中 的 安 全 问题．最后，我们讨论模型可 解 释 性 相关 研究 所面 临 的挑战以及未来可行的研究方向．
１　 机 器 学 习 可 解 释 性 问 题
在介绍具体的可解释问题与相应的解决方法之 前，我们先简单地 介 绍 什 么 是 可 解 释 性 以 及 为 什 么 需要可解释性．在数据挖掘和 机 器学 习 场景 中，可 解 释性被定义为向人类解释或以呈现可理解的术语的 能力［１１］．从本 质 上 讲，可 解 释 性 是 人 类 与 决 策 模 型 之 间 的 接 口 ，它 既 是 决 策 模 型 的 准 确 代 理 ，又 是 人 类 所可以理 解 的［１２］．在 自 上 而 下 的 机 器 学 习 任 务 中， 模型通常建立在一 组 统 计 规 则 和 假 设 之 上，因 而 可 解释性至关重要，因 为 它 是 所 定 义 的 规 则 和 假 设 的 基石．此外，模 型 可 解 释 性 是 验 证 假 设 是 否 稳 健，以 及所定义的规 则 是 否 完 全 适 合 任 务 的 重 要 手 段．与 自上而下的任务不 同，自 下 而 上 的 机 器 学 习 通 常 对 应于手动和繁重任 务 的 自 动 化，即 给 定 一 批 训 练 数 据 ，通 过 最 小 化 学 习 误 差 ，让 模 型 自 动 地 学 习 输 入 数 据与输出类别 之 间 的 映 射 关 系．在 自 下 而 上 的 学 习 任 务 中 ，由 于 模 型 是 自 动 构 建 的 ，我 们 不 清 楚 其 学 习 过程，也不清楚其 工 作 机 制，因 此，可 解 释 性 旨 在 帮 助人们理解机器学 习 模 型 是 如 何 学 习 的，它 从 数 据 中学到了什么，针 对 每 一 个 输 入 它 为 什 么 会 做 出 如 此决策以及它所做的决策是否可靠．
在 机 器 学 习 任 务 中 ，除 了 可 解 释 性 ，常 常 会 提 到 另外２个 概 念：模 型 准 确 性 （ａｃｃｕｒａｃｙ）和 模 型 复 杂 度（ｍｏｄｅｌ　ｃｏｍｐｌｅｘｉｔｙ）．准确性反映了模型 的拟 合 能 力以及在某种 程 度 上 准 确 预 测 未 知 样 本 的 能 力．模 型复杂度反映了模 型 结 构 上 的 复 杂 性，只 与 模 型 本 身有关，与模 型 训 练 数 据 无 关．在 线 性 模 型 中，模 型 的复杂度由非零权 重 的 个 数 来 体 现；在 决 策 树 模 型

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０７３

中 ，模 型 的 复 杂 度 由 树 的 深 度 体 现 ；在 神 经 网 络 模 型 中，模型复杂度则 由 神 经 网 络 的 深 度、宽 度、模 型 的 参数量以及模型的 计 算 量 来 体 现［１３］．模 型 的 复 杂 度 与模型准确性相关联，又与模型 的可 解释 性 相对 立． 通 常 情 况 下 ，结 构 简 单 的 模 型 可 解 释 性 好 ，但 拟 合 能 力差，往往准 确 率 不 高．结 构 复 杂 的 模 型，拟 合 能 力 强 ，准 确 性 高 ，但 由 于 模 型 参 数 量 大 、工 作 机 制 复 杂 、 透 明 性 低 ，因 而 可 解 释 性 又 相 对 较 差 ．
那 么 ，在 实 际 的 学 习 任 务 中 ，我 们 是 选 择 结 构 简 单易于解释的模型 然 后 训 练 它，还 是 训 练 复 杂 的 最 优模型 然 后 开 发 可 解 释 性 技 术 解 释 它 呢？基 于 这 ２种不同的选 择，机 器 学 习 模 型 可 解 释 性 总 体 上 可 分为２ 类：事 前 （ａｎｔｅ－ｈｏｃ）可 解 释 性 和 事 后 （ｐｏｓｔ－ ｈｏｃ）可解释性．其中，ａｎｔｅ－ｈｏｃ可 解 释 性 指 通 过 训 练 结构简单、可解释性 好 的 模 型 或 将 可 解 释 性 结 合 到 具体的模 型 结 构 中 的 自 解 释 模 型 使 模 型 本 身 具 备 可解释能力．ｐｏｓｔ－ｈｏｃ可 解 释 性 指 通 过 开 发 可 解 释 性技术解释已 训 练 好 的 机 器 学 习 模 型．根 据 解 释 目 标和 解 释 对 象 的 不 同，ｐｏｓｔ－ｈｏｃ可 解 释 性 又 可 分 为 全局可解 释 性 （ｇｌｏｂａｌ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ）和 局 部 可 解 释性（ｌｏｃａｌ　ｉｎｔｅｒｐｒｅｔａ－ｂｉｌｉｔｙ）．全 局 可 解 释 性 旨 在 帮 助人们理 解 复 杂 模 型 背 后 的 整 体 逻 辑 以 及 内 部 的 工作机制［１２］，局部可解释性旨 在 帮 助 人 们 理 解 机 器 学习模型 针 对 每 一 个 输 入 样 本 的 决 策 过 程 和 决 策 依 据 ． ［１４］
２　ａｎｔｅ－ｈｏｃ可 解 释 性
ａｎｔｅ－ｈｏｃ可解释 性 指 模 型 本 身 内 置 可 解 释 性， 即对于一个已训练 好 的 学 习 模 型，无 需 额 外 的 信 息 就可以 理 解 模 型 的 决 策 过 程 或 决 策 依 据．模 型 的 ａｎｔｅ－ｈｏｃ可解释性 发 生 在 模 型 训 练 之 前，因 而 也 称 为事前可解释性．在学习任务中，我们通常 采用结 构 简单、易于理解的自解释 模 型 来 实 现 ａｎｔｅ－ｈｏｃ可 解 释 性 ，如 朴 素 贝 叶 斯 、线 性 回 归 、决 策 树 、基 于 规 则 的 模型．此外，我们也可以通过构 建将可 解释 性直接 结 合到具体的模型结构中的学习模型来实现模型的内 置可解释性 ． ［１５］ ２．１　 自 解 释 模 型
对 于 自 解 释 模 型 ，我 们 从 ２ 个 角 度 考 虑 模 型 的 可 解 释 性 和 透 明 性 ，即 模 型 整 体 的 可 模 拟 性 （ｓｉｍｕｌａｔａｂｉ－ ｌｉｔｙ）和模型单个组件的可分解性（ｄｅｃｏｍｐｏｓａｂｉｌｉｔｙ）．
严 格 意 义 上 来 讲，如 果 我 们 认 为 某 个 模 型 是 透 明 的 ，那 么 我 们 一 定 能 从 整 体 上 完 全 理 解 一 个 模 型 ，

也应该能够将输入 数 据 连 同 模 型 的 参 数 一 起，在 合 理的时间 步 骤 内 完 成 产 生 预 测 所 需 的 每 一 个 计 算 （即整体上的可模拟性）．比如 在朴 素贝叶斯 模型 中， 由于条件独立性的 假 设，我 们 可 以 将 模 型 的 决 策 过 程转化 为 概 率 运 算 ．在 ［１６－１７］ 线 性 模 型 中，我 们 可 以 基 于 模 型 权 重 ，通 过 矩 阵 运 算 线 性 组 合 样 本 的 特 征 值 ， 复现线性模型的决 策 过 程，其 中 模 型 权 重 体 现 了 特 征之间 的 相 关 关 系 ．而 ［１３，１７－１８］ 在 决 策 树 模 型 中，每 一棵决策树都由表示特征或者属性的内部节点和表 示类别的叶子节点 组 成，树 的 每 一 个 分 支 代 表 一 种 可 能 的 决 策 结 果 ．决 ［１９－２０］ 策 树 中 每 一 条 从 根 节 点 到 不同叶子 节 点 的 路 径 都 代 表 着 一 条 不 同 的 决 策 规 则，因而每一棵决策 树 都 可 以 被 线 性 化 为 一 系 列 由 ｉｆ－ｔｈｅｎ形式组成的决策规则 ．因 ［２０－２３］ 此，对 于 新 的 观 测 样 本 ，我 们 可 以 通 过 从 上 到 下 遍 历 决 策 树 ，结 合 内 部 节 点 中 的 条 件 测 试 ，基 于ｉｆ－ｔｈｅｎ 决 策 规 则 判 定 样 本是否必 须 遵 循 左 或 右 分 支 来 模 拟 决 策 树 的 决 策 过程．
自解释模型的可分解性要求模型的每个部分， 包 括 模 型 结 构 、模 型 参 数 ，模 型 的 每 一 个 输 入 以 及 每 一维特征都允许直 观 的 解 释［２４］．在 朴 素 贝 叶 斯 模 型 中 ，由 于 条 件 独 立 性 的 假 设 ，模 型 的 预 测 可 以 很 容 易 地转化为单 个 特 征 值 的 贡 献———特 征 向 量，特 征 向 量的每一维表示每个特征值对最终分类结果的贡献 程度［１７］．在线 性 模 型 中，模 型 的 权 重 直 接 反 映 了 样 本特征重要性，既 包 括 重 要 性 大 小 也 包 括 相 关 性 方 向［２５］．权重绝 对 值 越 大，则 该 特 征 对 最 终 预 测 结 果 的贡献越大，反 之 则 越 小．如 果 权 重 值 为 正，则 该 特 征与最终的预测类别正相关，反之 则 负相关．在 决 策 树 模 型 中 ，每 个 节 点 包 含 了 特 征 值 的 条 件 测 试 ，判 定 样 本 属 于 哪 一 分 支 以 及 使 用 哪 一 条 规 则 ，同 时 ，每 一 条规则也为最终的分类结果提 供 了 解 释．此 外，决 策 树模型自 带 的 基 于 信 息 理 论 的 筛 选 变 量 标 准 也 有 助于理解在模型决策过程中哪些变量起到了显著的 作用．
然 而 ，由 于 人 类 认 知 的 局 限 性 ，自 解 释 模 型 的 内 置可解释性受模型 的 复 杂 度 制 约，这 要 求 自 解 释 模 型结构一定不 能 过于 复杂．因 此，上 述 模 型 只 有 具 有 合 理 的 规 模 才 能 具 有 有 效 的 可 解 释 性 ．例 如 对 于 高 维 的线性模型，其内 置 可 解 释 性 未 必 优 于 ＤＮＮ．此 外， 对于决策树模型和 基 于 规 则 的 模 型，如 果 树 深 度 太 深或者模型的规则太复杂，人类也未 必 能 理 解 ． ［１２，２０］ 但 如 果 模 型 结 构 太 简 单 ，模 型 的 拟 合 能 力 必 然 受 限 ， 因此模型可能会学习错误的特征来最小化在训练集

２０７４

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

上 的 经 验 误 差 ，而 这 些 特 征 可 能 与 人 类 认 知 相 违 背 ， 对于人类 而 言 同 样 也 很 难 解 释．因 此，自 解 释 模 型 的内置可解 释 性 与 模 型 准 确 性 之 间 始 终 存 在 一 个 平 衡 ． ［１３］ ２．２　 广 义 加 性 模 型
在实际学习任务 中，简 单 模 型 （如 线 性 模 型）因 为准确率低而无法 满 足 需 要，而 复 杂 模 型 的 高 准 确 率又通常是牺 牲 自 身 可 解 释 性 为 代 价 的．作 为 一 种 折中，广义加性模型 既 能 提 高 简 单 线 性 模 型 的 准 确 率，又能保留线性 模 型 良 好 的 内 置 可 解 释 性 ． ［２４，２６－２７］ 广义加性模型一般形式为
ｇ（ｙ）＝ｆ１（ｘ１）＋ｆ２（ｘ２）＋ … ＋ｆｎ （ｘｎ ）， 其中，ｆｉ（·）为 单 特 征 （ｓｉｎｇｌｅ－ｆｅａｔｕｒｅ）模 型，也 称 为 特征ｘｉ 对应的形函数（ｓｈａｐｅ　ｆｕｎｃｔｉｏｎ）．广义加性模 型通过线性函数组合每一单特征模型得到最终的决 策形式．在广义加性模型中，形 函数本身 可能 是非 线 性的，每一个单特征 模 型 可 能 采 用 一 个 非 常 复 杂 的 形函数ｆｉ（ｘｉ）来量化每一个特 征 ｘｉ 与 最 终 决 策 目 标之间的关系，因而 可 以 捕 获 到 每 一 个 特 征 与 最 终 决策目标之间的非 线 性 关 系，因 此 广 义 加 性 模 型 准 确率高于简单 线 性 模 型．又 因 为 广 义 加 性 模 型 通 过 简单的线性函数组合每一个单特征模型得到最终的 决 策 形 式 ，消 除 了 特 征 之 间 的 相 互 作 用 ，因 此 可 以 保 留简单线性模型良 好 的 可 解 释 性，从 而 解 决 了 复 杂 模型因为特征之间复杂的相关关系而削弱自身可解 释性的问题．
Ｌｏｕ等人 提 ［２４］ 出 了 一 种 基 于 有 限 大 小 的 梯 度 提升树加性模型方 法，该 方 法 在 回 归 和 分 类 问 题 上 精度显著优于 传 统 方 法，同 时 还 保 持 了 ＧＡＭ 模 型 的可解释性．Ｒａｖｉｋｕｍａｒ等 人 结 ［２８］ 合 稀 疏 线 性 建 模 和加性非参数回归 的 思 想，提 出 了 一 种 称 之 为 稀 疏 加性模型的高维非 参 数 回 归 分 类 方 法，解 决 了 高 维 空间中加性模型的拟合问题，同时 基 于１ 正 则 的 稀 疏性，可 实 现 特 征 的 有 效 选 择．Ｐｏｕｌｉｎ 等 人 开 ［１６］ 发 了一个图形化解释 框 架，提 供 了 对 加 性 模 型 的 图 形 化解释，包括对模型 整 体 的 理 解 以 及 决 策 特 征 的 可 视 化 ，以 帮 助 建 立 用 户 与 决 策 系 统 之 间 的 信 任 关 系 ． ２．３　 注 意 力 机 制
神经 网 络 模 型 由 于 模 型 结 构 复 杂，算 法 透 明 性 低，因而模型 本 身 的 可 解 释 性 差．因 此，神 经 网 络 模 型的自身可解释性只能通过额外引入可解释性模块 来 实 现 ，一 种 有 效 的 方 法 就 是 引 入 注 意 力 机 制 （ａｔｔｅｎ－ ｔｉｏｎ　ｍｅｃｈａｎｉｓｍ）［２９－３１］．

注意力机制 源 于 对 人 类 认 知 神 经 学 的 研 究．在 认 知 科 学 中 ，由 于 信 息 处 理 的 瓶 颈 ，人 脑 可 以 有 意 或 无意地从大量输入信息中选择小部分有用信息来重 点 处 理 ，同 时 忽 略 其 他 可 见 的 信 息 ，这 就 是 人 脑 的 注 意力机制［３２］．在 计 算 能 力 有 限 的 情 况 下，注 意 力 机 制是解决信息超载 问 题 的 一 种 有 效 手 段，通 过 决 定 需要关注的输入部 分，将 有 限 的 信 息 处 理 资 源 分 配 给更重要的任务．此外，注意 力 机 制 具有 良 好 的可 解 释性，注意力权重 矩 阵 直 接 体 现 了 模 型 在 决 策 过 程 中感兴趣的区域．
近年 来，基 于 注 意 力 机 制 的 神 经 网 络 已 成 为 神 经 网 络 研 究 的 一 大 热 点 ，并 在 自 然 语 言 处 理 、计 算 机 视觉、推荐 系 统 等 领 域 有 着 大 量 的 应 用［３３］．在 自 然 语言 处 理 领 域，Ｂａｈｄａｎａｕ 等 人 将 ［２９］ 注 意 力 机 制 引 入到基于编码器－ 解 码 器 架 构 的 机 器 翻 译 中，有 效 地提高了“英语 － 法语”翻译的性能．在编码阶段，机 器翻译模型 采 用 双 向 循 环 神 经 网 络 （Ｂｉ－ＲＮＮ）将 源 语 言 编 码 到 向 量 空 间 中 ；在 解 码 阶 段 ，注 意 力 机 制 为 解码器的隐藏状态 分 配 不 同 的 权 重，从 而 允 许 解 码 器在生成法语翻译的每个步骤选择性地处理输入句 子的不同部分．最后 通 过 可 视 化 注 意 力 权 重（如 图 １ （ａ）所示），用户 可 以 清 楚 地 理 解 一 种 语 言 中 的 单 词 是如何依赖另一种语言中的单词进行正确翻译的． Ｙａｎｇ等人 将 ［３４］ 分层注意力机 制 引 入 到 文 本 分 类 任 务 中 ，显 著 提 高 了 情 感 分 析 任 务 的 性 能 ，同 时 注 意 力 权重量化了每一个 词 的 重 要 性，可 帮 助 人 们 清 晰 地 理解每一个词 对 最 终 情 感 分 类 结 果 的 贡 献 （如 图 １ （ｂ）所示）．在计 算 机 视 觉 领 域，Ｘｕ 等 人 将 ［３２］ 注 意 力 机制应用于看图说话（ｉｍａｇｅ　ｃａｐｔｉｏｎ）任务 中以 产 生 对图片的描述．首 先 利 用 卷 积 神 经 网 络 （ＣＮＮ）提 取 图 片 特 征 ，然 后 基 于 提 取 的 特 征 ，利 用 带 注 意 力 机 制 的循环神 经 网 络 （ＲＮＮ）生 成 描 述．在 这 个 过 程 中， 注 意 力 实 现 了 单 词 与 图 片 之 间 的 对 齐 ，因 此 ，通 过 可 视化注意力权重矩 阵，人 们 可 以 清 楚 地 了 解 到 模 型 在 生 成 每 一 个 单 词 时 所 对 应 的 感 兴 趣 的 图 片 区 域 （如 图２ 所 示）．此 外，注 意 力 机 制 还 被 广 泛 地 应 用 于 推 荐系统中，以 研 究 可 解 释 的 推 荐 系 统 ．具 ［３５－３９］ 体 地， 这些方 法 首 先 基 于 历 史 记 录，利 用注 意力 机 制 计 算 针 对 每 一 条 记 录 的 注 意 力 分 值 ，从 而 给 不 同 的 偏 好 设 置不同的权重，或者通过注意 力 机制 对 用 户行 为、用 户 表 征 进 行 建 模 来 学 习 用 户 的 长 期 偏 好 ，以 推 荐 用 户 可能感兴趣的下一个项目；最后，通 过 可 视 化用 户历 史记录列表中每一条记录的注意力分值来提供对推 荐结果的解释，以增强推荐系统自身的可解释性．

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０７５

Ｆｉｇ．１　Ｖｉｓｕａｌｉｚａｔｉｏｎ　ｏｆ　ａｔｔｅｎｔｉｏｎ　ｗｅｉｇｈｔ　ｉｎ　ｎａｔｕｒａｌ　ｌａｎｇｕａｇｅ　ｐｒｏｃｅｓｓｉｎｇ　ａｐｐｌｉｃａｔｉｏｎｓ 图 １　 自 然 语 言 处 理 应 用 中 的 注 意 力 权 重 可 视 化

Ｆｉｇ．２　Ａｌｉｇｎｍｅｎｔ　ｏｆ　ｗｏｒｄｓ　ａｎｄ　ｉｍａｇｅｓ　ｂｙ　ａｔｔｅｎｔｉｏｎ　ｉｎ　ｉｍａｇｅ　ｃａｐｔｉｏｎ　ｔａｓｋ 图 ２　 看 图 说 话 任 务 中 注 意 力 实 现 单 词 与 图 片 的 对 齐［３２］

３　ｐｏｓｔ－ｈｏｃ可 解 释 性
ｐｏｓｔ－ｈｏｃ可解释 性 也 称 事 后 可 解 释 性，发 生 在 模型训练之后．对于一个给定的训练 好 的学 习模 型， ｐｏｓｔ－ｈｏｃ可解释性旨在利用解释方法或 构 建 解 释模 型，解释学 习 模 型 的 工 作 机 制、决 策 行 为 和 决 策 依

据．因此，ｐｏｓｔ－ｈｏｃ可解 释性 的重点在 于 设 计高 保 真 的解释方法或构建高精度的解释模型．
根据解释 目 的 和 解 释 对 象 的 不 同，ｐｏｓｔ－ｈｏｃ可 解释性又分为全局 可 解 释 性 和 局 部 可 解 释 性，所 对 应的方法分别称为全局解释方法和局部解释方法． 经典 的 ｐｏｓｔ－ｈｏｃ解 释 方 法 及 其 满 足 的 属 性 如 表 １ 所示：

Ｍｅｔｈｏｄ　 ｉｎＴｒｅｅ［２３］
ＳＧＬ［４０］ ＧＩＲＰ［５１］ ＭＡＧＩＸ［４２］ ＤｅｅｐＶＩＤ［４３］ ＡＭ［４４］ Ｎｇｕｙｅｎ　ｅｔ　ａｌ．［４５］

Ｔａｂｌｅ　１　Ｓｕｍｍａｒｙ　ｏｆ　Ｃｌａｓｓｉｃ　ｐｏｓｔ－ｈｏｃ　Ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　Ｍｅｔｈｏｄｓ 表１　经典的 ｐｏｓｔ－ｈｏｃ解释方法总结

Ｇ?Ｌ　 Ｇ　 Ｇ　 Ｇ　

ＭＡ?ＭＳ　 ＭＳ ＭＳ ＭＡ

ＴＭＬ　 √ √ √

ＦＣＮ　 × × √

ＣＮＮ　 × × √

ＲＮＮ　 × × √

Ｆｉｄｅｌｉｔｙ　 ○ ○ ○

Ｇ　

ＭＡ

√

√

√

√

○

Ｇ　

ＭＡ

×

×

√

×

○

Ｇ　

ＭＳ

×

√

√

×

Ｇ　

ＭＳ

×

√

√

×

Ｓｅｃｕｒｉｔｙ　
× × × ×

Ｄｏｍａｉｎ
ＣＶ?ＮＬＰ ＣＶ ＣＶ ＣＶ

２０７６

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

Ｃｏｎｔｉｎｕｅｄ （Ｔａｂｌｅ　１）

Ｍｅｔｈｏｄ　 Ｙｕａｎ　ｅｔ　ａｌ．［４６］ Ｓａｌｉｅｎｃｙ　Ｍａｓｋ［４７］
ＲＳＲＳ［４８］ ＬＩＭＥ［１３］ ＬＯＲＥ［４９］ Ａｎｃｈｏｒ［５０］ ＬＥＭＮＡ［５１］ Ｇｒａｄ［５２］ ＤｅｃｏｎｖＮｅｔ［５３］ ＧｕｉｄｅｄＢＰ［５４］ Ｉｎｔｅｇｒａｔｅｄ［５５］ ＳｍｏｏｔｈＧｒａｄ［５６］ ＬＲＰ［５７］ ＤｅｅｐＬＩＦＴ［５８］ Ｇｕｉｄｅｄ　Ｉｎｖｅｒｓｉｏｎ［５９］ ＣＡＭ［６０］ Ｇｒａｄ－ＣＡＭ［６１］
ＡＩ２［６２］ ＯｐｅｎＢｏｘ［６３］

Ｇ?Ｌ　 ＭＡ?ＭＳ　

Ｇ　

ＭＳ

Ｌ　

ＭＡ

Ｌ　

ＭＡ

Ｌ　

ＭＡ

Ｌ　

ＭＡ

Ｌ　

ＭＡ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｌ　

ＭＳ

Ｇ，Ｌ　 ＭＳ

ＴＭＬ　 × × × √ √ √ × × × × × × × × × × × × ×

ＦＣＮ　 × √ √ √ √ √ × √ × × √ √ √ √ × × × × √

ＣＮＮ　 × √ √ √ √ √ × √ √ √ √ √ √ √ √ √ √ √ ×

ＲＮＮ　 √ × × √ √ √ √ √ × × √ √ √ √ × × × × ×

Ｆｉｄｅｌｉｔｙ　 ○ ○ ○ ○ ○
○ ●

Ｓｅｃｕｒｉｔｙ　

Ｄｏｍａｉｎ

×

ＮＬＰ

×

ＣＶ

×

ＣＶ

×

ＣＶ?ＮＬＰ

×

×

ＣＶ?ＮＬＰ

×

ＮＬＰ?Ｍａｌｗａｒｅ

×

ＣＶ?ＮＬＰ

×

ＣＶ

×

ＣＶ

×

ＣＶ?ＮＬＰ

×

ＣＶ?ＮＬＰ

×

ＣＶ?ＮＬＰ

×

ＣＶ?Ｇｅｎｏｍｉｃｓ

√

ＣＶ

×

ＣＶ

×

ＣＶ

√

ＣＶ

√

ＣＶ

　Ｎｏｔｅ：Ｇ＝ｇｌｏｂａｌ，Ｌ＝ｌｏｃａｌ，ＭＡ＝ｍｏｄｅｌ－ａｇｎｏｓｔｉｃ，ＭＳ＝ｍｏｄｅｌ－ｓｐｅｃｉｆｉｃ，ＴＭＬ＝ｔｒａｄｉｔｉｏｎａｌ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ，√ ＝ｓｅｃｕｒｅ，× ＝ｎｏｔ　ｓｅｃｕｒｅ， ○＝ｌｏｗ， ＝ｍｉｄｄｌｅ，●＝ｈｉｇｈ，ＣＶ＝ｃｏｍｐｕｔｅｒ　ｖｉｓｉｏｎ，ＮＬＰ＝ｎａｔｕｒａｌ　ｌａｎｇｕａｇｅ　ｐｒｏｃｅｓｓｉｎｇ．

３．１　 全 局 解 释 机器学习模型的全局可解释性旨在帮助人们从
整体上理解模型背后的复杂逻辑以及内部的工作机 制 ，例 如 模 型 是 如 何 学 习 的 、模 型 从 训 练 数 据 中 学 到 了 什 么 、模 型 是 如 何 进 行 决 策 的 等 ，这 要 求 我 们 能 以 人类可理解的方式来表示一个训练好的复杂学习模 型．典型的 全 局 解 释 方 法 包 括 解 释 模 型?规 则 提 取、 模 型 蒸 馏 、激 活 最 大 化 解 释 等 ． ３．１．１　 规 则 提 取
早期针对模型可解释性的研究主要集中于解释 规 则 或 解 释 模 型 提 取 ，即 通 过 从 受 训 模 型 中 提 取 解 释 规 则 的 方 式 ，提 供 对 复 杂 模 型 尤 其 是 黑 盒 模 型 整 体 决 策 逻 辑 的 理 解［６４－６７］．规 则 提 取 技 术 以 难 以 理 解 的 复 杂 模 型 或 黑 盒 模 型 作 为 入 手 点 ，利 用 可 理 解 的 规 则 集 合 生成可解释的符号描述，或 从 中 提 取 可 解 释 模 型（如 决策树、基于规 则 的 模 型 等 ）［６８－７０］，使 之 具 有 与 原 模 型相当的决策 能 力．解 释 模 型 或 规 则 提 取 是 一 种 有 效的开箱技术，有效 地 提 供 了 对 复 杂 模 型 或 黑 盒 模 型内部工作机制的深入理解．根据解 释 对象不同，规 则 提 取 方 法 可 分 为 针 对 树 融 合 （ｔｒｅｅ　ｅｎｓｅｍｂｌｅ）模 型

的规则提取 和 ［２３，４０，７１－７３］ 针对神经网络的 规则提取． 针 对 复 杂 的 树 融 合 模 型 （例 如 随 机 森 林 、提 升 树
等 ）的 规 则 提 取 方 法 通 常 包 含 ４ 个 部 分 ：１）从 树 融 合 模型中提取规则，一 个 集 成 的 树 模 型 通 常 由 多 个 决 策树构成，每棵树 的 根 节 点 到 叶 子 节 点 的 每 一 条 路 径都表示一条决策 规 则，将 从 每 一 棵 决 策 树 中 提 取 的规则进行组合即可得到从树融合模型中提取的规 则；２）基于规则长 度、规 则 频 率、误 差 等 指 标 对 提 取 的规则进行排序，其 中 规 则 长 度 反 映 了 规 则 的 复 杂 度 ，规 则 频 率 反 映 满 足 规 则 的 数 据 实 例 的 比 例 ，误 差 则 反 映 了 规 则 的 决 策 能 力 ；３）基 于 排 序 结 果 ，对 规 则 中的无关项和冗余项进行剪枝并选择一组相关的非 冗余规则；４）基于 挑 选 的 规 则 构 建 一 个 可 解 释 的 规 则 学 习 器 ，用 于 决 策 和 解 释 ．
针对神经网络的规则提取方法可以分为２类： 分解 法 （ｄｅｃｏｍｐｏｓｉｔｉｏｎａｌ　ｍｅｔｈｏｄ）［４１，７４－７５］和 教 学 法 （ｐｅｄａｇｏｇｉｃａｌ　ｍｅｔｈｏｄ）［７６－７８］．分解法 的 显 著 特 点 是 注 重 从 受 训 神 经 网 络 中 提 取 单 个 单 元 （如 隐 含 单 元 、输 出单元）层次上规 则，这 要 求 神 经 网 络 是 “透 明”的， 即我们可以接 触 到 模 型 的 具 体 架 构 和 参 数．分 解 法

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０７７

要求受训神经网络中的每一个隐含单元和输出单元

的计算结果都能映射成一个对应于一条规则的二进

制结果．因此，每一个隐含单元 或输出 单元 都可以 被

解释为一个阶 跃 函 数 或 一 条 布 尔 规 则．分 解 法 通 过

聚合在单个单元级 别 提 取 的 规 则，形 成 整 个 受 训 神

经网络的复合规则 库，最 后 基 于 复 合 规 则 库 提 供 对

神经网络的整体解释．与分 解法不同，教学 法将受 训

神 经 网 络 模 型 当 作 是 一 个 黑 盒 ，即 神 经 网 络 是 “不 透

明”的，我们无法利 用 其 结 构 和 参 数 信 息，只 能 操 纵

模型的输入和输出 ．因 ［４２，７９］ 此，教学法旨在 提 取 将 输

入直接映射到输出 的 规 则，基 本 思 想 是 结 合 符 号 学

习 算 法 ，利 用 受 训 神 经 网 络 来 为 学 习 算 法 生 成 样 本 ，

最后从生成的样例中提取规则 ． ［７７］

然 而 ，规 则 提 取 方 法 提 取 的 规 则 往 往 不 够 精 确 ，

因而只能提供近似 解 释，不 一 定 能 反 映 待 解 释 模 型

的真实行为．此外，规则提取方法提 供 的可 解释性 的

质量受规则本身复 杂 度 的 制 约，如 果 从 待 解 释 模 型

中提取的规则很复杂或者提取的决策树模型深度很

深 ，那 么 提 取 的 规 则 本 身 就 不 具 备 良 好 的 可 解 释 性 ，

因而无法为待解释模型提供有效的解释．

３．１．２　 模 型 蒸 馏

当 模 型 的 结 构 过 于 复 杂 时，要 想 从 整 体 上 理 解

受训模型的决 策 逻 辑 通 常 是 很 困 难 的．解 决 该 问 题

的一个有效途径是 降 低 待 解 释 模 型 的 复 杂 度，而 模

型蒸馏（ｍｏｄｅｌ　ｄｉｓｔｉｌｌａｔｉｏｎ）则 是 降 低 模 型 复 杂 度 的 一个最典型的方法 ． ［８０］

模 型 蒸 馏 ，也 称 知 识 蒸 馏 或 模 型 模 拟 学 习 ，是 一

种经典的模型压缩 方 法，其 目 的 在 于 将 复 杂 模 型 学

习的函数压缩为具有可比性能更小、更 快 的 模 型 ． ［８１］

模型蒸馏 的 核 心 思 想 是 利 用 结 构 紧 凑 的 学 生 模 型

（ｓｔｕｄｅｎｔ　ｍｏｄｅｌ）来模拟结构 复杂 的 教师模型（ｔｅａｃｈｅｒ

ｍｏｄｅｌ），从而完成从 教 师 模 型 到 学 生 模 型 的 知 识 迁

移 过 程 ，实 现 对 复 杂 教 师 模 型 的 知 识 “蒸 馏 ”．蒸 馏 的

难点在于压缩模型结构的同时如何保留教师模型从

海量数据中学 习 到 的 知 识 和 模 型 的 泛 化 能 力．一 种

有效的解决办法是利用软目标来辅助硬目标一起训

练 学 生 模 型 ，其 中 硬 目 标 为 原 始 数 据 的 类 别 信 息 ，软

目 标 为 教 师 模 型 的 分 类 概 率 值 ，包 含 的 信 息 量 大 ，体

现了不同类别之间 相 关 关 系 的 信 息［８２］．给 定 一 个 复

杂的教师模型和一 批 训 练 数 据，模 型 蒸 馏 方 法 首 先

利用教师模型生成 软 目 标，然 后 通 过 最 小 化 软 目 标

和硬目标的联合损 失 函 数 来 训 练 学 生 模 型，损 失 函

数定义为

Ｌ αＬ ＝ ｓｔｄｕｄｅｎｔ

＋ （ｓｏｆｔ）

（１－α）Ｌ（ｈａｒｄ），

其中，Ｌ（ｓｏｆｔ）为 软 目 标 损 失，要 求 学 生 模 型 生 成 的 软 目标与教师模型生 成 的 软 目 标 要 尽 可 能 的 接 近，保 证学生 模 型 能 有 效 地 学 习 教 师 模 型 中 的 暗 知 识 （ｄａｒｋ　ｋｎｏｗｌｅｄｇｅ）；Ｌ（ｈａｒｄ）为 硬 目 标 损 失，要 求 学 生 模型能够保留教师模型良好的决策性能．
由于模型蒸馏可以完成从教师模型到学生模型 的知识迁移，因而 学 生 模 型 可 以 看 作 是 教 师 模 型 的 全局近似，在一定 程 度 上 反 映 了 教 师 模 型 的 整 体 逻 辑 ，因 此 我 们 可 以 基 于 学 生 模 型 ，提 供 对 教 师 模 型 的 全局解释．在利用模型蒸馏 作 为全 局 解 释 方法 时，学 生模型通常采用可 解 释 性 好 的 模 型 来 实 现，如 线 性 模 型、决 策 树、广 义 加 性 模 型 以 及 浅 层 神 经 网 络 等［８３－８５］．Ｈｉｎｔｏｎ等人 提 ［８２］ 出 了 一 种 知 识 蒸 馏 方 法， 通过训练单一的相对较小的网络来模拟原始复杂网 络或集成网络模型的预测概率来提炼复杂网络的知 识 ，以 模 拟 原 始 复 杂 网 络 的 决 策 过 程 ，并 且 证 明 单 一 网络能达到复 杂 网 络 几 乎 同 样 的 性 能．为 了 进 一 步 提升 蒸 馏 知 识 的 可 解 释 性，Ｆｒｏｓｓｔ 等 人 扩 ［８４］ 展 了 Ｈｉｎｔｏｎ提出的知识蒸馏方法，提 出利 用决策 树 来 模 拟复杂深度神经 网 络 模 型 的 决 策．Ｔａｎ 等 人 基 ［８５］ 于 广义加性模型的良 好 可 解 释 性，提 出 利 用 模 型 蒸 馏 的方法来学习描述输入特征与复杂模型的预测之间 关系的全局加性模 型，并 基 于 加 性 模 型 对 复 杂 模 型 进行全局解 释．Ｃｈｅ等 人 将 ［８６］ 基 于 模 型 蒸 馏 的 可 解 释方法应用于医疗 诊 断 模 型 的 可 解 释 性 研 究 中，提 出利用梯度提升树进行知识蒸馏的方式来学习可解 释模型，不仅在急 性 肺 损 伤 病 人 无 呼 吸 机 天 数 预 测 任务中取得了优异 的 性 能，而 且 还 可 以 为 临 床 医 生 提供良好的可解释性．Ｄｉｎｇ等 人 利 ［８７］ 用 知 识 蒸 馏 解 释基于社交媒体的 物 质 使 用 预 测 模 型，通 过 运 用 知 识蒸馏框架来构建 解 释 模 型，取 得 了 与 最 先 进 的 预 测模型相当的性能，而 且 还 可 以 提 供 对 用 户 的 社 交 媒体行为 与 物 质 使 用 之 间 的 关 系 深 入 理 解．Ｘｕ 等 人 开 ［８８］ 发了 ＤａｒｋＳｉｇｈｔ可 解 释 方 法，通 过 利 用 模 型 蒸馏的方式从黑盒 模 型 中 提 取 暗 知 识，并 以 可 视 化 的形式对提取的暗 知 识 进 行 呈 现，以 帮 助 分 析 师 直 观地了解模型决策逻辑．
此 外，基 于 模 型 蒸 馏 的 解 释 方 法 还 被 广 泛 地 应 用于 模 型 诊 断 与 验 证［４３，８９－９０］．Ｔａｎ 等 人 提 ［８９］ 出 了 一 种针对黑盒风险评分模型的２阶段模型审计方法， 对于一个给定的黑盒风险评分模型和一批审计数据， 该方法首先利用模型蒸馏的方法得到一个解释模型， 同时基于审计数据和其真实标签训练一个透明的结 果预测模型，并通 过 比 较 解 释 模 型 和 结 果 预 测 模 型

２０７８

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

来 理 解 特 征 与 风 险 评 分 之 间 的 相 关 关 系 ；最 后 ，通 过 使用统计测试的方式来确定黑盒模型是否使用了审 计数据中不存在的其他特征．同时，通 过评 估受保 护 特征对风险评分的贡献与其对实际结果的贡献的差 异，可以检测黑盒风险评分模型中是否存在偏差 ． ［９０］
模 型 蒸 馏 解 释 方 法 实 现 简 单 ，易 于 理 解 ，且 不 依 赖待解释模型的具 体 结 构 信 息，因 而 作 为 一 种 模 型 无关的解释方法，常被用于解 释 黑盒 机器 学 习模 型． 然而，蒸馏模型只是 对 原 始 复 杂 模 型 的 一 种 全 局 近 似，它们之间 始 终 存 在 差 距．因 此，基 于 蒸 馏 模 型 所 做出的解释不一定能反映待解释模型的真实行为． 此 外 ，知 识 蒸 馏 过 程 通 常 不 可 控 ，无 法 保 障 待 解 释 模 型从海量数据中学到的知识有效地迁移到蒸馏模型 中，因而导致解释结 果 质 量 较 低 无 法 满 足 精 确 解 释 的需要． ３．１．３　 激 活 最 大 化
在自 下 而 上 的 深 度 学 习 任 务 中，给 定 一 批 训 练 数据，ＤＮＮ 不仅可以自动地学习输入数据与输出类 别之间的映射关系，同 时 也 可 以 从 数 据 中 学 到 特 定 的特征 表 示 （ｆｅａｔｕｒｅ　ｒｅｐｒｅｓｅｎｔａｔｉｏｎ）．然 而，考 虑 到 数据集中存在偏差，我 们 无 法 通 过 模 型 精 度 来 保 证 模型表征的 可 靠 性，也 无 法 确 定 ＤＮＮ 用 于 预 测 的 内部工作模式［９１］．因此，深入 理 解 并 呈 现 ＤＮＮ 中 每 一个隐含层的神经 元 所 捕 获 的 表 征，有 助 于 从 语 义 上、视觉上帮助人们理解 ＤＮＮ 内 部 的 工 作 逻 辑 ． ［９２］ 为此，许多研究者探索如何在输入空间实现对 ＤＮＮ 任意层神经单元计 算 内 容 的 可 视 化，并 使 其 尽 可 能 通 用 ，以 便 能 够 深 入 了 解 神 经 网 络 不 同 单 元 代 表 的 特 定 含 义 ．其 中 ，最 有 效 和 使 用 最 广 泛 的 一 种 方 法是通过在特定的层上找到神经元的首选输入最 大 化 神 经 元 激 活 ，因 此 该 方 法 也 称 为 激 活 最 大 化 （ａｃｔｉｖａｔｉｏｎ　ｍａｘｉｍｉｚａｔｉｏｎ，ＡＭ）方法 ． ［５２］
激 活 最 大 化 方 法 思 想 较 为 简 单，即 通 过 寻 找 有 界范数的输入模式，最 大 限 度 地 激 活 给 定 的 隐 藏 单 元，而一个单元最大 限 度 地 响 应 的 输 入 模 式 可 能 是 一个单元 正 在 做 什 么 的 良 好 的 一 阶 表 示 ．给 ［４４，９３－９４］ 定一个 ＤＮＮ 模 型，寻 找 最 大 化 神 经 元 激 活 的 原 型 样本 ｘ＊ 的 问 题 可 以 被 定 义 成 一 个 优 化 问 题，其 形 式化定义为
ｘ＊ ＝ａｒｇ　ｍａｘ（ｆｌ（ｘ）－λ ｘ　２）， ｘ
其 中，优化目标第一项ｆｌ（ｘ）为 ＤＮＮ 第ｌ 层某一个 神经元在 当 前 输 入 ｘ 下 的 激 活 值；第 ２ 项 为２ 正 则，用于保证优 化 得 到 的 原 型 样 本（ｐｒｏｔｏｔｙｐｅ）与 原 样本尽可能地 接 近．整 个 优 化 过 程 可 以 通 过 梯 度 上

升来求解．最后，通过可视化生成的原型 样 本 ｘ＊ ，可 以帮助我们理解该神经元在其感受野中所捕获到的 内容．当然，我 们 可 以 分 析 任 意 层 的 神 经 元，以 理 解 ＤＮＮ 不同层所编码的不同表 示 内容．当 我 们 分 析 输 出层神经元的最大 激 活 时，可 以 找 到 某 一 类 别 所 对 应的最具代表性的原型样本．
激 活 最 大 化 方 法 虽 然 原 理 简 单，但 如 何 使 其 正 常工作同样面 临 着 一 些 挑 战．由 于 样 本 搜 索 空 间 很 大，优化过程可能 产 生 含 有 噪 声 和 高 频 模 式 的 不 现 实图像，导致原型 样 本 虽 能 最 大 化 神 经 元 激 活 却 难 以理解．为了 获 取 更 有 意 义、更 自 然 的 原 型 样 本，优 化 过 程 必 须 采 用 自 然 图 像 先 验 约 束 ，为 此 ，一 些 研 究 者创造性地提出了人工构造先验，包括α 范数、高斯 模糊等 ．此 ［９５－９６］ 外，一 些 研 究 者 将 激 活 最 大 化 框 架 与 生成模型相结合，利 用 生 成 模 型 产 生 的 更 强 的 自 然 图像先验 正 则 化 优 化 过 程．Ｎｇｕｙｅｎ 等 人 提 ［４５］ 出 利 用生成对抗网络与激活最大化优化相结合的方法来 生 成 原 型 样 本 ，优 化 问 题 被 重 定 义 为
ｚ＊ ＝ａｒｇ　ｍａｘ（ｆｌ（ｇ（ｚ））－λ ｚ　２）， ｚ∈Ｚ
其 中 ，第 １ 项 为 解 码 器 与 原 神 经 元 激 活 值 的 结 合 ，第 ２项为代 码 空 间 中 的２ 正 则．该 方 法 不 直 接 优 化 图 像 ，转 而 优 化 代 码 空 间 以 找 到 可 以 最 大 化 神 经 元 激 活 的解ｚ＊ ，一旦最优解ｚ＊ 找到，则可 以 通 过 解 码 得 到 原型样本ｚ＊ ，即ｘ＊ ＝ｇ（ｚ＊ ）．实 验 结 果 表 明（如 图 ３ 所示），将激活最大化与生成模型相结合 的方法 可 以 产生更真实、更具有可解释性的原型样本．从图 ３ 可 以 看 出 ：模 型 成 功 捕 获 了 与 类 别 相 对 应 的 特 征 表 示 ．
对不同层生成的原型样本的可视化结果表明， ＤＮＮ 在若干抽象层次上进行表示学习，从模型的 第 一层到最后一层，模 型 学 习 到 的 特 征 表 征 由 局 部 过 渡到整体，由一般任务过渡 到 特 定 任 务．以 图像 分 类 任务中的 ＣＮＮ 为 例，低 层 神 经 元 通 常 可 以 捕 获 到 图 片 中 的 颜 色 、边 缘 等 信 息 ；中 间 层 神 经 元 有 更 复 杂 的 不 变 性 ，可 以 捕 获 相 似 的 纹 理 ；中 高 层 神 经 元 可 以 捕获图片中的显著 变 化，并 可 以 聚 焦 到 特 定 类 别 对 应 的 局 部 特 征 ，如 狗 的 脸 部 、鸟 的 脚 部 等 ；最 后 ，高 层 神经元则通过组合 局 部 特 征 表 征，从 而 学 习 到 整 个 分类目标的整体表 征［５３］．此 外，神 经 元 具 有 多 面 性， 可以对与同一语义概念相关的不同图像做出反应， 例如，人脸检测神经 元 可 以 同 时 对 人 脸 和 动 物 面 孔 做 出反 应 ． ［９７］
激活最大化解释方法是一种模型相关的解释方 法 ，相 比 规 则 提 取 解 释 和 模 型 蒸 馏 解 释 ，其 解 释 结 果 更 准 确 ，更 能 反 映 待 解 释 模 型 的 真 实 行 为 ．同 时 ，利 用

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０７９

Ｆｉｇ．３　Ｃｌａｓｓ－ｄｉｓｃｒｉｍｉｎａｔｉｖｅ　ｐｒｏｔｏｔｙｐｅｓ　ｇｅｎｅｒａｔｅｄ　ｂｙ　ｃｏｍｂｉｎｉｎｇ　ｇｅｎｅｒａｔｉｖｅ　ｍｏｄｅｌ　ｗｉｔｈ　ａｃｔｉｖａｔｉｏｎ　ｍａｘｉｍｉｚａｔｉｏｎ 图３　 利用生成模型与激活最大化相结合生成的类别对应原型样本［４５］

激 活 最 大 化 解 释 方 法 ，可 从 语 义 上 、视 觉 上 帮 助 人 们 理解模型是如何从数据中进行学习的以及模型从数 据中学到了什么．然而，激活最 大化本身 是一个 优 化 问 题 ，在 通 过 激 活 最 大 化 寻 找 原 型 样 本 的 过 程 中 ，优 化过程中的噪音和不确定性可能导致产生的原型样 本难以解释．尽 管 可 以 通 过 构 造 自 然 图 像 先 验 约 束 优化过程来解决这 一 问 题，但 如 何 构 造 更 好 的 自 然 图像先验本身就是一大难题．此外，激 活最 大化方 法 只能用于优化连续 性 数 据，无 法 直 接 应 用 于 诸 如 文 本、图数据等 离 散 型 数 据［４６］，因 而 该 方 法 难 以 直 接 用于解释自然语言处理模型和图神经网络模型． ３．２　 局 部 解 释
机器学习模型的局部可解释性旨在帮助人们理 解学习模型针对每一个特定输入样本的决策过程和 决策依据．与全局可解释性 不同，模型 的局 部可 解 释 性以输入样本为导 向，通 常 可 以 通 过 分 析 输 入 样 本 的每一维特征对模型最终决策结果的贡献来实现． 在 实 际 应 用 中 ，由 于 模 型 算 法 的 不 透 明 性 、模 型 结 构 的复杂性以及应用 场 景 的 多 元 性，提 供 对 机 器 学 习 模型的全局解释通 常 比 提 供 局 部 解 释 更 困 难，因 而 针对模型局部可解 释 性 的 研 究 更 加 广 泛，局 部 解 释 方法相对于全 局 解 释 方 法 也 更 常 见．经 典 的 局 部 解 释 方 法 包 括 敏 感 性 分 析 解 释 、局 部 近 似 解 释 、梯 度 反 向 传 播 解 释 、特 征 反 演 解 释 以 及 类 激 活 映 射 解 释 等 ． ３．２．１　 敏 感 性 分 析
敏感 性 分 析 （ｓｅｎｓｉｔｉｖｉｔｙ　ａｎａｌｙｓｉｓ）是 指 在 给 定 的一组假设下，从定 量 分 析 的 角 度 研 究 相 关 自 变 量 发生某种变化对某一特定的因变量影响程度的一种 不确定分析技术［９８］，其核心思 想 是 通 过 逐 一 改 变 自

变量的值来解释因变量受自变量变化影响大小的规

律．敏感性分析 被 广 泛 地 应 用 于 机 器 学 习 及 其 应 用 中，如机器 学 习 模 型 分 析 、生 ［９９－１０１］ 态 建 模 等 ［１０２］ ．近

年 来 ，敏 感 性 分 析 作 为 一 种 模 型 局 部 解 释 方 法 ，被 用

于分析待解释样本的每一维特征对模型最终分类结

果的影响 ，以 ［１０３－１０５］ 提 供 对 某 一 个 特 定 决 策 结 果 的

解释．根据是否需要利用模型 的 梯度 信 息，敏 感性 分

析方法可分为模型相关方法和模型无关方法．

模型相关方法利用模型的局部梯度信息评估特

征 与 决 策 结 果 的 相 关 性 ，常 见 的 相 关 性 定 义 为

（ ） Ｒｉ（ｘ）＝

ｆ（ｘ）

２
，

ｘｉ

其 中 ，ｆ（ｘ）为 模 型 的 决 策 函 数 ，ｘｉ 为 待 解 释 样 本 ｘ

的 第ｉ 维 特 征 ．直 观 地 ，相 关 性 分 数 Ｒｉ（ｘ）可 以 看

ｄ
∑ 作 是 模 型 梯 度 的２ 范 数 的 分 解 ，即 Ｒｉ（ｘ）＝ ｉ＝１ "ｆ（ｘ）２．在模型相关方法中，相 关 性 分 数 Ｒｉ（ｘ）

可通过梯度反向传播来求 解．最后，通 过以热 力图 的

形式可视化相关性分数可以直观地理解输入的每一

维特征对决策结果的影响程度．

在 模 型 无 关 敏 感 性 分 析 方 法 中，待 解 释 模 型 可

以 看 作 是 黑 盒 ，我 们 无 需 利 用 模 型 的 梯 度 信 息 ，只 关

注待解释样本特征值变化对模型最终决策结果的影

响．Ｒｏｂｎｉｋ－ｉｋｏｎｊａ等人 提 ［１０６］ 出 通 过 对 输 入 样 本 单

个属性值的预测进行分解的方式来观察属性值对该

样本预测结果的影响．具体 地，该方法 通 过 观 察 去 掉

某一特定属性前后模型预测结果的变化来确定该属

性 对 预 测 结 果 的 重 要 性 ，即 ：

Ｒｉ（ｘ）＝ｆ（ｘ）－ｆ（ｘ＼ｘｉ）．

２０８０

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

类似地，Ｌｉｕ等 人 提 ［４８］ 出 了 “限 制 支 持 域 集”的 概 念 ，它 被 定 义 为 一 组 受 大 小 限 制 且 不 重 叠 的 区 域 ， 并且满足如下属性：删 除 任 何 一 个 区 域 将 会 导 致 模 型分类出错．其本质思想是，如 果某个特 定 区域的 缺 失导致模型分类结 果 发 生 反 转，则 该 区 域 必 定 为 模 型正确决策提供支持．因此，最 终可通 过分 析特 定 图 像区域是否存在与模型决策结果之间的依赖关系来 可视化模型决策规则．Ｆｏｎｇ等人 提 ［４７］ 出了一种基于

有意义扰动的敏感 性 分 析 方 法，通 过 添 加 扰 动 或 删 除待解释图片的不同区域来最小化模型目标类别分 类概率的方式学习 一 个 显 著 性 掩 码，以 识 别 对 模 型 决策结果影响最大 的 图 像 部 分，并 可 视 化 显 著 性 掩 码作 为 对 该 决 策 结 果 的 解 释，如 图 ４ 所 示．Ｌｉ等 人 则 ［１０７］ 提出通过观 察 修 改 或 删 除 特 征 子 集 前 后 模 型决策结果的相应变化的方式来推断待解释样本的 决策特征．

Ｆｉｇ．４　Ｌｅａｒｎ　ａ　ｓａｌｉｅｎｃｙ　ｍａｓｋ　ｂｙ　ｂｌｕｒｒｉｎｇ　ａｎ　ｉｍａｇｅ　ｔｏ　ｍｉｎｉｍｉｚｅ　ｔｈｅ　ｐｒｏｂａｂｉｌｉｔｙ　ｏｆ　ｉｔｓ　ｔａｒｇｅｔ　ｃｌａｓｓ 图４　 通过图像模糊的方式最小化分类概率来学习显著性掩码［４７］

　 　 然 而 ，敏 感 性 分 析 方 法 解 释 的 是 决 策 函 数 ｆ（ｘ） 局部变化对决策结 果 的 影 响，而 不 是 解 释 决 策 函 数 本身，只能捕获到单 个 特 征 对 最 终 决 策 结 果 的 影 响 程 度 ，而 不 一 定 关 注 实 际 的 决 策 相 关 特 征 ，因 而 相 关 性 分 值 Ｒｉ（ｘ）对 应 的 热 力 图 在 空 间 上 是 分 散 而 不 连 续的．因此，敏感性分析方法提 供的解 释结果 通常 相 对粗糙且难以理解．此外，敏感性 分析 方法 无法解 释 特征之间的相关关系对最终决策结果的影响． ３．２．２　 局 部 近 似
局部近似解释方法的核心思想是利用结构简单 的可解释模型拟合待解释模型针对某一输入实例的 决策结果，然后基于 解 释 模 型 对 该 决 策 结 果 进 行 解 释．该方法通 常 基 于 如 下 假 设：给 定 一 个 输 入 实 例， 模型针对该实例以及该实例邻域内样本的决策边界 可以通过可解 释 的 白 盒 模 型 来 近 似．在 整 个 数 据 空 间 中 ，待 解 释 模 型 的 决 策 边 界 可 以 任 意 的 复 杂 ，但 模 型针对某一特定实 例 的 决 策 边 界 通 常 是 简 单 的，甚 至是近线性的［１３］．我 们 通 常 很 难 也 不 需 要 对 待 解 释 模型的整体决策边 界 进 行 全 局 近 似，但 可 在 给 定 的 实例及其邻域内利用可解释模型对待解释模型的局 部决策边界进行近 似，然 后 基 于 可 解 释 模 型 提 供 对 待解释模型的决策依据的解释．
Ｒｉｂｅｉｒｏ等 人［１３］基 于 神 经 网 络 的 局 部 线 性 假 设，提出了一种 模 型 无 关 局 部 可 解 释 方 法 （ＬＩＭＥ）． 具体地，对于每一个输入实例，ＬＩＭＥ 首先利用该实 例以及该实例的一组近邻训练一个易于解释的线性

回归模型来拟合待 解 释 模 型 的 局 部 边 界，然 后 基 于 该线性模 型 解 释 待 解 释 模 型 针 对 该 实 例 的 决 策 依 据 ，其 中 ，线 性 模 型 的 权 重 系 数 直 接 体 现 了 当 前 决 策 中该实例的 每 一 维 特 征 重 要 性．Ｇｕｉｄｏｔｔｉ等 人 提 ［４９］ 出了一种适用于关系表数据的基于局部规则的黑盒 模型决策 结 果 解 释 方 法 （ＬＯＲＥ）．给 定 一 个 二 分 类 模型ｆ 及一个由ｆ 标记的特定实例 ｘ，ＬＯＲＥ 首先 利用ａｄ－ｈｏｃ遗传算 法 生 成 给 定 实 例 ｘ 的 一 组 平 衡 邻 居 实 例 来 构 建 一 个 简 单 的 、可 解 释 的 预 测 模 型 ，以 逼近 二 分 类 模 型 ｆ 针 对 实 例ｘ 的 决 策 边 界；然 后， 基于该解释模型，从 生 成 的 实 例 集 合 中 提 取 一 个 决 策 树 模 型 ；最 后 ，从 决 策 树 模 型 中 提 取 决 策 规 则 作 为 对实例ｘ 的 分 类 结 果 的 局 部 解 释．Ｒｉｂｅｉｒｏ等 人［５０，１０８］ 提出了一种称之为锚点解释（ａｎｃｈｏｒ）的 局部 解 释 方 法，针对每一个输入 实 例，该 方 法 利 用 被 称 之 为 “锚 点”的ｉｆ－ｔｈｅｎ 规 则 来 逼 近 待 解 释 模 型 的 局 部 边 界． Ａｎｃｈｏｒ方 法 充 分 地 结 合 了 模 型 无 关 局 部 解 释 方 法 的 优点和规则的良好可解释性，在 Ａｎｃｈｏｒ方法中用 于 解 释 的 “锚 点 ”通 常 是 直 观 、易 于 理 解 的 ，而 且 解 释 覆 盖 范 围 非 常 清 晰 ．通 过 构 造 ，“锚 点 ”不 仅 可 以 与 待 解 释模型保持一致，而 且 还 可 以 以 确 保 正 确 理 解 和 高 保真的方式将待解释模型的决策行为传达给用户．
然而，ＬＩＭＥ，ＬＯＲＥ 以 及 Ａｎｃｈｏｒ等 解 释 方 法 均假设输入样本的 特 征 相 互 独 立，因 而 无 法 准 确 地 解释诸如 ＲＮＮ 等专门对序列 数 据 中 的 依 赖 关 系 进 行建模 的 模 型．为 此，Ｇｕｏ等 人 提 ［５１］ 出 了 ＬＥＭＮＡ，

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０８１

一种专用于安全应用场景 中 的 ＲＮＮ 模 型 的 高 保 真 解释方法，其 核 心 思 想 与 ＬＩＭＥ 等 方 法 相 似，即 利 用可解释模 型 来 近 似 ＲＮＮ 的 局 部 决 策 边 界，并 针 对每一个输入实例，产 生 一 组 可 解 释 的 特 征 以 解 释 针 对 该 实 例 的 决 策 依 据．与 ＬＩＭＥ 不 同 的 是， ＬＥＭＮＡ 假设待解 释 模 型 的 局 部 边 界 是 非 线 性 的， 为了保证解 释 的 保 真 度，ＬＥＭＮＡ 通 过 训 练 混 合 回 归模型来近似 ＲＮＮ 针对每个 输 入 实 例 的 局 部 决 策 边界．此外，ＬＥＭＮＡ 引入了 融 合 Ｌａｓｓｏ正 则 来 处 理 ＲＮＮ 模型中的特征依赖问题，有效 地弥补 了 ＬＩＭＥ 等方法的不足．
基 于 局 部 近 似 的 解 释 方 法 实 现 简 单，易 于 理 解 且不依赖待解释模 型 的 具 体 结 构，适 于 解 释 黑 盒 机 器学习模型．但 解 释 模 型 只 是 待 解 释 模 型 的 局 部 近 似 ，因 而 只 能 捕 获 模 型 的 局 部 特 征 ，无 法 解 释 模 型 的 整体决策行为．针对每一个 输入实例，局部近 似解 释 方法均需要重新训练一个解释模型来拟合待解释模 型针对该实例的决 策 结 果，因 而 此 类 方 法 的 解 释 效 率通常不高．此外，大多数的局 部近似 解释 方法 假 设 待解释实例的特征 相 互 独 立，因 此 无 法 解 释 特 征 之 间的相关关系对决策结果的影响． ３．２．３　 反 向 传 播
基于反 向 传 播 （ｂａｃｋ　ｐｒｏｐａｇａｔｉｏｎ）的 解 释 方 法 的核心思想是利用 ＤＮＮ 的 反 向 传 播 机 制 将 模 型 的 决策重要性信号从模型的输出层神经元逐层传播到 模型的输入以推导输入样本的特征重要性．
Ｓｉｍｏｎｙａｎ等人 最 ［５２］ 先提 出 了 利 用 反 向 传 播 推 断特征重要性的 解 释 方 法（Ｇｒａｄ），通 过 利 用 反 向 传 播算法计算模型的输出相对于输入图片的梯度来求 解 该 输 入 图 片 所 对 应 的 分 类 显 著 图 （Ｓａｌｉｅｎｃｙ　Ｍａｐ）． 与 Ｇｒａｄ方法类似，Ｚｅｉｌｅｒ等人 提 ［５３］ 出了反卷积网络 （ＤｅｃｏｎｖＮｅｔ），通过将 ＤＮＮ 的高层激活反 向 传 播到 模型的输 入 以 识 别 输 入 图 片 中 负 责 激 活 的 重 要 部 分 ．不 同 的 是 ，在 处 理 线 性 整 流 单 元 （ＲｅＬＵ）过 程 中 ， 当使用 Ｇｒａｄ 方 法 反 向 传 播 重 要 性 时，如 果 正 向 传 播过程中 ＲｅＬＵ 的输 入 为 负，则 反 向 传 播 过 程 中 传 入 ＲｅＬＵ 的梯度值为零．而在反卷积网络中反向传播 一个重要 信 号 时，当 且 仅 当 信 号 值 为 负，进 入 ＲｅＬＵ 的重要信号被置零，而 不 考 虑 前 向 传 播 过 程 中 输 入 到 ＲｅＬＵ 的 信 号 的 符 号．Ｓｐｒｉｎｇｅｎｂｅｒｇ 等 人［５４］将 Ｇｒａｄ方法与反卷 积 网 络 相 结 合 提 出 了 导 向 反 向 传 播方法（ＧｕｉｄｅｄＢＰ），通过在反 向 传 播 过 程 中 丢 弃 负 值来修改 ＲｅＬＵ 函 数 的 梯 度．与 只 计 算 输 出 针 对 当 前输 入 的 梯 度 不 同，Ｓｕｎｄａｒａｒａｊａｎ 等 人 提 ［５５］ 出 了 一

种集成梯度方法（Ｉｎｔｅｇｒａｔｅｄ），该方法 通 过计算 输 入 从某些起始值按比例放大到当前值的梯度的积分代 替单一梯度，有 效 地 解 决 了 ＤＮＮ 中 神 经 元 饱 和 问 题导致无法利用梯度信息反映特征重要性的问题．
然而，Ｇｒａｄ，ＧｕｉｄｅｄＢＰ 以 及Ｉｎｔｅｇｒａｔｅｄ 等 方 法 通过反向传播所得到的显著图通常包含很多视觉可 见 的 噪 音 ，如 图 ５ 所 示 ，而 我 们 无 法 确 定 这 种 噪 音 是 否真实地反映 了 模 型 在 分 类 过 程 中 的 决 策 依 据．为 此，Ｓｍｉｌｋｏｖ等人 提 ［５６］ 出 了 一 种 平 滑 梯 度 的 反 向 传 播解释方法（ＳｍｏｏｔｈＧｒａｄ），该方法通 过向 输 入 样 本 中引入噪声解决了 Ｇｒａｄ等 方 法 中 存 在 的 视 觉 噪 音 问题．ＳｍｏｏｔｈＧｒａｄ方法的核心思想是通过向 待解 释 样本中添加噪声对 相 似 的 样 本 进 行 采 样，然 后 利 用 反向传播方法求解 每 个 采 样 样 本 的 决 策 显 著 图，最 后将所有求解得到的显著图进行平均并将其作为对 模型针对该样本的决策结果的解释．
尽管上述基于梯度反向传播的方法可以定位输 入样本中决策特征，但 却 无 法 量 化 每 个 特 征 对 模 型 决策结果的贡献程度．因此，Ｌａｎｄｅｃｋｅｒ等人 提 ［１０９］ 出 一种贡献传播方法，该 方 法 首 先 利 用 加 性 模 型 计 算 ＤＮＮ 高层特征对模型分类结果的贡献，然后通过 反 向传播将高层特征的贡献逐层传递到模型的输入， 以确定每一层的每一个神经元节点对其下一层神经 元节点的相对贡献．给定一个 待解 释 样 本，该方法 不 仅可以定位样本中 的 重 要 特 征，而 且 还 能 量 化 每 一 个特征对于分类结果的重要 性．Ｂａｃｈ 等 人 则 ［５７］ 提 出 了一种分层相关 性 传 播 方 法 （ＬＲＰ），用 于 计 算 单 个 像素对 图 像 分 类 器 预 测 结 果 的 贡 献．一 般 形 式 的 ＬＲＰ 方法假设分类器可以 被 分 解 为 多 个 计 算 层，每 一层都可以被建模为一个多维向量并且该多维向量 的每一维都对应一个相 关 性 分 值，ＬＲＰ 的 核 心 则 是 利用反向传播将高层的相关性分值递归地传播到低 层直至传播到输入 层．Ｓｈｒｉｋｕｍａｒ等 人 对 ［５８］ ＬＲＰ 方 法进行了改进（ＤｅｅｐＬＩＦＴ），通过在输入空间中 定 义 参考点并参考神经元激活的变化按比例传播相关分 数．其研究结果表明，在不进 行数 值 稳定 性 修 正 的 情 况下，原始 ＬＲＰ 方法 的 输 出 结 果 等 价 于 Ｇｒａｄ 方 法 所求显著图与 输 入 之 间 的 乘 积．与 梯 度 反 向 传 播 方 法不同的是，ＬＲＰ 方 法 不 要 求 ＤＮＮ 神 经 元 的 激 活 是可微的或平滑 的．基 于 此 优 点，Ｄｉｎｇ 等 人 首 ［１１０］ 次 将 ＬＲＰ 方 法 应 用 于 基 于 注 意 力 机 制 的 编 码 器 － 解 码器框架，以度量神 经 网 络 中 任 意 ２ 个 神 经 元 之 间 关联程度的相关性．在汉英 翻 译 案例中的研究 表 明， 该方法有助于解释神经机器翻译系统的内部工作机

２０８２

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

Ｆｉｇ．５　Ｃｏｍｐａｒｉｓｏｎ　ｏｆ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｑｕａｌｉｔｙ　ｏｆ　ｆｏｕｒ　ｇｒａｄｉｅｎｔ　ｂａｃｋ－ｐｒｏｐａｇａｔｉｏｎ　ｂａｓｅｄ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｍｅｔｈｏｄｓ 图 ５　４种 梯 度 反 向 传 播 解 释 方 法 解 释 效 果 对 比［５９］

制并分 析 翻 译 错 误．类 似 地，Ａｒｒａｓ等 人 将 ［１１１］ ＬＲＰ 方法引入到自然语 言 处 理 任 务 中，并 且 从 定 性 和 定 量的 角 度 证 明 ＬＲＰ 方 法 既 可 以 用 于 文 档 级 别 的 细 粒 度 分 析 ，也 可 以 作 为 跨 文 档 的 数 据 集 级 别 的 分 析 ， 以识别对分类器决策很重要的单词．
基 于 反 向 传 播 的 解 释 方 法 通 常 实 现 简 单、计 算 效率高且充分利用了模型 的结构特性．然而，从理 论 上 易 知 ，如 果 预 测 函 数 在 输 入 附 近 变 得 平 坦 ，那 么 预 测函数相 对 于 输 入 的 梯 度 在 该 输 入 附 近 将 变 得 很 小，进而导致无法利 用 梯 度 信 息 定 位 样 本 的 决 策 特 征．尽管Ｉｎｔｅｇｒａｔｅｄ 方 法 在 一 定 程 度 上 解 决 了 该 问 题 ，但 同 时 也 增 加 了 计 算 开 销 ，并 且Ｉｎｔｅｇｒａｔｅｄ 方 法 的解释结果中依然存在许多人类无法理解的噪音． 此 外 ，梯 度 信 息 只 能 用 于 定 位 重 要 特 征 ，而 无 法 量 化 特征对决策结果的 重 要 程 度，利 用 基 于 重 要 性 或 相 关性反向传播的解释方法则可以解决该问题． ３．２．４　 特 征 反 演
尽 管 敏 感 性 分 析、局 部 近 似 以 及 梯 度 反 向 传 播 等方法在一定程度上可以提供对待解释模型决策结 果的局部解释，但它 们 通 常 忽 略 了 待 解 释 模 型 的 中 间层，因而遗漏了大量的中间信 息．而利 用 模型 的 中 间层信息，我们能 更 容 易 地 表 征 模 型 在 正 常 工 作 条 件下的决策行为，进而可提供更 准确 的解 释 结果．特 征反演（ｆｅａｔｕｒｅ　ｉｎｖｅｒｓｉｏｎ）作 为 一 种 可 视 化 和 理 解

ＤＮＮ 中间特征表征的技术，可以充分利用模型的中 间层信息，以提供 对 模 型 整 体 行 为 及 模 型 决 策 结 果 的解释．
特征反演解释方法可分为模型 级（ｍｏｄｅｌ－ｌｅｖｅｌ） 解释方 法 和 实 例 级 （ｉｎｓｔａｎｃｅ－ｌｅｖｅｌ）解 释 方 法．模 型 级解释方法旨 在 从 输 入 空 间 中 寻 找 可 以 表 示 ＤＮＮ 神经元 所 学 到 的 抽 象 概 念 的 解 释 原 型 （如 激 活 最 大化方法），并通过可视化 和 理 解 ＤＮＮ 每 一 层 特 征 表示的方式，提 供 对 ＤＮＮ 每 一 层 所 提 取 信 息 的 理 解 ．然 ［５２，９５，１１２－１１３］ 而 ，模 型 级 解 释 方 法 的 反 演 结 果 通 常 相 对 粗 糙 且 难 以 理 解 ，此 外 ，如 何 从 输 入 样 本 中 自 动化提取用于模型决策的重要特征仍然面临着巨大 的挑战．针对模型级方法的 不 足，实例 级 特征 反演 方 法试图回答输 入 样 本 的 哪 些 特 征 被 用 于 激 活 ＤＮＮ 的神经元以做出特定的决策．其 中，最具 代表 性的 是 Ｄｕ等人 提 ［５９］ 出 的 一 个 实 例 级 特 征 反 演 解 释 框 架， 该框架通过在执行导向特征反演过程中加入类别依 赖约束，不仅可以 准 确 地 定 位 待 输 入 实 例 中 的 用 于 模型决策的重要特征 （如 图 ６ 所 示），还 可 以 提 供 对 ＤＮＮ 模型决策过程的深入理解． ３．２．５　 类 激 活 映 射
最新研究表明：ＣＮＮ 不同层次的卷积 单元 包 含 大量的位置 信 息，使 其 具 有 良 好 的 定 位 能 力［１１４］．基 于卷积单元的定位 能 力，我 们 可 以 定 位 出 输 入 样 本

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０８３

Ｆｉｇ．６　Ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｅｘａｍｐｌｅ　ｏｆ　ｇｕｉｄｅｄ　ｆｅａｔｕｒｅ　ｉｎｖｅｒｓｉｏｎ　ｍｅｔｈｏｄ 图 ６　 导 向 特 征 反 演 方 法 解 释 示 例［５９］

中用于 ＣＮＮ 决 策 的 核 心 区 域，如 分 类 任 务 中 的 决 策特征、目标 检 测 任 务 中 的 物 体 位 置 等．然 而，传 统 ＣＮＮ 模型通常在 卷 积 和 池 化 之 后 采 用 全 连 接 层 对 卷积层提取的特征 图 进 行 组 合 用 于 最 终 决 策，因 而 导致网络的定位能力丧失．
为解决这 一 问 题，Ｚｈｏｕ 等 人 提 ［６０］ 出 了 类 激 活 映 射 （ｃｌａｓｓ　ａｃｔｉｖａｔｉｏｎ　ｍａｐｐｉｎｇ，ＣＡＭ）解 释 方 法 ，该 方法利用全局 平 均 池 化 （ｇｌｏｂａｌ　ａｖｅｒａｇｅ　ｐｏｏｌｉｎｇ）层 来替代传统 ＣＮＮ 模 型 中 除 ｓｏｆｔｍａｘ 层 以 外 的 所 有 全连接层，并通过将 输 出 层 的 权 重 投 影 到 卷 积 特 征 图来识别图像 中 的 重 要 区 域．具 体 地，ＣＡＭ 首 先 利 用全局平均池化操作 输 出 ＣＮＮ 最 后 一 个 卷 积 层 每 个单元的特征图的 空 间 平 均 值，并 通 过 对 空 间 平 均 值进 行 加 权 求 和 得 到 ＣＮＮ 的 最 终 决 策 结 果．同 时， ＣＡＭ 通过计算最后一个卷积层的特征图的加权和， 得到 ＣＮＮ 模型 的 类 激 活 图，而 一 个 特 定 类 别 所 对 应的类激活图则反映 了 ＣＮＮ 用 来 识 别 该 类 别 的 核 心图像区域．最后，通过以热 力 图 的 形 式 可 视 化 类 激 活图得到最终的解 释 结 果．研 究 结 果 表 明，全 局 平 均

池化层的优势远不止于作为一个正则器来防止网络 过 拟 合 ，事 实 上 ，通 过 稍 加 调 整 ，全 局 平 均 池 化 还 可 以 将 ＣＮＮ 良好的定位能力保留到网络的最后一层［６０］．
然 而，ＣＡＭ 方 法 需 要 修 改 网 络 结 构 并 重 训 练 模型，因 而 在 实 际 应 用 中 并 不 实 用．因 此，Ｓｅｌｖａｒａｊｕ 等人 对 ［６１］ ＣＡＭ 方法进 行 了 改 进，提 出 了 一 种 将 梯 度信息与特征映射相结合的梯度加权类激活映射方 法（Ｇｒａｄ－ＣＡＭ）．给 定 一 个 输 入 样 本，Ｇｒａｄ－ＣＡＭ 首 先计算目标类别相对于最后一个卷积层中每一个特 征图的梯度并对梯 度 进 行 全 局 平 均 池 化，以 获 得 每 个 特 征 图 的 重 要 性 权 重 ；然 后 ，基 于 重 要 性 权 重 计 算 特征图的加权激活，以 获 得 一 个 粗 粒 度 的 梯 度 加 权 类激活图，用于定 位 输 入 样 本 中 具 有 类 判 别 性 的 重 要区域，如 图 ７（ｃ）所 示．与 ＣＡＭ 相 比，Ｇｒａｄ－ＣＡＭ 无需修改网络架构 或 重 训 练 模 型，避 免 了 模 型 的 可 解释性与准确性之 间 的 权 衡，因 而 可 适 用 于 多 种 任 务以及任何 基 于 ＣＮＮ 结 构 的 模 型，对 于 全 卷 积 神 经网络，Ｇｒａｄ－ＣＡＭ 退 化 为 ＣＡＭ 方 法．尽 管 Ｇｒａｄ－ ＣＡＭ 具有良好的类别判别能力并能很好地定位相关

Ｆｉｇ．７　Ｖｉｓｕａｌｉｚａｔｉｏｎ　ｏｆ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｒｅｓｕｌｔｓ　ｏｆ　Ｇｒａｄ－ＣＡＭ　ａｎｄ　Ｇｕｉｄｅｄ　Ｇｒａｄ－ＣＡＭ　ｍｅｔｈｏｄｓ 图７　Ｇｒａｄ－ＣＡＭ 与 Ｇｕｉｄｅｄ　Ｇｒａｄ－ＣＡＭ 方法解释结果可视化［６１］

２０８４

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

图像区 域，但 缺 乏 诸 如 ＤｅｃｏｎｖＮｅｔ［５３］和 ＧｕｉｄｅｄＢＰ［５４］ 等像素级别梯度可视化解释方法显示细粒度特征重 要性的能 力［６１］．为 获 得 更 细 粒 度 的 特 征 重 要 性，作 者将 Ｇｒａｄ－ＣＡＭ 与 ＧｕｉｄｅｄＢＰ 方法相结合提出了导 向梯度加权类 激 活 映 射 方 法 （Ｇｕｉｄｅｄ　Ｇｒａｄ－ＣＡＭ）， 该方法首先利用双线性插值将梯度加权类激活图上 采样到输 入 图 片 分 辨 率 大 小，然 后 点 乘 ＧｕｉｄｅｄＢＰ 方法的输出结果，得 到 细 粒 度 的 类 判 别 性 特 征 定 位 图，如 图 ７（ｄ）所 示．研 究 结 果 表 明，Ｇｕｉｄｅｄ　Ｇｒａｄ－ ＣＡＭ 方法解释效果优于 ＧｕｉｄｅｄＢＰ 和 Ｇｒａｄ－ＣＡＭ．
类 激 活 映 射 解 释 方 法 实 现 简 单 、计 算 效 率 高 ， 解释结果视觉效果 好 且 易 于 理 解，但 这 类 方 法 只 适 用于解释 ＣＮＮ 模 型，很 难 扩 展 到 全 连 接 神 经 网 络 （ＦＣＮ）以 及 ＲＮＮ 等 模 型．此 外，ＣＡＭ 方 法 需 要 修 改网络结构并重训 练 模 型，模 型 的 准 确 性 与 可 解 释 性之间始终存在一 个 权 衡，且 针 对 重 训 练 模 型 做 出 的解释结果与原待解释模型的真实行为之间存在一 定的不一 致 性，因 而 在 真 实 应 用 场 景 中 很 难 适 用． Ｇｒａｄ－ＣＡＭ 虽然解决了 ＣＡＭ 需要进 行网络 修改 和 模型 重 训 练 的 问 题，但 仍 然 与 ＣＡＭ 方 法 一 样 只 能 提供粗粒度的解释 结 果，无 法 满 足 安 全 敏 感 应 用 场 景（如自动驾驶、医 疗 诊 断 等）中 对 精 细 化 解 释 的 需 要．Ｇｕｉｄｅｄ　Ｇｒａｄ－ＣＡＭ 方 法 作 为 ＣＡＭ 和 Ｇｒａｄ－ＣＡＭ 的 加 强 版 ，既 不 需 要 修 改 网 络 结 构 或 重 训 练 模 型 ，又 能提供更细粒度的 解 释 结 果，但 由 于 引 入 了 导 向 反 向传播方法，因而该 方 法 同 样 存 在 由 于 负 梯 度 归 零 导致无法定位与模型决策结果呈负相关的样本特征 的 局 限 性 ． ［１１５］ ３．２．６　 其 他 方 法
除了 上 述 ５ 种 典 型 的 局 部 可 解 释 方 法 外，其 他 研究者从不同的角度对模型可解释性进行了深入研 究，并提出 了 一 些 新 的 局 部 解 释 方 法，包 括 抽 象 解 释 和 ［６２］ 准确 一 致 解 释 等 ［６３］ ．
针对 ＤＮＮ 系统的可靠 分 析 技 术 所 面 临 的 主 要 挑战是如何在解释神经网络某些特性的同时将其扩 展到大规模的 ＤＮＮ 分类 器，因 此，分 析 方 法 必 须 考 虑到任何经过大量中间神经元处理的大规模输入集 上所有可能的 模 型 输 出 结 果．由 于 模 型 的 输 入 空 间 通常是巨大的，因而 通 过 在 所 有 可 能 的 输 入 样 本 上 运行模型 来 检 查 它 们 是 否 满 足 某 一 特 性 是 不 可 行 的．为 解 决 这 一 挑 战，避 免 状 态 空 间 爆 炸，Ｇｅｈｒ 等 人 将 ［６２］ 程 序 分 析 中 的 经 典 抽 象 解 释 框 架 应 用 于 ＤＮＮ 分析，首次提出了可扩展的、可用于验证和分析 ＤＮＮ 安全性和 鲁 棒 性 的 抽 象 解 释 系 统 （ＡＩ２）．具 体

地，ＡＩ２ 首 先 构 造 一 个 包 含 一 系 列 逻 辑 约 束 和 抽 象 元素的数值 抽 象 域；由 于 ＤＮＮ 的 每 一 层 处 理 的 是 具体的数值，因而抽象元素 无 法 在网络中 传 播．为 解 决此 问 题，ＡＩ２ 通 过 定 义 一 个 被 称 之 为 抽 象 转 换 器 （ａｂｓｔｒａｃｔ　ｔｒａｎｓｆｏｒｍｅｒ）的 函 数 将 ＤＮＮ 的 每 一 层 转 换为对应的 抽 象 层，并 基 于 抽 象 元 素 过 近 似 （ｏｖｅｒ－ ａｐｐｒｏｘｉｍａｔｉｏｎ）原神经网络 每 一 层 的 处 理 函 数 以 捕 获其真实行为；最后，ＡＩ２ 基于 抽 象 转 换 器 返 回 的 抽 象结果，分 析 并 验 证 神 经 网 络 的 鲁 棒 性 和 安 全 性． ＡＩ２ 不 用 真 正 运 行 ＤＮＮ 模 型 即 可 验 证 ＤＮＮ 的 某 些特定属性，因而 计 算 效 率 高，可 扩 展 到 大 规 模、更 复杂的 ＤＮＮ 网络．但 由 于 采 用 了 过 近 似 处 理，尽 管 ＡＩ２ 能提供可靠的解释但无法保证解释的准确性．
现有局部解释方法包括抽象解释都很难保证解 释 结 果 的 准 确 性 和 一 致 性 ，为 此 ，许 多 学 者 开 始 研 究 针对 ＤＮＮ 模 型 的 精 确 解 释 方 法．Ｃｈｕ 等 人 提 ［６３］ 出 了一种准确 一 致 的 解 释 方 法 （ＯｐｅｎＢｏｘ），可 为 分 段 线性神经网络（ＰＬＮＮ）家族 模型 提 供 精 确 一 致 的 解 释．作者研究证明，ＰＬＮＮ 在数学上 等价 于 一系 列的 局部线性分类器，其 中 每 一 个 线 性 分 类 器 负 责 分 类 输 入 空 间 中 的 一 组 样 本．因 此，给 定 一 个 待 解 释 ＰＬＮＮ 模型，ＯｐｅｎＢｏｘ首先利 用 神 经 网 络 的 前 向 传 播机制和矩 阵 运 算 将 给 定 的 ＰＬＮＮ 模 型 表 示 成 数 学上与之等价的、由 一 系 列 数 据 依 赖 的 局 部 线 性 分 类 器 组 成 的 线 性 解 释 模 型 ；然 后 ，针 对 每 一 个 待 解 释 样本，ＯｐｅｎＢｏｘ基于该样 本 所 对 应 的 局 部 线 性 分 类 器提供对 ＰＬＮＮ 分 类 结 果 的 解 释．研 究 结 果 表 明， 由于线性解释模型数学上与 待解 释 ＰＬＮＮ 等 价，因 此基于线性解释模型给出的解释结果能精确地反映 ＰＬＮＮ 的真实决策行 为，并 且 线 性 解 释 模 型 针 对 每 一个输入的 决 策 结 果 与 待 解 释 ＰＬＮＮ 的 决 策 结 果 完全一致，从而解 决 了 模 型 的 可 解 释 性 与 准 确 性 之 间的权衡难题．此 外，针 对 近 似 的 样 本，ＯｐｅｎＢｏｘ 可 以给出一 致 的 解 释，保 证 了 解 释 结 果 的 一 致 性．然 而，ＯｐｅｎＢｏｘ作 为 针 对 ＰＬＮＮ 家 族 的 特 定 解 释 方 法 ，只 能 解 释 线 性 神 经 网 络 模 型 ，无 法 用 于 解 释 非 线 性神经网络模型．此外，如何将其扩展到 ＣＮＮ，ＲＮＮ 等更复杂的神经网络模型同样面临着巨大的挑战．
４　 可 解 释 性 应 用
机器学习模型可解释性相关技术潜在应用非常 广泛，具体包括模 型 验 证、模 型 诊 断、辅 助 分 析 以 及 知识发现等．

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０８５

４．１　 模 型 验 证 传统的模型验证方法通常是通过构造一个与训
练集不相交的验证 集，然 后 基 于 模 型 在 验 证 集 上 的 误差来评估模型的 泛 化 性 能，从 而 提 供 对 模 型 好 坏 的一个粗粒度的验证．然而，由于 数据 集中可能 存 在 偏 差 ，并 且 验 证 集 也 可 能 与 训 练 集 同 分 布 ，我 们 很 难 简单地通过评估模型在验证集上的泛化能力来验证 模型的可靠性，也很 难 验 证 模 型 是 否 从 训 练 数 据 中 学到了真正的 决 策 知 识．以 冰 原 狼 与 哈 士 奇 的 分 类 为例，由于训练集 中 所 有 冰 原 狼 样 本 图 片 的 背 景 均 为雪地，导致分类 模 型 可 能 从 训 练 集 中 学 到 数 据 偏 差从而将雪作为冰 原 狼 的 分 类 特 征，又 由 于 验 证 集 与训练集同分布，模 型 在 验 证 集 上 的 分 类 性 能 与 在 训练集上的性能同 样 优 异，因 而 导 致 传 统 的 模 型 验 证方法将该模型识 别 为 一 个 好 的 分 类 模 型［１３］．很 显 然 ，这 样 的 模 型 通 常 是 不 可 靠 的 ，一 旦 模 型 在 推 理 阶 段 遇 到 背 景 为 雪 地 的 哈 士 奇 样 本 图 片 ，分 类 模 型 会 做 出 错 误 的 决 策 ，而 模 型 的 这 种 行 为 将 会 给 实 际 场 景 尤 其是风险敏感场景中的真实应用带来潜在的威胁．
针 对 传 统 模 型 验 证 方 法 的 不 足，我 们 可 以 利 用 模型的可解释性及相关解释方法对模型可靠性进行 更细粒度的评估和 验 证，从 而 消 除 模 型 在 实 际 部 署 应用中的潜在 风 险．基 于 可 解 释 性 的 模 型 验 证 方 法 一 般 思 路 如 下 ：首 先 构 造 一 个 可 信 验 证 集 ，消 除 验 证 集 中 可 能 存 在 的 数 据 偏 差 ，保 证 验 证 数 据 的 可 靠 性 ； 然 后 ，基 于 可 信 验 证 集 ，利 用 相 关 解 释 方 法 提 供 对 模 型整体决策行为（全 局 解 释）或 模 型 决 策 结 果 （局 部 解释）的解释；最后，基 于 解 释 方 法 给 出 的 解 释 结 果

并结合人类认知，对 模 型 决 策 行 为 和 决 策 结 果 的 可 靠性进行验证，以 检 查 模 型 是 否 在 以 符 合 人 类 认 知 的形式正常工作．
在 冰 原 狼 与 哈 士 奇 分 类 的 例 子 中，Ｒｉｂｅｉｒｏ 等 人 利 ［１３］ 用 局 部 解 释 方 法 ＬＩＭＥ 解 释 分 类 模 型 针 对 一个背景为雪的哈 士 奇 图 片 的 分 类 结 果，发 现 分 类 模型将该图片错误 地 分 类 为 冰 原 狼，而 解 释 方 法 给 出的解释结果表明模型做出决策的依据是图片背景 中的雪，如图８（ａ）所 示．很 显 然，该 解 释 结 果 与 人 类 的认知相违背，表明 模 型 在 学 习 的 过 程 中 错 误 地 将 雪作为冰原狼的决 策 特 征，从 而 证 明 该 模 型 是 不 可 靠的．类似地，Ｌａｐｕｓｃｈｋｉｎ等人 利 ［１１６］ 用 ＬＲＰ 解释 方 法定性 地分析一个从ＩｍａｇｅＮｅｔ中 迁 移 训练 得 到的 ＣＮＮ 模型和一个在 ＰＡＳＣＡＬ　ＶＯＣ　２００７ 数 据 集 上 训练得到 的 Ｆｉｓｈｅｒ向 量 （ＦＶ）分 类 器 的 决 策 结 果， 以检测训练数 据 中 的 潜 在 缺 陷 和 偏 差．研 究 结 果 表 明 ，尽 管 ２ 个 模 型 具 有 相 似 的 分 类 精 度 ，但 在 对 输 入 样本进行分类 时 却 采 用 了 完 全 不 同 的 分 类 策 略．从 ＬＲＰ 解释方法给出的解释结果可以看出，如图８（ｂ） 所示，在 对轮 船图 片进行分类 时，ＦＶ 分类 器依 据 的 是海水特征，而 ＣＮＮ 模 型 则 能 正 确 地 捕 获 到 轮 船 的轮廓信息．与此同时，如果将 位 于 水外 的 轮 船 作 为 测试 样 本，ＦＶ 分 类 器 的 分 类 性 能 将 大 幅 下 降，而 ＣＮＮ 模型则几乎不 受 影 响．这 一 验 证 结 果 表 明，ＦＶ 分类器的决策行为存在偏差而 ＣＮＮ 模型表现正 常． 因此，我们认为 ＣＮＮ 模 型 比 ＦＶ 分 类 器 更 可 靠，在 进行模型选 择 时，我 们 将 会 选 择 ＣＮＮ 模 型 作 为 最 终的分类模型．

Ｆｉｇ．８　Ｅｘａｍｐｌｅｓ　ｏｆ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ－ｂａｓｅｄ　ｍｏｄｅｌ　ｖａｌｉｄａｔｉｏｎ 图 ８　 基 于 可 解 释 性 的 模 型 验 证 示 例

　　而对于可解释方法所识别出的不可靠的模型， 我们可以采取相应的对策 来进行改 进．比如 说，我 们 可以通过在训练模 型 时 引 入 归 纳 偏 置，提 高 模 型 在 预测阶段的泛化能 力，从 而 使 其 能 对 未 知 样 本 做 出 正确的决策．我们也可以通过修正训练集 分布，消 除 数据中存在的偏差，并 利 用 修 正 后 的 数 据 集 重 训 练 模型达到消除模型决策偏差的目的．

４．２　 模 型 诊 断 由于 机 器 学 习 模 型 内 部 工 作 机 制 复 杂、透 明 性
低，模型开发人员 往 往 缺 乏 可 靠 的 推 理 或 依 据 来 辅 助他们进行模型开 发 和 调 试，因 而 使 得 模 型 开 发 迭 代 过 程 变 得 更 加 耗 时 且 容 易 出 错 ．而 模 型 可 解 释 性 相 关技术作为一种细粒度分析和解释模型的有效手段， 可用于分析和调试 模 型 的 错 误 决 策 行 为，以 “诊 断”

２０８６

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

模型中存在的缺陷，并 为 修 复 模 型 中 的 缺 陷 提 供 有 力的支撑．近年来，随着模型可 解释性 研究 不断取 得 新的突破，基于可解 释 性 的 机 器 学 习 模 型 诊 断 相 关 研究也吸引了越来越多的关注 ． ［１１７－１２０］
研究 表 明：基 于 模 型 特 征 表 示 可 视 化 以 及 中 间 层分析的解释方法（如 激 活 最 大 化、特 征 反 演 等）可 以有效地用于 解 释 和 诊 断 复 杂 模 型．典 型 的 解 决 方 案包括可视化模型的中间激活状态或内部特征表示 以及可视 化 模 型 中 的 数 据 流 图 ，以 ［１２１－１２３］ 增 强 对 复 杂模型的解释和理 解，同 时 分 析 和 评 估 模 型 或 算 法 的性能，为 在 模 型 开 发 的 不 同 阶 段 （如 前 期 特 征 工 程 、中 期 超 参 调 整 以 及 后 期 模 型 微 调 等 ）交 互 式 改 进 模型提供有 效 的 指 导［１２４］．此 外，一 些 其 他 的 研 究 方 法 则 通 过 识 别 与 模 型 “漏 洞 ”相 关 的 重 要 特 征 或 实 例 来进行模 型 诊 断 和 调 试．Ｋｒａｕｓｅ 等 人 基 ［１２５］ 于 敏 感 性分析解释方法的思想，设计了一个 名为 Ｐｒｏｓｐｅｃｔｏｒ 的系统，通过修改特 征 值 并 检 查 预 测 结 果 的 相 应 变 化来确定敏 感 性 特 征．Ｃａｄａｍｕｒｏ 等 人 提 ［１１７］ 出 了 一 种概念分析和诊断 循 环 的 模 型 诊 断 方 法，允 许 终 端 用户迭代地检测模 型 “漏 洞”，以 找 到 对 模 型 “漏 洞” 贡献最大的训练实 例，从 而 确 定 模 型 出 错 的 根 本 原 因．Ｋｒａｕｓｅ等人 提 ［１２６］ 出 了 一 个 可 视 化 模 型 诊 断 工 作流，通过利用局部 解 释 方 法 度 量 输 入 实 例 中 的 局 部特征相关性，以帮 助 数 据 科 学 家 和 领 域 专 家 理 解 和诊断模型所做出的决策．具体地，该 工作 流首先 利 用聚合统计查看数据在正确决策和错误决策之间的 分 布 ；然 后 ，基 于 解 释 方 法 理 解 用 于 做 出 这 些 决 策 的 特 征 ；最 后 基 于 原 始 数 据 ，对 影 响 模 型 决 策 的 潜 在 根 本原因进行深入分析．
针对已发现的模 型 “漏 洞”，我 们 可 以 基 于 模 型 诊断方法给出的推 理 结 果，采 取 相 应 的 措 施 对 模 型 进 行 “治 疗 ”，如 提 高 训 练 数 据 的 质 量 、选 择 可 靠 特 征 以及调整模 型 超 参 等．Ｐａｉｖａ等 人 提 ［１２７］ 出 了 一 种 可 视化数据分类方法，该 方 法 通 过 点 布 局 策 略 实 现 数 据集的可视化，允许 用 户 选 择 并 指 定 用 于 模 型 学 习 过 程 的 训 练 数 据，从 而 提 高 训 练 集 的 整 体 质 量． Ｂｒｏｏｋｓ等人 提 ［１２８］ 出了一个用于 改 进 特 征 工 程 的 交 互式可视化分析系 统，该 系 统 支 持 错 误 驱 动 的 特 征 构思过程并为误分类样本提供交互式可视化摘要， 允许在误分类样本和正确分类样本之间进行特征级 别 的 比 较 ，以 选 择 能 减 小 模 型 预 测 错 误 率 的 特 征 ，从 而 提 高 模 型 性 能 并 修 复 模 型 中 的 “漏 洞 ”． ４．３　 辅 助 分 析
除 了 用 于 模 型 验 证 与 模 型 诊 断 之 外，可 解 释 性

相关技术还可用于 辅 助 分 析 与 决 策，以 提 高 人 工 分 析和决策的效率．相关研究 表明，基于 可 解释 性的 辅 助分析技术在医疗 数 据 分 析、分 子 模 拟 以 及 基 因 分 析等多个领域取得 了 巨 大 的 成 功，有 效 地 解 决 了 人 工分析耗时费力的难题．
在智 慧 医 疗 领 域，许 多 学 者 尝 试 将 深 度 学 习 及 可解释性技术应用 于 构 建 自 动 化 智 能 诊 断 系 统，以 辅助医护人员分析 病 人 的 医 疗 诊 断 数 据，从 而 提 高 人工诊断的效率［６，１２９］．Ｒａｊｐｕｒｋａｒ等人 基 ［６］ 于大 规 模 病人胸片数据开发了基于深度学习的肺炎检测系统 （ＣｈｅＸＮｅｔ），其检 测 性 能 甚 至 超 过 了 放 射 科 医 师 的 诊断 水 平，该 系 统 通 过 将 可 解 释 方 法 ＣＡＭ 应 用 于 解释检测系统的决策依据并可视化对应的解释结果 （如图９所示），可 以 为 医 师 分 析 病 人 医 疗 影 像 数 据 以快 速 定 位 病 人 的 病 灶 提 供 大 量 的 辅 助 信 息． Ａｒｖａｎｉｔｉ等人 研 ［１２９］ 究结果表明，在 给 定 一 个 良 好 标 注的数据集 的 前 提 下，可 以 利 用 ＣＮＮ 模 型 成 功 地 实现对前列腺 癌 组 织 微 阵 列 的 自 动 格 里 森 分 级．同 时 ，利 用 解 释 方 法 给 出 自 动 分 级 系 统 的 分 级 依 据 ，可 实现病理专家级的 分 级 效 果，从 而 为 简 化 相 对 繁 琐 的分级任务提供了支撑．
Ｆｉｇ．９　Ａｐｐｌｉｃａｔｉｏｎ　ｏｆ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｉｎ　ｍｅｄｉｃａｌ　ｄｉａｇｎｏｓｉｓ 图 ９　 可 解 释 方 法 在 医 疗 诊 断 中 的 应 用［６］
在 量 子 化 学 领 域，分 子 动 力 学 模 拟 是 理 解 化 学 反 应 机 理 、速 率 和 产 率 的 关 键 ，然 而 由 于 分 子 的 完 整 波 函 数 相 对 复 杂 ，且 难 以 计 算 和 近 似 ，导 致 人 们 通 常 难以理解，因而如 何 创 建 人 类 可 解 释 的 分 子 表 示 成 为２１世纪物 质 模 拟 的 一 大 挑 战［１３０］．为 解 决 这 一 难 题，许多学者将机器 学 习 及 可 解 释 性 技 术 引 入 到 分 子模拟任务中，用 于 辅 助 分 析 分 子 结 构 与 分 子 性 质 之间的关系 ．其 ［１３１－１３３］ 中，Ｓｃｈüｔｔ等人 提 ［１３３］ 出一种通 过结合强大的结构和表示能力以实现较高预测性能 和良好可解 释 性 的 深 度 张 量 神 经 网 络 （ＤＴＮＮ），用 于预测分子结构与电子性质之 间的 关 系．同 时，作 者 利用基于测试电荷扰动的敏感性分析方法测量在给 定的位置插入电荷对 ＤＴＮＮ 输出结果的影 响，从而 找到与解释分子结构与性质关系最相关的每个单独

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０８７

的分子空间 结 构．Ｈｓｅ等 人 提 ［１３２］ 出 一 种 利 用 机 器 学习来辅助分子动 力 学 模 拟 的 方 法，该 方 法 利 用 模 拟产生的大 量 数 据 训 练 贝 叶 斯 神 经 网 络 （ＢＮＮ）来 预测１，２－二氧杂 环 丁 烷 从 初 始 核 位 置 的 离 解 时 间． 为了 构 建 一 个 可 解 释 的 ＢＮＮ 模 型，作 者 将 模 型 的 权重和偏置分布参 数 化 为 拉 普 拉 斯 分 布，以 确 定 与 准确预测离解时间以及实际的物理过程相关的输入 特征．研究结果表明，该方法不 仅可以 准确地 再现 化 合物的离解过程，而 且 能 自 动 地 从 模 拟 数 据 中 提 取 相关信息，而 不 需 要 预 先 了 解 相 关 化 学 反 应．同 时， 通过解释 ＢＮＮ 所捕获 的 特 征 与 实 际 物 理 过 程 之 间 的 相 关 关 系 ，可 以 在 不 了 解 电 子 结 构 的 情 况 下 ，确 定 核坐标与离解时间 之 间 的 物 理 相 关 性，从 而 为 人 们 在化学领域取得概念性的突破提供灵感．
在基 因 组 分 析 领 域，由 基 因 组 学 研 究 不 断 进 步 而产生的数据爆炸，给 传 统 的 基 因 组 分 析 方 法 带 来 了巨大的挑战，同时 也 给 数 据 驱 动 的 深 度 学 习 技 术 在基因组分析研究中的发展 和 应 用 带 来 了 机 遇 ． ［１３４］ 相关研究表明，深度 学 习 在 基 因 组 分 析 中 的 应 用 已 突显出了其 强 大 的 优 势 ．然 ［１３５－１３８］ 而，人 们 期 望 深 度 学习模型不仅能成 功 地 预 测 结 果，还 能 识 别 有 意 义 的基因序 列，并 对 所 研 究 的 科 学 问 题 （如 基 因 与 疾 病、药物之间的关系）提 供 进 一 步 的 见 解，因 而 模 型 的可解释性 在 应 用 中 显 得 至 关 重 要．Ｌａｎｃｈａｎｔｉｎ 等 人 将 ［１３７］ ３种 ＤＮＮ 模型（即 ＣＮＮ，ＲＮＮ 以及 ＣＮＮ－ ＲＮＮ）应用于预 测 给 定 的 ＤＮＡ 序 列 中 某 一 特 定 的 转录因子是否有结 合 位 点，并 且 提 出 了 一 套 基 于 解 释方法的可视化策 略，用 于 解 释 对 应 的 预 测 模 型 并 从中提取隐含的序列模式．其中，作者 基于 反向传 播 解释方法，通 过 计 算 预 测 概 率 相 对 于 输 入 ＤＮＡ 序 列的梯度来构建显著图［５２］，用 于 度 量 并 显 示 核 苷 酸 的重要性．同 时，作 者 利 用 时 间 域 输 出 分 值 来 识 别 ＤＮＮ 序列中与特 定 转 录 因 子 结 合 位 点 相 关 的 关 键 序列位置，并利用类 激 活 最 大 化 方 法 生 成 与 特 定 预 测结果相关的 Ｍｏｔｉｆ模式．实 验 结 果 证 明，这 一 系 列 的可视化策 略 可 为 研 究 人 员 分 析 ＤＮＡ 序 列 结 构、 组成成分与特定转录因子结合位点之间的关系提供 大量的辅助信息．类 似 地，Ａｌｉｐａｎａｈｉ等 人 构 ［１３８］ 建 了 一个名为 ＤｅｅｐＢｉｎｄ的系统，通 过 训 练 一 个 ＣＮＮ 模 型将 ＤＮＡ 和 ＲＮＡ 序列映射到蛋白质结合位点上， 以了解 ＤＮＡ 和 ＲＮＡ 结合蛋白 的 序 列 特 异 性．为 了 进一步探索遗传变 异 对 蛋 白 质 结 合 位 点 的 影 响，作 者采用了基于扰动 的 敏 感 性 分 析 方 法，通 过 计 算 突 变对 ＤｅｅｐＢｉｎｄ预测结果的影响生成“突变图”，以解

释序列中每个可能的点突破对结合亲和力的影响． 作者表明，ＤｅｅｐＢｉｎｄ可 用 于 揭 示 ＲＮＡ 结 合 蛋 白 质 在 选 择 性 剪 接 中 的 调 节 作 用 ，并 辅 助 研 究 人 员 分 析 、 识别、分组及可视 化 可 影 响 转 录 因 子 结 合 和 基 因 表 达 的 疾 病 相 关 遗 传 变 异 ，从 而 有 望 实 现 精 准 医 学 ． ４．４　 知 识 发 现
近 年 来 ，随 着 人 工 智 能 相 关 技 术 的 发 展 ，基 于 机 器学习的自动决策系统被广泛地应用到各个领域， 如恶意程序分析、自动化医疗 诊 断 以 及 量化 交 易 等． 然而，由于实际任 务 的 复 杂 性 以 及 人 类 认 知 和 领 域 知识的局限性，人们 可 能 无 法 理 解 决 策 系 统 给 出 的 结 果 ，因 而 缺 乏 对 相 关 领 域 问 题 更 深 入 的 理 解 ，进 而 导致许多科学 问 题 难 以 得 到 有 效 的 解 决．最 新 研 究 成果表明，通过将 可 解 释 性 相 关 技 术 与 基 于 机 器 学 习的自动决策系统 相 结 合，可 有 效 地 挖 掘 出 自 动 决 策系统从数据中学 到 的 新 知 识，以 提 供 对 所 研 究 科 学问题的深入理解，从 而 弥 补 人 类 认 知 与 领 域 知 识 的局限性．
在 二 进 制 分 析 领 域，许 多 潜 在 的 启 发 式 方 法 都 是针对某一个特定 的 函 数 的，而 挖 掘 这 些 潜 在 的 方 法通常需要丰富的 领 域 知 识，因 而 很 难 通 过 人 工 的 方式对所有的启发 式 方 法 进 行 汇 总．Ｇｕｏ 等 人 将 ［５１］ 可解释方法 ＬＥＭＮＡ 应 用 于 一 个 基 于 ＬＳＴＭ 的 二 进制函数入口检测器，以提 供 对 ＬＳＴＭ 检 测结果 的 解释．通过分析解释结果，作者 发 现 检测模型 确 实 从 训练数据中学到了用于识别函数入口的潜在特征， 这表明利用 ＬＥＭＮＡ 解 释 方 法 可 以 挖 掘 出 检 测 模 型从数据中学到的 新 知 识，从 而 对 总 结 针 对 某 个 特 殊函数的所有潜在的启发式方法提供帮助．
在 医 疗 保 健 领 域，由 于 病 人 病 理 错 综 复 杂 且 因 人而异，医护人员 往 往 无 法 通 过 有 限 的 医 疗 诊 断 知 识挖掘潜在的致病 因 素 及 其 之 间 的 相 互 作 用，而 对 潜在因素的忽视极其可能 带 来 致 命 的威胁．Ｙａｎｇ等 人 基 ［４１］ 于重 症 监 护 室 （ＩＣＵ）治 疗 记 录 数 据 构 建 了 一个带注意 力 机 制 的 ＲＮＮ 模 型，用 于 分 析 医 疗 条 件与ＩＣＵ 死亡率之间的关系，而 这些 关 系 在 以往 的 医疗实践中往 往 没 有 得 到 很 好 的 研 究．作 者 研 究 结 果表明，利用可解 释 性 技 术 有 助 于 发 现 与 医 疗 保 健 中某些结果相关的 潜 在 影 响 因 素 或 相 互 作 用，从 而 使得从自动化医疗诊断模型中学习新的诊断知识成 为可能．
此 外 ，作 为 知 识 发 现 的 重 要 手 段 ，模 型 可 解 释 性 及其相关解释方法还被广泛地应用到了数据挖掘领 域，以从海量数据中自动地挖掘隐含的新知识 ． ［１３９－１４２］

２０８８

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

这类研究核心思想是基于所研究的领域及科学目标 构建海量数据集，然 后 对 构 建 的 数 据 集 进 行 清 洗 并 利用机器学习模型从清洗后的数据中提取数据映射 模式，最后利用解释 方 法 从 挖 掘 到 的 数 据 模 式 识 别 代表新知识的模式并利用可视化技术将新知识呈现 给用户．
５　 可 解 释 性 与 安 全 性 分 析
模型可解释性研究的初衷是通过构建可解释的 模型或设计解释方 法 提 高 模 型 的 透 明 性，同 时 验 证 和评估模 型 决 策 行 为 和 决 策 结 果 的 可 靠 性 和 安 全 性，消除模型 在 实 际 部 署 应 用 中 的 安 全 隐 患．然 而， 模型可解释性相关技术同样可以被攻击者利用以探 测机器学习模型中 的 “漏 洞”，因 而 会 给 机 器 学 习 模 型以及真实应用场景中尤其是风险敏感场景中的机 器学习应用带来威胁．此外，由于 解释 方法 与待解 释 模型之间可能存在 不 一 致 性，因 而 可 解 释 系 统 或 可 解释方法本身就存在一定的安全风险． ５．１　 安 全 隐 患 消 除
如第 ４ 节 中 所 述，模 型 可 解 释 性 及 相 关 解 释 方 法不仅可以用于评 估 和 验 证 机 器 学 习 模 型，以 弥 补 传统模型验证方法 的 不 足，保 证 模 型 决 策 行 为 和 决 策结果的可靠性和 安 全 性，还 可 用 于 辅 助 模 型 开 发 人员和安全分析师诊断和调试模型以检测模型中的 缺 陷 ，并 为 安 全 分 析 师 修 复 模 型 “漏 洞 ”提 供 指 导 ，从 而消除模型在实际部署应用中的安全 隐 患．并且，通 过同时向终端用户提供模型的预测结果及对应的解 释 结 果 ，可 提 高 模 型 决 策 的 透 明 性 ，进 而 有 助 于 建 立 终端用户与决策系统之间的信任关系．
除 了 用 于 消 除 上 述 内 在 安 全 隐 患 之 外，模 型 可 解释性相关技 术 还 可 以 帮 助 抵 御 外 在 安 全 风 险．人 工 智 能 安 全 领 域 相 关 研 究 表 明 即 使 决 策 “可 靠 ”的 机 器学习模型也同样 容 易 受 到 对 抗 样 本 攻 击，只 需 要 在输入样本中添加 精 心 构 造 的、人 眼 不 可 察 觉 的 扰 动就可以轻 松 地 让 模 型 决 策 出 错 ．这 ［８，１４３－１４４］ 种 攻 击 危害性大、隐蔽性 强、变 种 多 且 难 以 防 御，严 重 地 威 胁着人工智能 系 统 的 安 全．而 现 存 防 御 方 法 大 多 数 是针对某一个特定的对抗样本攻击设计的静态的经 验性防御，因 而 防 御 能 力 极 其 有 限．然 而，不 管 是 哪 种攻击方法，其本质 思 想 都 是 通 过 向 输 入 中 添 加 扰 动以转移模型的决策注意力，最 终使 模型决策出 错． 由于这种攻击使得 模 型 决 策 依 据 发 生 变 化，因 而 解 释方法针对对抗样本的解释结果必然与其针对对应

的正常样本的解释结果不 同．因此，我 们可以 通过 对 比并利用这种解释 结 果 的 反 差 来 检 测 对 抗 样 本，而 这种方法并不特定 于 某 一 种 对 抗 攻 击，因 而 可 以 弥 补传统经验性防御的不足．
除 上 述 防 御 方 法 外，很 多 学 者 从 不 同 的 角 度 提 出了一些新的基于可解释性技术的对抗防御方法． 其中，Ｔａｏ等人 认 ［１４５］ 为对抗攻击与模型的可解释 性 密 切 相 关 ，即 对 于 正 常 样 本 的 决 策 结 果 ，可 以 基 于 人 类可感知的特征或 属 性 来 进 行 推 理，而 对 于 对 抗 样 本的决策结果我们则通常无 法 解 释．基于 这一 认 知， 作者提出一种针对人脸识别模型的对抗样本检测方 法，该方法首先利 用 敏 感 性 分 析 解 释 方 法 识 别 与 人 类可感知属性相对 应 的 神 经 元，称 之 为 “属 性 见 证” 神 经 元 ；然 后 ，通 过 加 强 见 证 神 经 元 同 时 削 弱 其 他 神 经元将原始模型转 换 为 属 性 导 向 模 型，对 于 正 常 样 本 ，属 性 导 向 模 型 的 预 测 结 果 与 原 始 模 型 一 致 ，对 于 对 抗 样 本 二 者 预 测 结 果 则 不 一 致 ；最 后 ，利 用 ２ 个 模 型预测结果的不一 致 性 来 检 测 对 抗 样 本，实 现 对 对 抗攻击的防御．Ｌｉｕ等 人 则 ［１４６］ 基 于 对 分 类 模 型 的 解 释，提出了一种新的对抗样 本 检 测 框 架．给 定一个 恶 意样本检测器，该 框 架 首 先 选 择 一 个 以 确 定 为 恶 意 样本的样本子集作 为 种 子 样 本，然 后 构 建 一 个 局 部 解释器解 释 种 子 样 本 被 分 类 器 视 为 恶 意 样 本 的 原 因，并通过朝着解 释 器 确 定 的 规 避 方 向 来 扰 动 每 一 个种子样本的方式产生对 抗样 本．最后，通 过利 用 原 始数据和生成的对抗样本对检测器进行对抗训练， 以提高检测器对对 抗 样 本 的 鲁 棒 性，从 而 降 低 模 型 的外在安全风险． ５．２　 安 全 威 胁
尽管可解释性技术是为保证模型可靠性和安全 性而设计的，但其 同 样 可 以 被 恶 意 用 户 滥 用 而 给 实 际部署应用的机器学习系统 带 来安全 威胁．比如 说， 攻击者可以利用解释方法探测能触发模型崩溃的模 型 漏 洞 ，在 对 抗 攻 击 中 ，攻 击 者 还 可 以 利 用 可 解 释 方 法 探 测 模 型 的 决 策 弱 点 或 决 策 逻 辑 ，从 而 为 设 计 更 强 大的攻击提 供详细 的信 息．在 本 文 中，我 们 将 以 对 抗 攻击为例，阐述可解释性技术可能带来的安全风险．
在 白 盒 对 抗 攻 击 中，攻 击 者 可 以 获 取 目 标 模 型 的 结 构 、参 数 信 息 ，因 而 可 以 利 用 反 向 传 播 解 释 方 法 的思想来 探 测 模 型 的 弱 点［１４７］．其 中，Ｇｏｏｄｆｅｌｌｏｗ 等 人 提 ［１４３］ 出了快速梯度符号攻击 方 法（ＦＧＳＭ），通 过 计算模型 输 出 相 对 于 输 入 样 本 的 梯 度 信 息 来 探 测 模型的敏感性，并 通 过 朝 着 敏 感 方 向 添 加 一 个 固 定 规模的噪音来生成对抗样本．Ｐａｐｅｒｎｏｔ等 人 基 ［１４８］ 于

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０８９

Ｇｒａｄ［５２］解 释 方 法 提 出 了 雅 可 比 显 著 图 攻 击 （ＪＳＭＡ）， 该攻击方 法 首 先 利 用 Ｇｒａｄ 解 释 方 法 生 成 显 著 图， 然后基于选择 图 来 选 择 最 重 要 的 特 征 进 行 攻 击．利 用 Ｇｒａｄ方法提供的特征重要性信息，ＪＭＳＡ 攻击只 需要扰动少量的特征就能达到很高的攻击成功率， 因而攻击的隐蔽性更强．对于黑盒对 抗攻击，由于 无 法获取模型的结构 信 息，只 能 操 纵 模 型 的 输 入 和 输 出［１４９］，因而攻 击 者 可 以 利 用 模 型 无 关 解 释 方 法 的 思想 来 设 计 攻 击 方 法．其 中，Ｐａｐｅｒｎｏｔ等 人 提 ［１５０］ 出 了一种针 对 黑 盒 机 器 学 习 模 型 的 替 代 模 型 攻 击 方 法．该方法首先 利 用 模 型 蒸 馏 解 释 方 法 的 思 想 训 练 一个替代模型来拟 合 目 标 黑 盒 模 型 的 决 策 结 果，以 完 成 从 黑 盒 模 型 到 替 代 模 型 的 知 识 迁 移 过 程 ；然 后 ， 利用已有的攻击方法针对替代模型生成对抗样本； 最后，利用生成的 对 抗 样 本 对 黑 盒 模 型 进 行 迁 移 攻 击．Ｌｉ等 人 提 ［９］ 出 了 一 种 基 于 敏 感 性 分 析 解 释 方 法 的文本对抗攻击 方 法 （ＴｅｘｔＢｕｇｇｅｒ），用 于 攻 击 真 实 场景中的情感 分 析 模 型 和 垃 圾 文 本 检 测 器．该 方 法 首先通过观察去掉某个词前后模型决策结果的变化 来定位文本中的重 要 单 词，然 后 通 过 利 用 符 合 人 类 感知的噪 音 逐 个 扰 动 重 要 的 单 词 直 到 达 到 攻 击 目 标．该 研 究 表 明，利 用 ＴｅｘｔＢｕｇｇｅｒ攻 击 方 法 可 以 轻 松的 攻 破 Ｇｏｏｇｌｅ　Ｃｌｏｕｄ，Ｍｉｃｒｏｓｏｆｔ　Ａｚｕｒｅ，Ａｍａｚｏｎ ＡＷＳ，ＩＢＭ　Ｗａｔｓｏｎ，Ｆａｃｅｂｏｏｋ　ｆａｓｔＴｅｘｔ等 平 台 提 供 的商业自然语言处 理 机 器 学 习 服 务，并 且 攻 击 成 功 率 高 、隐 蔽 性 强 ． ５．３　 自 身 安 全 问 题
由于采用了近似处理或是基于优化手段，大多数 解 释 方 法 只 能 提 供 近 似 的 解 释 ，因 而 解 释 结 果 与 模 型 的真实行为之间存在一定的不一致性．而最 新研究 表 明，攻击者可以利 用 解 释 方 法 与 待 解 释 模 型 之 间 的 这种不一致性设计针对可解释系统的新型对抗样本 攻 击 ，因 而 严 重 的 威 胁 着 可 解 释 系 统 的 自 身 安 全 ．
根据 攻 击 目 的 不 同，现 存 针 对 可 解 释 系 统 的 新 型对抗样本攻击可以 分 为 ２ 类：１）在 不 改 变 模 型 的 决策结果的前提下，使 解 释 方 法 解 释 出 错［１５１］；２）使 模型决策出错而不改变解释方 法 的 解 释 结 果［１５２］．其 中，Ｇｈｏｒｂａｎｉ等 人 首 ［１５１］ 次 将 对 抗 攻 击 的 概 念 引 入 到了神经网络的可解释性中并且提出了模型解释脆 弱性的概念．具体地，他们将针 对解释 方法 的对抗 攻 击定义为优化问题：
ａｒｇ　ｍａｘＤ（Ｉ（ｘｔ；Ｎ ），Ｉ（ｘｔ＋δ；Ｎ ）） δ
ｓ．ｔ．‖δ‖! ≤ε，ｆ（ｘｔ＋δ）＝ｆ（ｘｔ）， 其中，Ｉ（ｘｔ；Ｎ ）为解释系统对神经网络 Ｎ 针 对 样 本

ｘｔ 决策结果ｆ（ｘｔ）的 解 释，δ 为 样 本 中 所 需 添 加 的 扰动，Ｄ（·）用 于 度 量 扰 动 前 后 解 释 结 果 的 变 化．通 过优化上述目标函 数，可 以 在 不 改 变 模 型 决 策 结 果 的前提下，生成能 让 解 释 方 法 产 生 截 然 不 同 的 解 释 结果 的 对 抗 样 本．针 对 Ｇｒａｄ［５２］，Ｉｎｔｅｇｒａｔｅｄ［５５］以 及 ＤｅｅｐＬＩＦＴ［５８］等反向传播解 释 方 法 的 对 抗 攻 击 实 验 证 明 ，上 述 解 释 方 法 均 容 易 受 到 对 抗 样 本 攻 击 ，因 而 只能提供脆弱的模型解 释．与 Ｇｈｏｒｂａｎｉ等 人 研 究 相 反，Ｚｈａｎｇ等人 提 ［１５２］ 出了 Ａｃｉｄ攻击，旨在 生 成 能 让 模型分类出错而不改变解释方法解释结果的对抗样 本．通过 对 表 示 导 向 的 （如 激 活 最 大 化、特 征 反 演 等）、模 型 导 向 的 （如 基 于 掩 码 模 型 的 显 著 性 检 测 等［１５３］）以及 扰 动 导 向 的 （如 敏 感 性 分 析 等 ）三 大 类 解释方法进行 Ａｃｉｄ攻击和经验性评估，作者发现 生 成欺骗分类器及其解释方法的对抗样本实际上并不 比生成仅能欺骗分类器的 对 抗样 本 更 困 难．因此，这 几 类 解 释 方 法 同 样 是 脆 弱 的 ，在 对 抗 的 环 境 下 ，其 提 供的解释结果未必可靠．此 外，这 种 攻击 还会 使基 于 对比攻击前后解释 结 果 的 防 御 方 法 失 效，导 致 对 抗 攻击更难防御．
上 述 研 究 表 明 ：现 存 解 释 方 法 大 多 数 是 脆 弱 的 ， 因此只能提供 有 限 的 安 全 保 证．但 由 于 可 解 释 性 技 术潜在应用广泛，因而其自 身 安全 问题 不容 忽视．以 医 疗 诊 断 中 的 可 解 释 系 统 为 例 ，在 临 床 治 疗 中 ，医 生 会根据可解释系统提供的解释结果对病人进行相应 的诊断和治疗，一 旦 解 释 系 统 被 新 型 对 抗 攻 击 方 法 攻击，那么提供的 解 释 结 果 必 然 会 影 响 医 生 的 诊 断 过程，甚至是误导医 生 的 诊 断 而 给 病 人 带 来 致 命 的 威胁．因此，仅 有 解 释 是 不 够 的，为 保 证 机 器 学 习 及 可解释性技术在实 际 部 署 应 用 中 的 安 全，解 释 方 法 本身必须是安全的，而 设 计 更 精 确 的 解 释 方 法 以 消 除解释方法与决策系统之间的不一致性则是提高解 释方法鲁棒性进而消除其外在安全隐患的重要途径．
６　 当 前 挑 战 与 未 来 方 向
尽管模型可解释性研究已取得一系列瞩目的研 究 成 果 ，但 其 研 究 还 处 于 初 级 阶 段 ，依 然 面 临 着 许 多 的挑战且存在许多的关键 问题 尚 待解 决．其 中，可 解 释性研究当前面临的一个挑战是如何设计更精确、 更友好的解释方法，消 除 解 释 结 果 与 模 型 真 实 行 为 之 间 的 不 一 致 ；第 ２ 个 挑 战 是 如 何 设 计 更 科 学 、更 统 一的可解释性评估 指 标，以 评 估 可 解 释 方 法 解 释 性 能和安全性．

２０９０

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

６．１　 解 释 方 法 设 计 精 确 地 理 解 机 器 学 习 的 工 作 原 理 ，研 究 透 明 的 、
可解释且可证明机 器 学 习 技 术，有 助 于 推 动 机 器 学 习研究的进一步发 展，同 时 有 助 于 促 进 人 工 智 能 相 关技术的落地 应 用．这 要 求 机 器 学 习 可 解 释 性 研 究 必须具备能精确地揭示模型内部工作逻辑同时向人 类提供可以足够准确理解模型决策的信息的能力． 因此，无论是ａｎｔｅ－ｈｏｃ可解 释 性还是 ｐｏｓｔ－ｈｏｃ可 解 释 性 ，我 们 所 设 计 的 解 释 方 法 都 必 须 是 精 确 的 ，我 们 的解释方法提供的解释结果都必须忠实于模型的真 实决策行为．
由于模型的决策准确性与模型自身可解释性之 间存在一个权衡，现有关 于 ａｎｔｅ－ｈｏｃ可 解 释 性 的 研 究 多 局 限 于 诸 如 线 性 回 归 、决 策 树 等 算 法 透 明 、结 构 简单的模型，对 于 复 杂 的 ＤＮＮ 模 型 则 只 能 依 赖 于 注意力机制提供一个粗粒 度的解释．因此，如何设 计 可解释的机器学习模型以消除模型准确性与可解释 性之间的制 约 是 ａｎｔｅ－ｈｏｃ可 解 释 性 研 究 所 面 临 的 一大挑战，也是未来 可 解 释 性 研 究 发 展 的 一 个 重 要 趋势．其中，一种直观的方法是 将机器 学习 与因果 模 型相结合，让机器学 习 系 统 具 备 从 观 察 数 据 中 发 现 事物间的因果结构和定量推断的能 力．同时，我们 还 可以将机器学习与常识推理和类比计算等技术相结 合，形成可解 释 的、能 自 动 推 理 的 学 习 系 统．未 来 我 们还可以考虑利用仿生学知识并结合更先进的认知 理论对人类认知建 模，以 设 计 具 备 人 类 自 我 解 释 能 力的机器学习模型，实 现 具 有 一 定 思 维 能 力 并 且 能 自我推理自我解释的强人工智能系统．
对于 ｐｏｓｔ－ｈｏｃ可解释 性 而 言，大 多 数 的 研 究 都 在尝试采用近似的 方 法 来 模 拟 模 型 的 决 策 行 为，以 从全局的角度解释模型的整体决策逻辑或者从局部 的角度解释模型的单个决策 结果．然 而，由于 近似 过 程往往不够精确，解 释 方 法 给 出 的 解 释 结 果 无 法 正 确地反映待解释模型的实际运行状态和真实决策行 为，而解释方法与决 策 模 型 之 间 的 这 种 不 一 致 性 甚 至严重地威胁着可解释系统自身的安全．因此，当 前 ｐｏｓｔ－ｈｏｃ可解释性相关研究面临的 巨大挑战 是如 何 设计忠实 于 决 策 模 型 的 安 全 可 保 障 的 精 确 解 释 方 法，以消除解释结果 与 模 型 真 实 行 为 之 间 的 不 一 致 性，从而保证解释结果的可靠 性和 安全 性．未来一 个 有前景的潜在研究方向是设计数学上与待解释模型 等价的解释方法或解释模型．对于全 连 接神经 网 络， Ｃｈｕ等人 已 ［６３］ 经 给 出 了 相 应 的 研 究 方 法 并 取 得 了 一定的研究成果，我 们 则 可 以 基 于 具 体 模 型 的 内 部

机理和神经网络的前向 传 播 机 制，将 Ｃｈｕ 等 人 提 出 的研究方法扩 展 到 ＣＮＮ，ＲＮＮ 等 更 复 杂 神 经 网 络 模 型 ，从 而 实 现 对 复 杂 模 型 的 精 确 解 释 ． ６．２　 解 释 方 法 评 估
目 前，可 解 释 性 研 究 领 域 缺 乏 一 个 用 于 评 估 解 释 方 法 的 科 学 评 估 体 系 ，尤 其 是 在 计 算 机 视 觉 领 域 ， 许多解释方法的评 估 还 依 赖 于 人 类 的 认 知，因 而 只 能 定 性 评 估 ，无 法 对 解 释 方 法 的 性 能 进 行 量 化 ，也 无 法对同类型的研究工作进 行精 确地 比 较．并且，由 于 人类认知的局限性，人 们 只 能 理 解 解 释 结 果 中 揭 示 的 显 性 知 识 ，而 通 常 无 法 理 解 其 隐 性 知 识 ，因 而 无 法 保证基于认知的评估方法的可靠性．
对于ａｎｔｅ－ｈｏｃ可解释 性 而 言，其 评 估 挑 战 在 于 如何量化模型的内在解释 能 力．对 于 同 一 应用 场 景， 我们可能会采用不 同 的 模 型，同 一 模 型 也 可 能 会 应 用到不同的场景中，而 对 于 如 何 衡 量 和 比 较 这 些 模 型的可解释性 目 前 仍 没 有 达 成 共 识．由 于 模 型 自 身 可解释性受实际应 用 场 景、模 型 算 法 本 身 以 及 人 类 理 解 能 力 的 制 约 ，未 来 我 们 可 以 从 应 用 场 景 、算 法 功 能、人类认知这３ 个角度来设计评估指标．这些指标 虽 各 有 利 弊 但 相 互 补 充 ，可 以 实 现 多 层 次 、细 粒 度 的 可 解 释 性 评 估 ，以 弥 补 单 一 评 估 指 标 的 不 足 ．
对于 ｐｏｓｔ－ｈｏｃ可解释 性 而 言，其 评 估 挑 战 在 于 如何量化解释结果的保真 度 和一 致 性．如 前所 述，由 于人类认知的局限 性，解 释 方 法 针 对 机 器 学 习 模 型 给出的解释结果并不 总 是 “合 理”的，而 我 们 很 难 判 断这种与人类认知相违背的解释结果到底是由于模 型自身的错误行为 还 是 解 释 方 法 的 局 限 性，抑 或 是 人类认知的局限性造成的．因此，我们 需 要设 计可 靠 的 评 估 指 标 对 解 释 方 法 进 行 定 量 的 评 估．Ｇｕｏ 等 人 提 ［５１］ 出利用 解 释 方 法 给 出 的 预 测 结 果 与 待 解 释 模型预测结果之间的均方根误 差（ＲＭＳＥ）来评 估 解 释方法的保真度，然 而 这 种 评 估 指 标 无 法 用 于 评 估 激 活 最 大 化 、敏 感 性 分 析 、反 向 传 播 以 及 特 征 反 演 等 不提供预测结果的 解 释 方 法．Ｃｈｕ 等 人 提 ［６３］ 出 利 用 输入样本及其邻近样本的解释结果的余弦相似性来 评估解释方法，然 而 这 种 方 法 无 法 用 于 评 估 解 释 结 果的保真度．此外，目前还缺 乏 用 于 评估 针 对 同 一 模 型的不同解释方法的评估 指标．因此，未 来 我们 需 要 从解释结果的保真 度、一 致 性 以 及 不 同 解 释 方 法 的 差异性等角度设计 评 价 指 标，对 解 释 方 法 进 行 综 合 评估．

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０９１

７　 结 束 语
机器学习可解释性是一个非常有前景的研究领 域 ，该 领 域 已 经 成 为 了 国 内 外 学 者 的 研 究 热 点 ，并 且 取得了许多瞩目的研究成 果．但到目 前 为止，机器 学 习可解释性研究还 处 于 初 级 阶 段，依 然 存 在 许 多 关 键问题尚待解 决．为 了 总 结 现 有 研 究 成 果 的 优 势 与 不足，探讨 未 来 研 究 方 向，本 文 从 可 解 释 性 相 关 技 术 、潜 在 应 用 、安 全 性 分 析 等 方 面 对 现 有 研 究 成 果 进 行 了 归 类 、总 结 和 分 析 ，同 时 讨 论 了 当 前 研 究 面 临 的 挑战和未来潜在的 研 究 方 向，旨 在 为 推 动 模 型 可 解 释性研究的进一步发展和应用提供一定帮助．
参考文献
［１］ Ｓｃｈｒｏｆｆ　Ｆ，Ｋａｌｅｎｉｃｈｅｎｋｏ　Ｄ，Ｐｈｉｌｂｉｎ　Ｊ．Ｆａｃｅｎｅｔ：Ａ　ｕｎｉｆｉｅｄ ｅｍｂｅｄｄｉｎｇ　ｆｏｒ　ｆａｃｅ　ｒｅｃｏｇｎｉｔｉｏｎ　ａｎｄ　ｃｌｕｓｔｅｒｉｎｇ ［Ｃ］??Ｐｒｏｃ　ｏｆ ｔｈｅ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ． Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１５：８１５－８２３
［２］ Ｓｕｎ　Ｙｉ，Ｌｉａｎｇ　Ｄｉｎｇ，Ｗａｎｇ　Ｘｉａｏｇａｎｇ，ｅｔ　ａｌ．Ｄｅｅｐｉｄ３：Ｆａｃｅ ｒｅｃｏｇｎｉｔｉｏｎ　ｗｉｔｈ　ｖｅｒｙ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１５０２．００８７３，２０１５
［３］ Ｔａｉｇｍａｎ　Ｙ，Ｙａｎｇ　Ｍｉｎｇ，Ｒａｎｚａｔｏ　Ｍ　Ａ，ｅｔ　ａｌ．Ｄｅｅｐｆａｃｅ： Ｃｌｏｓｉｎｇ　ｔｈｅ　ｇａｐ　ｔｏ　ｈｕｍａｎ－ｌｅｖｅｌ　ｐｅｒｆｏｒｍａｎｃｅ　ｉｎ　ｆａｃｅ ｖｅｒｉｆｉｃａｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１４： １７０１－１７０８
［４］ Ｇｅｉｇｅｒ　Ａ，Ｌｅｎｚ　Ｐ，Ｕｒｔａｓｕｎ　Ｒ．Ａｒｅ　ｗｅ　ｒｅａｄｙ　ｆｏｒ　ａｕｔｏｎｏｍｏｕｓ ｄｒｉｖｉｎｇ？ｔｈｅ　ｋｉｔｔｉ　ｖｉｓｉｏｎ　ｂｅｎｃｈｍａｒｋ　ｓｕｉｔｅ［Ｃ］??Ｐｒｏｃ　ｏｆ　ＩＥＥＥ Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１２：３３５４－３３６１
［５］ Ｔｏｂｉｙａｍａ　Ｓ，Ｙａｍａｇｕｃｈｉ　Ｙ，Ｓｈｉｍａｄａ　Ｈ，ｅｔ　ａｌ．Ｍａｌｗａｒｅ ｄｅｔｅｃｔｉｏｎ　ｗｉｔｈ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｕｓｉｎｇ　ｐｒｏｃｅｓｓ　ｂｅｈａｖｉｏｒ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　４０ｔｈ　Ａｎｎｕａｌ　Ｃｏｍｐｕｔｅｒ　Ｓｏｆｔｗａｒｅ　ａｎｄ Ａｐｐｌｉｃａｔｉｏｎｓ　Ｃｏｎｆ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１６：５７７－５８２
［６］ Ｒａｊｐｕｒｋａｒ　Ｐ，Ｉｒｖｉｎ　Ｊ，Ｚｈｕ　Ｋ，ｅｔ　ａｌ．Ｃｈｅｘｎｅｔ：Ｒａｄｉｏｌｏｇｉｓｔ－ ｌｅｖｅｌ　ｐｎｅｕｍｏｎｉａ　ｄｅｔｅｃｔｉｏｎ　ｏｎ　ｃｈｅｓｔ　ｘ－ｒａｙｓ　ｗｉｔｈ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７１１．０５２２５，２０１７
［７］ Ｉｂｒａｈｉｍ　Ｍ， Ｌｏｕｉｅ　Ｍ， Ｍｏｄａｒｒｅｓ　Ｃ， ｅｔ　ａｌ． Ｇｌｏｂａｌ Ｅｘｐｌａｎａｔｉｏｎｓ　ｏｆ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ：Ｍａｐｐｉｎｇ　ｔｈｅ　Ｌａｎｄｓｃａｐｅ　ｏｆ Ｐｒｅｄｉｃｔｉｏｎｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１９０２．０２３８４，２０１９
［８］ Ｓｚｅｇｅｄｙ　Ｃ，Ｚａｒｅｍｂａ　Ｗ，Ｓｕｔｓｋｅｖｅｒ　Ｉ，ｅｔ　ａｌ．Ｉｎｔｒｉｇｕｉｎｇ ｐｒｏｐｅｒｔｉｅｓ　ｏｆ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１３０２． ６１９９，２０１３
［９］ Ｌｉ　Ｊｉｎｆｅｎｇ，Ｊｉ　Ｓｈｏｕｌｉｎｇ，Ｄｕ　Ｔｉａｎｙｕ，ｅｔ　ａｌ．ＴｅｘｔＢｕｇｇｅｒ： Ｇｅｎｅｒａｔｉｎｇ　ａｄｖｅｒｓａｒｉａｌ　ｔｅｘｔ　ａｇａｉｎｓｔ　ｒｅａｌ－ｗｏｒｌｄ　ａｐｐｌｉｃａｔｉｏｎｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２６ｔｈ　Ａｎｎｕａｌ　Ｎｅｔｗｏｒｋ　ａｎｄ　Ｄｉｓｔｒｉｂｕｔｅｄ Ｓｙｓｔｅｍｓ　Ｓｅｃｕｒｉｔｙ　Ｓｙｍｐ．Ｒｅｓｔｏｎ，ＶＡ：ＩＳＯＣ，２０１９

［１０］ Ｄｕ　Ｔｉａｎｙｕ，Ｊｉ　Ｓｈｏｕｌｉｎｇ，Ｌｉ　Ｊｉｎｆｅｎｇ，ｅｔ　ａｌ．ＳｉｒｅｎＡｔｔａｃｋ： Ｇｅｎｅｒａｔｉｎｇ　ａｄｖｅｒｓａｒｉａｌ　ａｕｄｉｏ　ｆｏｒ　ｅｎｄ－ｔｏ－ｅｎｄ　ａｃｏｕｓｔｉｃ　ｓｙｓｔｅｍｓ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１９０１．０７８４６，２０１９
［１１］ Ｄｏｓｈｉ－Ｖｅｌｅｚ　Ｆ， Ｋｉｍ　Ｂ．Ｔｏｗａｒｄｓ　ａ　ｒｉｇｏｒｏｕｓ　ｓｃｉｅｎｃｅ　ｏｆ ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ： １７０２．０８６０８，２０１７
［１２］ Ｇｕｉｄｏｔｔｉ　Ｒ， Ｍｏｎｒｅａｌｅ　Ａ，Ｒｕｇｇｉｅｒｉ　Ｓ，ｅｔ　ａｌ．Ａ　ｓｕｒｖｅｙ　ｏｆ ｍｅｔｈｏｄｓ　ｆｏｒ　ｅｘｐｌａｉｎｉｎｇ　ｂｌａｃｋ　ｂｏｘ　ｍｏｄｅｌｓ ［Ｊ］． ＡＣＭ Ｃｏｍｐｕｔｉｎｇ　Ｓｕｒｖｅｙｓ，２０１８，５１（５）：Ｎｏ．９３
［１３］ Ｒｉｂｅｉｒｏ　Ｍ　Ｔ，Ｓｉｎｇｈ　Ｓ，Ｇｕｅｓｔｒｉｎ　Ｃ．Ｗｈｙ　ｓｈｏｕｌｄ　Ｉ　ｔｒｕｓｔ　ｙｏｕ？ Ｅｘｐｌａｉｎｉｎｇ　ｔｈｅ　ｐｒｅｄｉｃｔｉｏｎｓ　ｏｆ　ａｎｙ　ｃｌａｓｓｉｆｉｅｒ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ ２２ｎｄ　ＡＣＭ　ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　ａｎｄ Ｄａｔａ　Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１６：１１３５－１１４４
［１４］ Ｂａｅｈｒｅｎｓ　Ｄ，Ｓｃｈｒｏｅｔｅｒ　Ｔ， Ｈａｒｍｅｌｉｎｇ　Ｓ，ｅｔ　ａｌ．Ｈｏｗ　ｔｏ ｅｘｐｌａｉｎ　ｉｎｄｉｖｉｄｕａｌ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ　ｄｅｃｉｓｉｏｎｓ ［Ｊ］．Ｊｏｕｒｎａｌ　ｏｆ Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｒｅｓｅａｒｃｈ，２０１０，１１（６）：１８０３－１８３１
［１５］ Ｍｅｌｉｓ　Ｄ　Ａ，Ｊａａｋｋｏｌａ　Ｔ．Ｔｏｗａｒｄｓ　ｒｏｂｕｓｔ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ｗｉｔｈ ｓｅｌｆ－ｅｘｐｌａｉｎｉｎｇ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３２ｎｄ　Ｉｎｔ Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ， ＮＹ：Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，２０１８：７７７５－７７８４
［１６］ Ｐｏｕｌｉｎ　Ｂ，Ｅｉｓｎｅｒ　Ｒ，Ｓｚａｆｒｏｎ　Ｄ，ｅｔ　ａｌ．Ｖｉｓｕａｌ　ｅｘｐｌａｎａｔｉｏｎ　ｏｆ ｅｖｉｄｅｎｃｅ　ｗｉｔｈ　ａｄｄｉｔｉｖｅ　ｃｌａｓｓｉｆｉｅｒｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１８ｔｈ　Ｃｏｎｆ ｏｎ　Ｉｎｎｏｖａｔｉｖｅ　Ａｐｐｌｉｃａｔｉｏｎｓ　ｏｆ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ Ａｌｔｏ，ＣＡ：ＡＡＡＩ，２００６：１８２２－１８２９
［１７］ Ｋｏｎｏｎｅｎｋｏ　Ｉ． Ａｎ　 ｅｆｆｉｃｉｅｎｔ　 ｅｘｐｌａｎａｔｉｏｎ　 ｏｆ　 ｉｎｄｉｖｉｄｕａｌ ｃｌａｓｓｉｆｉｃａｔｉｏｎｓ　ｕｓｉｎｇ　ｇａｍｅ　ｔｈｅｏｒｙ ［Ｊ］．Ｊｏｕｒｎａｌ　ｏｆ　Ｍａｃｈｉｎｅ Ｌｅａｒｎｉｎｇ　Ｒｅｓｅａｒｃｈ，２０１０，１１（１）：１－１８
［１８］ Ｈａｕｆｅ　Ｓ，Ｍｅｉｎｅｃｋｅ　Ｆ，Ｇｒｇｅｎ　Ｋ，ｅｔ　ａｌ．Ｏｎ　ｔｈｅ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ ｏｆ　ｗｅｉｇｈｔ　ｖｅｃｔｏｒｓ　ｏｆ　ｌｉｎｅａｒ　ｍｏｄｅｌｓ　ｉｎ　ｍｕｌｔｉｖａｒｉａｔｅ　ｎｅｕｒｏｉｍａｇｉｎｇ ［Ｊ］．ＮｅｕｒｏＩｍａｇｅ，２０１４，８７：９６－１１０
［１９］ Ｈｕｙｓｍａｎｓ　Ｊ，Ｄｅｊａｅｇｅｒ　Ｋ， Ｍｕｅｓ　Ｃ，ｅｔ　ａｌ．Ａｎ　ｅｍｐｉｒｉｃａｌ ｅｖａｌｕａｔｉｏｎ　ｏｆ　ｔｈｅ　ｃｏｍｐｒｅｈｅｎｓｉｂｉｌｉｔｙ　ｏｆ　ｄｅｃｉｓｉｏｎ　ｔａｂｌｅ，ｔｒｅｅ　ａｎｄ ｒｕｌｅ　ｂａｓｅｄ　ｐｒｅｄｉｃｔｉｖｅ　ｍｏｄｅｌｓ［Ｊ］．Ｄｅｃｉｓｉｏｎ　Ｓｕｐｐｏｒｔ　Ｓｙｓｔｅｍｓ， ２０１１，５１（１）：１４１－１５４
［２０］ Ｂｒｅｓｌｏｗ　Ｌ　Ａ，Ａｈａ　Ｄ　Ｗ．Ｓｉｍｐｌｉｆｙｉｎｇ　ｄｅｃｉｓｉｏｎ　ｔｒｅｅｓ：Ａ　ｓｕｒｖｅｙ ［Ｊ］．Ｔｈｅ　Ｋｎｏｗｌｅｄｇｅ　Ｅｎｇｉｎｅｅｒｉｎｇ　Ｒｅｖｉｅｗ，１９９７，１２（１）： １－４０
［２１］ Ｆｒａｎｋ　Ｅ，Ｗｉｔｔｅｎ　Ｉ　Ｈ．Ｇｅｎｅｒａｔｉｎｇ　ａｃｃｕｒａｔｅ　ｒｕｌｅ　ｓｅｔｓ　ｗｉｔｈｏｕｔ ｇｌｏｂａｌ　ｏｐｔｉｍｉｚａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１５ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ．Ｓａｎ　Ｆｒａｎｃｉｓｃｏ：Ｍｏｒｇａｎ　Ｋａｕｆｍａｎｎ，１９９８： １４４－１５１
［２２］ Ｑｕｉｎｌａｎ　Ｊ　Ｒ．Ｇｅｎｅｒａｔｉｎｇ　ｐｒｏｄｕｃｔｉｏｎ　ｒｕｌｅｓ　ｆｒｏｍ　ｄｅｃｉｓｉｏｎ　ｔｒｅｅｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１０ｔｈ　Ｉｎｔ　Ｊｏｉｎｔ　Ｃｏｎｆ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ． Ｓａｎ　Ｆｒａｎｃｉｓｃｏ：Ｍｏｒｇａｎ　Ｋａｕｆｍａｎｎ，１９８７：３０４－３０７
［２３］ Ｄｅｎｇ　Ｈｏｕｔａｏ．Ｉｎｔｅｒｐｒｅｔｉｎｇ　ｔｒｅｅ　ｅｎｓｅｍｂｌｅｓ　ｗｉｔｈ　ｉｎｔｒｅｅｓ ［Ｊ］． Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｊｏｕｒｎａｌ　ｏｆ　Ｄａｔａ　Ｓｃｉｅｎｃｅ　ａｎｄ　Ａｎａｌｙｔｉｃｓ，２０１９，７ （４）：２７７－２８７
［２４］ Ｌｏｕ　Ｙｉｎ， Ｃａｒｕａｎａ　Ｒ， Ｇｅｈｒｋｅ　Ｊ．Ｉｎｔｅｌｌｉｇｉｂｌｅ　ｍｏｄｅｌｓ　ｆｏｒ ｃｌａｓｓｉｆｉｃａｔｉｏｎ　ａｎｄ　ｒｅｇｒｅｓｓｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１８ｔｈ　ＡＣＭ ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　ａｎｄ　Ｄａｔａ　ｍｉｎｉｎｇ． Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１２：１５０－１５８

２０９２

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

［２５］ Ｈｅｎｓｏｎ　Ｒ　Ｋ．Ｔｈｅ　ｌｏｇｉｃ　ａｎｄ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｏｆ　ｓｔｒｕｃｔｕｒｅ ｃｏｅｆｆｉｃｉｅｎｔｓ　ｉｎ　ｍｕｌｔｉｖａｒｉａｔｅ　ｇｅｎｅｒａｌ　ｌｉｎｅａｒ　ｍｏｄｅｌ　ａｎａｌｙｓｅｓ，Ｎｏ． ＥＤ４６７３８１ ［Ｒ］． Ｎｅｗ　Ｏｒｌｅａｎｓ， ＬＡ： ＥＲＩＣ　Ｄｏｃｕｍｅｎｔ Ｒｅｐｒｏｄｕｃｔｉｏｎ　Ｓｅｒｖｉｃｅ，２００２
［２６］ Ｈａｓｔｉｅ　Ｔ　Ｊ．Ｓｔａｔｉｓｔｉｃａｌ　Ｍｏｄｅｌｓ　ｉｎ　Ｓ ［Ｍ］．Ｎｅｗ　Ｙｏｒｋ： Ｒｏｕｔｌｅｄｇｅ，２０１７：２４９－３０７
［２７］ Ｗｏｏｄ　Ｓ　Ｎ．Ｇｅｎｅｒａｌｉｚｅｄ　ａｄｄｉｔｉｖｅ　ｍｏｄｅｌｓ：Ａｎ　ｉｎｔｒｏｄｕｃｔｉｏｎ ｗｉｔｈ　Ｒ ［Ｍ］．Ｎｅｗ　Ｙｏｒｋ：Ｃｈａｐｍａｎ　ａｎｄ　Ｈａｌｌ?ＣＲＣ，２０１７
［２８］ Ｒａｖｉｋｕｍａｒ　Ｐ，Ｌａｆｆｅｒｔｙ　Ｊ，Ｌｉｕ　Ｈａｎ，ｅｔ　ａｌ．Ｓｐａｒｓｅ　ａｄｄｉｔｉｖｅ ｍｏｄｅｌｓ［Ｊ］．Ｊｏｕｒｎａｌ　ｏｆ　ｔｈｅ　Ｒｏｙａｌ　Ｓｔａｔｉｓｔｉｃａｌ　Ｓｏｃｉｅｔｙ：Ｓｅｒｉｅｓ　Ｂ （Ｓｔａｔｉｓｔｉｃａｌ　Ｍｅｔｈｏｄｏｌｏｇｙ），２００９，７１（５）：１００９－１０３０
［２９］ Ｂａｈｄａｎａｕ　Ｄ，Ｃｈｏ　Ｋ，Ｂｅｎｇｉｏ　Ｙ．Ｎｅｕｒａｌ　ｍａｃｈｉｎｅ　ｔｒａｎｓｌａｔｉｏｎ ｂｙ　ｊｏｉｎｔｌｙ　ｌｅａｒｎｉｎｇ　ｔｏ　ａｌｉｇｎ　ａｎｄ　ｔｒａｎｓｌａｔｅ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ ａｒＸｉｖ：１４０９．０４７３，２０１４
［３０］ Ｖａｓｗａｎｉ　Ａ，Ｓｈａｚｅｅｒ　Ｎ，Ｐａｒｍａｒ　Ｎ，ｅｔ　ａｌ．Ａｔｔｅｎｔｉｏｎ　ｉｓ　ａｌｌ　ｙｏｕ ｎｅｅｄ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３１ｓｔ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ，ＮＹ：Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ， ２０１７：６０００－６０１０
［３１］ Ｃｈｏｉ　Ｅ，Ｂａｈａｄｏｒｉ　Ｍ　Ｔ，Ｓｕｎ　Ｊｉｍｅｎｇ，ｅｔ　ａｌ．Ｒｅｔａｉｎ：Ａｎ ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｐｒｅｄｉｃｔｉｖｅ　ｍｏｄｅｌ　ｆｏｒ　ｈｅａｌｔｈｃａｒｅ　ｕｓｉｎｇ　ｒｅｖｅｒｓｅ ｔｉｍｅ　ａｔｔｅｎｔｉｏｎ　ｍｅｃｈａｎｉｓｍ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３０ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ，ＮＹ： Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，２０１６：３５０４－３５１２
［３２］ Ｘｕ　Ｋ，Ｂａ　Ｊ，Ｋｉｒｏｓ　Ｒ，ｅｔ　ａｌ．Ｓｈｏｗ，ａｔｔｅｎｄ　ａｎｄ　ｔｅｌｌ：Ｎｅｕｒａｌ ｉｍａｇｅ　ｃａｐｔｉｏｎ　ｇｅｎｅｒａｔｉｏｎ　ｗｉｔｈ　ｖｉｓｕａｌ　ａｔｔｅｎｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ ｔｈｅ　３２ｎｄ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ．Ｔａｈｏｅ， ＣＡ： Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｓｏｃｉｅｔｙ，２０１５：２０４８－２０５７
［３３］ Ｃｈａｕｄｈａｒｉ　Ｓ，Ｐｏｌａｔｋａｎ　Ｇ，Ｒａｍａｎａｔｈ　Ｒ，ｅｔ　ａｌ．Ａｎ　Ａｔｔｅｎｔｉｖｅ ｓｕｒｖｅｙ　ｏｆ　ａｔｔｅｎｔｉｏｎ　ｍｏｄｅｌｓ［Ｊ］．ａｒＸｉｖ：１９０４．０２８７４，２０１９
［３４］ Ｙａｎｇ　Ｚｉｃｈａｏ， Ｙａｎｇ　Ｄｉｙｉ， Ｄｙｅｒ　Ｃ，ｅｔ　ａｌ．Ｈｉｅｒａｒｃｈｉｃａｌ ａｔｔｅｎｔｉｏｎ　ｎｅｔｗｏｒｋｓ　ｆｏｒ　ｄｏｃｕｍｅｎｔ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ ｔｈｅ　２０１６ Ｃｏｎｆ　ｏｆ　ｔｈｅ　Ｎｏｒｔｈ　Ａｍｅｒｉｃａｎ　Ｃｈａｐｔｅｒ　ｏｆ　ｔｈｅ Ａｓｓｏｃｉａｔｉｏｎ　ｆｏｒ　Ｃｏｍｐｕｔａｔｉｏｎａｌ　Ｌｉｎｇｕｉｓｔｉｃｓ：Ｈｕｍａｎ　Ｌａｎｇｕａｇｅ Ｔｅｃｈｎｏｌｏｇｉｅｓ．Ｓｔｒｏｕｄｓｂｕｒｇ，ＰＡ：Ａｓｓｏｃｉａｔｉｏｎ　ｆｏｒ　Ｃｏｍｐｕｔａｔｉｏｎａｌ Ｌｉｎｇｕｉｓｔｉｃｓ，２０１６：１４８０－１４８９
［３５］ Ｈｅ　Ｘｉａｎｇｎａｎ，Ｈｅ　Ｚｈａｎｋｕｉ，Ｓｏｎｇ　Ｊｉｎｇｋｕａｎ，ｅｔ　ａｌ．ＮＡＩＳ： Ｎｅｕｒａｌ　ａｔｔｅｎｔｉｖｅ　ｉｔｅｍ　ｓｉｍｉｌａｒｉｔｙ　ｍｏｄｅｌ　ｆｏｒ　ｒｅｃｏｍｍｅｎｄａｔｉｏｎ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ　ａｎｄ　Ｄａｔａ　Ｅｎｇｉｎｅｅｒｉｎｇ， ２０１８，３０（１２）：２３５４－２３６６
［３６］ Ｙｉｎｇ　Ｈａｏｃｈａｏ，Ｚｈｕａｎｇ　Ｆｕｚｈｅｎｇ，Ｚｈａｎｇ　Ｆｕｚｈｅｎｇ，ｅｔ　ａｌ． Ｓｅｑｕｅｎｔｉａｌ　ｒｅｃｏｍｍｅｎｄｅｒ　 ｓｙｓｔｅｍ　 ｂａｓｅｄ　 ｏｎ　 ｈｉｅｒａｒｃｈｉｃａｌ ａｔｔｅｎｔｉｏｎ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２７ｔｈ　Ｉｎｔ　Ｊｏｉｎｔ　Ｃｏｎｆ　ｏｎ Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ　Ａｌｔｏ，ＣＡ：ＡＡＡＩ，２０１８：３９２６－ ３９３２
［３７］ Ｚｈｏｕ　Ｃｈａｎｇ，Ｂａｉ　Ｊｉｎｚｅ，Ｓｏｎｇ　Ｊｕｎｓｈｕａｉ，ｅｔ　ａｌ．ＡＴＲａｎｋ：Ａｎ ａｔｔｅｎｔｉｏｎ－ｂａｓｅｄ　ｕｓｅｒ　ｂｅｈａｖｉｏｒ　ｍｏｄｅｌｉｎｇ　ｆｒａｍｅｗｏｒｋ　ｆｏｒ ｒｅｃｏｍｍｅｎｄａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３２ｎｄ　ＡＡＡＩ　Ｃｏｎｆ　ｏｎ Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ　Ａｌｔｏ，ＣＡ：ＡＡＡＩ，２０１８：４５６４－ ４５７１
［３８］ Ｙｕ　Ｓｈｕａｉ， Ｗａｎｇ　Ｙｏｎｇｂｏ，Ｙａｎｇ　Ｍｉｎ，ｅｔ　ａｌ．ＮＡＩＲＳ：Ａ Ｎｅｕｒａｌ　Ａｔｔｅｎｔｉｖｅ　Ｉｎｔｅｒｐｒｅｔａｂｌｅ　Ｒｅｃｏｍｍｅｎｄａｔｉｏｎ　Ｓｙｓｔｅｍ ［Ｃ］ ??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１２ｔｈ　ＡＣＭ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｗｅｂ　Ｓｅａｒｃｈ　ａｎｄ　Ｄａｔａ Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１９：７９０－７９３

［３９］ Ｓｅｏ　Ｓ， Ｈｕａｎｇ　Ｊｉｎｇ， Ｙａｎｇ　Ｈａｏ， ｅｔ　ａｌ．Ｉｎｔｅｒｐｒｅｔａｂｌｅ ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｗｉｔｈ　ｄｕａｌ　ｌｏｃａｌ　ａｎｄ　ｇｌｏｂａｌ ａｔｔｅｎｔｉｏｎ　ｆｏｒ　ｒｅｖｉｅｗ　ｒａｔｉｎｇ　ｐｒｅｄｉｃｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１１ｔｈ ＡＣＭ　Ｃｏｎｆ　ｏｎ　Ｒｅｃｏｍｍｅｎｄｅｒ　Ｓｙｓｔｅｍｓ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ， ２０１７：２９７－３０５
［４０］ Ｍａｓｈａｙｅｋｈｉ　Ｍ，Ｇｒａｓ　Ｒ．Ｒｕｌｅ　ｅｘｔｒａｃｔｉｏｎ　ｆｒｏｍ　ｄｅｃｉｓｉｏｎ　ｔｒｅｅｓ ｅｎｓｅｍｂｌｅｓ：Ｎｅｗ　ａｌｇｏｒｉｔｈｍｓ　ｂａｓｅｄ　ｏｎ　ｈｅｕｒｉｓｔｉｃ　ｓｅａｒｃｈ　ａｎｄ ｓｐａｒｓｅ　ｇｒｏｕｐ　ｌａｓｓｏ　ｍｅｔｈｏｄｓ ［Ｊ］．Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｊｏｕｒｎａｌ　ｏｆ Ｉｎｆｏｒｍａｔｉｏｎ　Ｔｅｃｈｎｏｌｏｇｙ　＆ Ｄｅｃｉｓｉｏｎ　Ｍａｋｉｎｇ，２０１７，１６（６）： １７０７－１７２７
［４１］ Ｙａｎｇ　Ｃｈｅｎｇｌｉａｎｇ，Ｒａｎｇａｒａｊａｎ　Ａ，Ｒａｎｋａ　Ｓ．Ｇｌｏｂａｌ　ｍｏｄｅｌ ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｖｉａ　ｒｅｃｕｒｓｉｖｅ　ｐａｒｔｉｔｉｏｎｉｎｇ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　４ｔｈ ＩＥＥＥ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｄａｔａ　Ｓｃｉｅｎｃｅ　ａｎｄ　Ｓｙｓｔｅｍｓ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１８：１５６３－１５７０
［４２］ Ｐｕｒｉ　Ｎ，Ｇｕｐｔａ　Ｐ，Ａｇａｒｗａｌ　Ｐ，ｅｔ　ａｌ．ＭＡＧＩＸ：Ｍｏｄｅｌ　ａｇｎｏｓｔｉｃ ｇｌｏｂａｌｌｙ　ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｅｘｐｌａｎａｔｉｏｎｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ： １７０６．０７１６０，２０１７
［４３］ Ｗａｎｇ　Ｊｕｎｐｅｎｇ，Ｇｏｕ　Ｌｉａｎｇ，Ｚｈａｎｇ　Ｗｅｉ，ｅｔ　ａｌ．ＤｅｅｐＶＩＤ：Ｄｅｅｐ ｖｉｓｕａｌ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ａｎｄ　ｄｉａｇｎｏｓｉｓ　ｆｏｒ　ｉｍａｇｅ　ｃｌａｓｓｉｆｉｅｒｓ　ｖｉａ ｋｎｏｗｌｅｄｇｅ　ｄｉｓｔｉｌｌａｔｉｏｎ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｖｉｓｕａｌｉｚａｔｉｏｎ ａｎｄ　Ｃｏｍｐｕｔｅｒ　Ｇｒａｐｈｉｃｓ，２０１９，２５（６）：２１６８－２１８０
［４４］ Ｅｒｈａｎ　Ｄ，Ｂｅｎｇｉｏ　Ｙ，Ｃｏｕｒｖｉｌｌｅ　Ａ，ｅｔ　ａｌ．Ｖｉｓｕａｌｉｚｉｎｇ　ｈｉｇｈｅｒ－ ｌａｙｅｒ　ｆｅａｔｕｒｅｓ　ｏｆ　ａ　ｄｅｅｐ　ｎｅｔｗｏｒｋ［Ｊ］．Ｕｎｉｖｅｒｓｉｔｙ　ｏｆ　Ｍｏｎｔｒｅａｌ， ２００９，１３４１（３）：１－１３
［４５］ Ｎｇｕｙｅｎ　Ａ，Ｄｏｓｏｖｉｔｓｋｉｙ　Ａ，Ｙｏｓｉｎｓｋｉ　Ｊ，ｅｔ　ａｌ．Ｓｙｎｔｈｅｓｉｚｉｎｇ ｔｈｅ　ｐｒｅｆｅｒｒｅｄ　ｉｎｐｕｔｓ　ｆｏｒ　ｎｅｕｒｏｎｓ　ｉｎ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｖｉａ　ｄｅｅｐ ｇｅｎｅｒａｔｏｒ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３０ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ，ＮＹ：Ｃｕｒｒａｎ Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，２０１６：３３８７－３３９５
［４６］ Ｙｕａｎ　Ｈａｏ，Ｃｈｅｎ　Ｙｏｎｇｊｕｎ，Ｈｕ　Ｘｉａ，ｅｔ　ａｌ．Ｉｎｔｅｒｐｒｅｔｉｎｇ　ｄｅｅｐ ｍｏｄｅｌｓ　ｆｏｒ　ｔｅｘｔ　ａｎａｌｙｓｉｓ　ｖｉａ　ｏｐｔｉｍｉｚａｔｉｏｎ　ａｎｄ　ｒｅｇｕｌａｒｉｚａｔｉｏｎ ｍｅｔｈｏｄｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３３ｒｄ　ＡＡＡＩ　Ｃｏｎｆ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ　Ａｌｔｏ，ＣＡ：ＡＡＡＩ，２０１９：５７１７－５７２４
［４７］ Ｆｏｎｇ　Ｒ　Ｃ，Ｖｅｄａｌｄｉ　Ａ．Ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｅｘｐｌａｎａｔｉｏｎｓ　ｏｆ　ｂｌａｃｋ ｂｏｘｅｓ　ｂｙ　ｍｅａｎｉｎｇｆｕｌ　ｐｅｒｔｕｒｂａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１６ｔｈ ＩＥＥＥ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ， ２０１７：３４２９－３４３７
［４８］ Ｌｉｕ　Ｌｉｎｇｑｉａｏ， Ｗａｎｇ　Ｌｅｉ．Ｗｈａｔ　ｈａｓ　ｍｙ　ｃｌａｓｓｉｆｉｅｒ　ｌｅａｒｎｅｄ？ ｖｉｓｕａｌｉｚｉｎｇ　ｔｈｅ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ　ｒｕｌｅｓ　ｏｆ　ｂａｇ－ｏｆ－ｆｅａｔｕｒｅ　ｍｏｄｅｌ　ｂｙ ｓｕｐｐｏｒｔ　ｒｅｇｉｏｎ　ｄｅｔｅｃｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２４ｔｈ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ： ＩＥＥＥ，２０１２：３５８６－３５９３
［４９］ Ｇｕｉｄｏｔｔｉ　Ｒ，Ｍｏｎｒｅａｌｅ　Ａ，Ｒｕｇｇｉｅｒｉ　Ｓ，ｅｔ　ａｌ．Ｌｏｃａｌ　ｒｕｌｅ－ｂａｓｅｄ ｅｘｐｌａｎａｔｉｏｎｓ　ｏｆ　ｂｌａｃｋ　ｂｏｘ　ｄｅｃｉｓｉｏｎ　ｓｙｓｔｅｍｓ ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１８０５．１０８２０，２０１８
［５０］ Ｒｉｂｅｉｒｏ　Ｍ　Ｔ，Ｓｉｎｇｈ　Ｓ，Ｇｕｅｓｔｒｉｎ　Ｃ．Ａｎｃｈｏｒｓ：Ｈｉｇｈ－ｐｒｅｃｉｓｉｏｎ ｍｏｄｅｌ－ａｇｎｏｓｔｉｃ　ｅｘｐｌａｎａｔｉｏｎｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３２ｎｄ　ＡＡＡＩ Ｃｏｎｆ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ　Ａｌｔｏ，ＣＡ：ＡＡＡＩ，２０１８： １５２７－１５３５
［５１］ Ｇｕｏ　Ｗｅｎｂｏ， Ｍｕ　Ｄｏｎｇｌｉａｎｇ， Ｘｕ　Ｊｕｎ， ｅｔ　ａｌ．Ｌｅｍｎａ： Ｅｘｐｌａｉｎｉｎｇ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｂａｓｅｄ　ｓｅｃｕｒｉｔｙ　ａｐｐｌｉｃａｔｉｏｎｓ ［Ｃ］?? Ｐｒｏｃ　ｏｆ　ｔｈｅ　２０１８ ＡＣＭ　ＳＩＧＳＡＣ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　ａｎｄ Ｃｏｍｍｕｎｉｃａｔｉｏｎｓ　Ｓｅｃｕｒｉｔｙ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１８：３６４－ ３７９

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０９３

［５２］ Ｓｉｍｏｎｙａｎ　Ｋ， Ｖｅｄａｌｄｉ　Ａ， Ｚｉｓｓｅｒｍａｎ　Ａ． Ｄｅｅｐ　ｉｎｓｉｄｅ ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｔｗｏｒｋｓ： Ｖｉｓｕａｌｉｓｉｎｇ　ｉｍａｇｅ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ ｍｏｄｅｌｓ　ａｎｄ　ｓａｌｉｅｎｃｙ　ｍａｐｓ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１３１２． ６０３４，２０１３
［５３］ Ｚｅｉｌｅｒ　Ｍ　Ｄ， Ｆｅｒｇｕｓ　Ｒ． Ｖｉｓｕａｌｉｚｉｎｇ　ａｎｄ　ｕｎｄｅｒｓｔａｎｄｉｎｇ ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１３ｔｈ　Ｅｕｒｏｐｅａｎ　Ｃｏｎｆ ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ．Ｂｅｒｌｉｎ：Ｓｐｒｉｎｇｅｒ，２０１４：８１８－８３３
［５４］ Ｓｐｒｉｎｇｅｎｂｅｒｇ　Ｊ　Ｔ，Ｄｏｓｏｖｉｔｓｋｉｙ　Ａ，Ｂｒｏｘ　Ｔ，ｅｔ　ａｌ．Ｓｔｒｉｖｉｎｇ　ｆｏｒ ｓｉｍｐｌｉｃｉｔｙ：Ｔｈｅ　ａｌｌ　ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｔ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ ａｒＸｉｖ：１４１２．６８０６，２０１４
［５５］ Ｓｕｎｄａｒａｒａｊａｎ　Ｍ， Ｔａｌｙ　 Ａ， Ｙａｎ　 Ｑｉｑｉ． Ｇｒａｄｉｅｎｔｓ　 ｏｆ ｃｏｕｎｔｅｒｆａｃｔｕａｌｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６１１．０２６３９，２０１６
［５６］ Ｓｍｉｌｋｏｖ　Ｄ，Ｔｈｏｒａｔ　Ｎ，Ｋｉｍ　Ｂ，ｅｔ　ａｌ．Ｓｍｏｏｔｈｇｒａｄ：Ｒｅｍｏｖｉｎｇ ｎｏｉｓｅ　ｂｙ　ａｄｄｉｎｇ　ｎｏｉｓｅ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７０６．０３８２５， ２０１７
［５７］ Ｂａｃｈ　Ｓ， Ｂｉｎｄｅｒ　Ａ， Ｍｏｎｔａｖｏｎ　Ｇ，ｅｔ　ａｌ．Ｏｎ　ｐｉｘｅｌ－ｗｉｓｅ
ｅｘｐｌａｎａｔｉｏｎｓ　ｆｏｒ　ｎｏｎ－ｌｉｎｅａｒ　ｃｌａｓｓｉｆｉｅｒ　ｄｅｃｉｓｉｏｎｓ　ｂｙ　ｌａｙｅｒ－ｗｉｓｅ ｒｅｌｅｖａｎｃｅ　ｐｒｏｐａｇａｔｉｏｎ ［Ｊ］．ＰｌｏＳ　Ｏｎｅ， ２０１５， １０ （７）：
ｅ０１３０１４０ ［５８］ Ｓｈｒｉｋｕｍａｒ　Ａ，Ｇｒｅｅｎｓｉｄｅ　Ｐ，Ｓｈｃｈｅｒｂｉｎａ　Ａ，ｅｔ　ａｌ．Ｎｏｔ　ｊｕｓｔ　ａ
ｂｌａｃｋ　ｂｏｘ：Ｌｅａｒｎｉｎｇ　ｉｍｐｏｒｔａｎｔ　ｆｅａｔｕｒｅｓ　ｔｈｒｏｕｇｈ　ｐｒｏｐａｇａｔｉｎｇ ａｃｔｉｖａｔｉｏｎ　ｄｉｆｆｅｒｅｎｃｅｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６０５．０１７１３，
２０１６ ［５９］ Ｄｕ　Ｍｅｎｇｎａｎ，Ｌｉｕ　Ｎｉｎｇｈａｏ，Ｓｏｎｇ　Ｑｉｎｇｑｕａｎ，ｅｔ　ａｌ．Ｔｏｗａｒｄｓ
ｅｘｐｌａｎａｔｉｏｎ　ｏｆ　ＤＮＮ－ｂａｓｅｄ　ｐｒｅｄｉｃｔｉｏｎ　ｗｉｔｈ　ｇｕｉｄｅｄ　ｆｅａｔｕｒｅ ｉｎｖｅｒｓｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２４ｔｈ　ＡＣＭ　ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　＆ Ｄａｔａ　Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ， ２０１８：１３５８－１３６７ ［６０］ Ｚｈｏｕ　Ｂｏｌｅｉ，Ｋｈｏｓｌａ　Ａ，Ｌａｐｅｄｒｉｚａ　Ａ，ｅｔ　ａｌ．Ｌｅａｒｎｉｎｇ　ｄｅｅｐ ｆｅａｔｕｒｅｓ　ｆｏｒ　ｄｉｓｃｒｉｍｉｎａｔｉｖｅ　ｌｏｃａｌｉｚａｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２８ｔｈ
ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ． Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１６：２９２１－２９２９ ［６１］ Ｓｅｌｖａｒａｊｕ　Ｒ　Ｒ，Ｃｏｇｓｗｅｌｌ　Ｍ，Ｄａｓ　Ａ，ｅｔ　ａｌ．Ｇｒａｄ－ＣＡＭ：
Ｖｉｓｕａｌ　ｅｘｐｌａｎａｔｉｏｎｓ　ｆｒｏｍ　ｄｅｅｐ　ｎｅｔｗｏｒｋｓ　ｖｉａ　ｇｒａｄｉｅｎｔ－ｂａｓｅｄ ｌｏｃａｌｉｚａｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｉｎｔｅｌ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ Ｖｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１７：６１８－６２６ ［６２］ Ｇｅｈｒ　Ｔ，Ｍｉｒｍａｎ　Ｍ，Ｄｒａｃｈｓｌｅｒ－Ｃｏｈｅｎ　Ｄ，ｅｔ　ａｌ．Ａｉ２：Ｓａｆｅｔｙ
ａｎｄ　ｒｏｂｕｓｔｎｅｓｓ　ｃｅｒｔｉｆｉｃａｔｉｏｎ　ｏｆ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｗｉｔｈ　ａｂｓｔｒａｃｔ ｉｎｔｅｒｐｒｅｔａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２０１８ＩＥＥＥ　Ｓｙｍｐｏｓｉｕｍ　ｏｎ Ｓｅｃｕｒｉｔｙ　ａｎｄ　Ｐｒｉｖａｃｙ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１８：３－１８ ［６３］ Ｃｈｕ　Ｌｉｎｇｙａｎｇ， Ｈｕ　Ｘｉａ， Ｈｕ　Ｊｕｈｕａ，ｅｔ　ａｌ．Ｅｘａｃｔ　ａｎｄ
ｃｏｎｓｉｓｔｅｎｔ　 ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　 ｆｏｒ　 ｐｉｅｃｅｗｉｓｅ　 ｌｉｎｅａｒ　 ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ：Ａ　ｃｌｏｓｅｄ　ｆｏｒｍ　ｓｏｌｕｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２４ｔｈ
ＡＣＭ　ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　＆ Ｄａｔａ Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１８：１２４４－１２５３ ［６４］ Ａｎｄｒｅｗｓ　Ｒ，Ｄｉｅｄｅｒｉｃｈ　Ｊ，Ｔｉｃｋｌｅ　Ａ　Ｂ．Ｓｕｒｖｅｙ　ａｎｄ　ｃｒｉｔｉｑｕｅ　ｏｆ ｔｅｃｈｎｉｑｕｅｓ　ｆｏｒ　ｅｘｔｒａｃｔｉｎｇ　ｒｕｌｅｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ　ａｒｔｉｆｉｃｉａｌ　ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ［Ｊ］．Ｋｎｏｗｌｅｄｇｅ－Ｂａｓｅｄ　Ｓｙｓｔｅｍｓ，１９９５，８（６）：３７３－
３８９ ［６５］ Ｔｉｃｋｌｅ　Ａ　Ｂ，Ａｎｄｒｅｗｓ　Ｒ，Ｇｏｌｅａ　Ｍ，ｅｔ　ａｌ．Ｔｈｅ　ｔｒｕｔｈ　ｗｉｌｌ　ｃｏｍｅ
ｔｏ　ｌｉｇｈｔ： Ｄｉｒｅｃｔｉｏｎｓ　ａｎｄ　ｃｈａｌｌｅｎｇｅｓ　ｉｎ　ｅｘｔｒａｃｔｉｎｇ　ｔｈｅ ｋｎｏｗｌｅｄｇｅ　ｅｍｂｅｄｄｅｄ　ｗｉｔｈｉｎ　ｔｒａｉｎｅｄ　ａｒｔｉｆｉｃｉａｌ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ，１９９８，９（６）：
１０５７－１０６８

［６６］ Ｔｉｃｋｌｅ　Ａ　Ｂ， Ｏｒｌｏｗｓｋｉ　Ｍ， Ｄｉｅｄｅｒｉｃｈ　Ｊ． ＤＥＤＥＣ： Ａ ｍｅｔｈｏｄｏｌｏｇｙ　ｆｏｒ　ｅｘｔｒａｃｔｉｎｇ　ｒｕｌｅｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ　ａｒｔｉｆｉｃｉａｌ　ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＡＩＳＢ９６ Ｗｏｒｋｓｈｏｐ　ｏｎ　Ｒｕｌｅ Ｅｘｔｒａｃｔｉｏｎ　ｆｒｏｍ　Ｔｒａｉｎｅｄ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ．Ａｍｓｔｅｒｄａｍ，Ｔｈｅ Ｎｅｔｈｅｒｌａｎｄｓ：ＩＯＳ　Ｐｒｅｓｓ，１９９６：９０－１０２
［６７］ Ｆｕ　Ｌｉｍｉｎ．Ｒｕｌｅ　ｇｅｎｅｒａｔｉｏｎ　ｆｒｏｍ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｊ］．ＩＥＥＥ Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｓｙｓｔｅｍｓ，Ｍａｎ，ａｎｄ　Ｃｙｂｅｒｎｅｔｉｃｓ，１９９４，２４ （８）：１１１４－１１２４
［６８］ Ｂａｓｔａｎｉ　Ｏ，Ｋｉｍ　Ｃ，Ｂａｓｔａｎｉ　Ｈ．Ｉｎｔｅｒｐｒｅｔｉｎｇ　ｂｌａｃｋｂｏｘ　ｍｏｄｅｌｓ ｖｉａ　ｍｏｄｅｌ　ｅｘｔｒａｃｔｉｏｎ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７０５．０８５０４， ２０１７
［６９］ Ｃｒａｖｅｎ　Ｍ　Ｗ．Ｅｘｔｒａｃｔｉｎｇ　ｃｏｍｐｒｅｈｅｎｓｉｂｌｅ　ｍｏｄｅｌｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｄ］． Ｍａｄｉｓｏｎ， ＷＩ： Ｄｅｐａｒｔｍｅｎｔ　ｏｆ Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅｓ，Ｕｎｉｖｅｒｓｉｔｙ　ｏｆ　Ｗｉｓｃｏｎｓｉｎ－Ｍａｄｉｓｏｎ，１９９６
［７０］ Ｂｏｚ　Ｏ．Ｅｘｔｒａｃｔｉｎｇ　ｄｅｃｉｓｉｏｎ　ｔｒｅｅｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　８ｔｈ　ＡＣＭ　ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ Ｄｉｓｃｏｖｅｒｙ　ａｎｄ　Ｄａｔａ　Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２００２：４５６－ ４６１
［７１］ Ｍａｓｈａｙｅｋｈｉ　Ｍ，Ｇｒａｓ　Ｒ．Ｒｕｌｅ　ｅｘｔｒａｃｔｉｏｎ　ｆｒｏｍ　ｒａｎｄｏｍ　ｆｏｒｅｓｔ： Ｔｈｅ　ＲＦ＋ ＨＣ　ｍｅｔｈｏｄｓ ［Ｇ］??ＬＮＣＳ　９０９１：Ｐｒｏｃ　ｏｆ　ｔｈｅ　２８ｔｈ Ｃａｎａｄｉａｎ　Ｃｏｎｆ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｂｅｒｌｉｎ：Ｓｐｒｉｎｇｅｒ， ２０１５：２２３－２３７
［７２］ Ｈａｒａ　Ｓ，Ｈａｙａｓｈｉ　Ｋ．Ｍａｋｉｎｇ　ｔｒｅｅ　ｅｎｓｅｍｂｌｅｓ　ｉｎｔｅｒｐｒｅｔａｂｌｅ：Ａ Ｂａｙｅｓｉａｎ　ｍｏｄｅｌ　ｓｅｌｅｃｔｉｏｎ　ａｐｐｒｏａｃｈ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ： １６０６．０９０６６，２０１６
［７３］ Ｈａｒａ　Ｓ，Ｈａｙａｓｈｉ　Ｋ．Ｍａｋｉｎｇ　ｔｒｅｅ　ｅｎｓｅｍｂｌｅｓ　ｉｎｔｅｒｐｒｅｔａｂｌｅ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６０６．０５３９０，２０１６
［７４］ Ｋｒｉｓｈｎａｎ　Ｒ．Ａ　ｓｙｓｔｅｍａｔｉｃ　ｍｅｔｈｏｄ　ｆｏｒ　ｄｅｃｏｍｐｏｓｉｔｉｏｎａｌ　ｒｕｌｅ ｅｘｔｒａｃｔｉｏｎ　ｆｒｏｍ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１０ｔｈ　Ｉｎｔ Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ， ＮＹ：Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，１９９６
［７５］ Ｂｏｎｄａｒｅｎｋｏ　Ａ，Ｚｍａｎｏｖｓｋａ　Ｔ，Ｂｏｒｉｓｏｖ　Ａ．Ｄｅｃｏｍｐｏｓｉｔｉｏｎａｌ ｒｕｌｅｓ　ｅｘｔｒａｃｔｉｏｎ　ｍｅｔｈｏｄｓ　ｆｒｏｍ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ ｔｈｅ　１６ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｓｏｆｔ　Ｃｏｍｐｕｔｉｎｇ．Ｂｅｒｌｉｎ：Ｓｐｒｉｎｇｅｒ， ２０１０：２５６－２６２
［７６］ Ｃｒａｖｅｎ　Ｍ　Ｗ，Ｓｈａｖｌｉｋ　Ｊ　Ｗ．Ｕｓｉｎｇ　ｓａｍｐｌｉｎｇ　ａｎｄ　ｑｕｅｒｉｅｓ　ｔｏ ｅｘｔｒａｃｔ　ｒｕｌｅｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ ８ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ．Ｔａｈｏｅ，ＣＡ：Ｉｎｔｅｒｎａｔｉｏｎａｌ Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｓｏｃｉｅｔｙ，１９９４：３７－４５
［７７］ Ｚｈｏｕ　Ｚｈｉｈｕａ，Ｊｉａｎｇ　Ｙｕａｎ，Ｃｈｅｎ　Ｓｈｉｆｕ．Ｅｘｔｒａｃｔｉｎｇ　ｓｙｍｂｏｌｉｃ ｒｕｌｅｓ　ｆｒｏｍ　ｔｒａｉｎｅｄ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｅｎｓｅｍｂｌｅｓ ［Ｊ］． ＡＩ Ｃｏｍｍｕｎｉｃａｔｉｏｎｓ，２００３，１６（１）：３－１５
［７８］ Ｄｅ　Ｆｏｒｔｕｎｙ　Ｅ　Ｊ， Ｍａｒｔｅｎｓ　Ｄ． Ａｃｔｉｖｅ　ｌｅａｒｎｉｎｇ－ｂａｓｅｄ ｐｅｄａｇｏｇｉｃａｌ　ｒｕｌｅ　ｅｘｔｒａｃｔｉｏｎ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｎｅｕｒａｌ Ｎｅｔｗｏｒｋｓ　ａｎｄ　Ｌｅａｒｎｉｎｇ　Ｓｙｓｔｅｍｓ，２０１５，２６（１１）：２６６４－２６７７
［７９］ Ｌａｋｋａｒａｊｕ　Ｈ，Ｋａｍａｒ　Ｅ，Ｃａｒｕａｎａ　Ｒ，ｅｔ　ａｌ．Ｉｎｔｅｒｐｒｅｔａｂｌｅ　＆ ｅｘｐｌｏｒａｂｌｅ　ａｐｐｒｏｘｉｍａｔｉｏｎｓ　ｏｆ　ｂｌａｃｋ　ｂｏｘ　ｍｏｄｅｌｓ ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７０７．０１１５４，２０１７
［８０］ Ｌｉｕ　Ｘｕａｎ， Ｗａｎｇ　Ｘｉａｏｇｕａｎｇ， Ｍａｔｗｉｎ　Ｓ．Ｉｍｐｒｏｖｉｎｇ　ｔｈｅ ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ｏｆ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｗｉｔｈ　ｋｎｏｗｌｅｄｇｅ ｄｉｓｔｉｌｌａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１８ｔｈ　ＩＥＥＥ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｄａｔａ Ｍｉｎｉｎｇ　Ｗｏｒｋｓｈｏｐｓ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１８：９０５－９１２

２０９４

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

［８１］ Ｂｕｃｉｌｕǎ Ｃ， Ｃａｒｕａｎａ　 Ｒ， Ｎｉｃｕｌｅｓｃｕ－Ｍｉｚｉｌ　 Ａ． Ｍｏｄｅｌ ｃｏｍｐｒｅｓｓｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１２ｔｈ　ＡＣＭ　ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ ｏｎ　Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　ａｎｄ　Ｄａｔａ　Ｍｉｎｉｎｇ．Ｎｅｗ　Ｙｏｒｋ： ＡＣＭ，２００６：５３５－５４１
［８２］ Ｈｉｎｔｏｎ　Ｇ，Ｖｉｎｙａｌｓ　Ｏ，Ｄｅａｎ　Ｊ．Ｄｉｓｔｉｌｌｉｎｇ　ｔｈｅ　ｋｎｏｗｌｅｄｇｅ　ｉｎ　ａ ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１５０３．０２５３１，２０１５
［８３］ Ｃｒａｖｅｎ　Ｍ， Ｓｈａｖｌｉｋ　 Ｊ　 Ｗ． Ｅｘｔｒａｃｔｉｎｇ　 ｔｒｅｅ－ｓｔｒｕｃｔｕｒｅｄ ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ　ｏｆ　ｔｒａｉｎｅｄ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１０ｔｈ　Ｉｎｔ Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ， ＮＹ：Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，１９９６：２４－３０
［８４］ Ｆｒｏｓｓｔ　Ｎ，Ｈｉｎｔｏｎ　Ｇ．Ｄｉｓｔｉｌｌｉｎｇ　ａ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｉｎｔｏ　ａ　ｓｏｆｔ ｄｅｃｉｓｉｏｎ　ｔｒｅｅ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７１１．０９７８４，２０１７
［８５］ Ｔａｎ　Ｓ，Ｃａｒｕａｎａ　Ｒ，Ｈｏｏｋｅｒ　Ｇ，ｅｔ　ａｌ．Ｌｅａｒｎｉｎｇ　ｇｌｏｂａｌ　ａｄｄｉｔｉｖｅ ｅｘｐｌａｎａｔｉｏｎｓ　ｆｏｒ　ｎｅｕｒａｌ　ｎｅｔｓ　ｕｓｉｎｇ　ｍｏｄｅｌ　ｄｉｓｔｉｌｌａｔｉｏｎ ［Ｊ］． ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１８０１．０８６４０，２０１８
［８６］ Ｃｈｅ　Ｚｈｅｎｇｐｉｎｇ， Ｐｕｒｕｓｈｏｔｈａｍ　Ｓ， Ｋｈｅｍａｎｉ　Ｒ， ｅｔ　ａｌ． Ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｄｅｅｐ　ｍｏｄｅｌｓ　ｆｏｒ　ＩＣＵ　ｏｕｔｃｏｍｅ　ｐｒｅｄｉｃｔｉｏｎ［Ｃ］?? Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＡＭＩＡ　Ａｎｎｕａｌ　Ｓｙｍｐ．Ｂｅｔｈｅｓｄａ，ＭＤ：Ａｍｅｒｉｃａｎ Ｍｅｄｉｃａｌ　Ｉｎｆｏｒｍａｔｉｃｓ　Ａｓｓｏｃｉａｔｉｏｎ，２０１６：３７１－３８０
［８７］ Ｄｉｎｇ　Ｔａｏ，Ｈａｓａｎ　Ｆ，Ｂｉｃｋｅｌ　Ｗ　Ｋ，ｅｔ　ａｌ．Ｉｎｔｅｒｐｒｅｔｉｎｇ　ｓｏｃｉａｌ ｍｅｄｉａ－ｂａｓｅｄ　ｓｕｂｓｔａｎｃｅ　ｕｓｅ　ｐｒｅｄｉｃｔｉｏｎ　ｍｏｄｅｌｓ　ｗｉｔｈ　ｋｎｏｗｌｅｄｇｅ ｄｉｓｔｉｌｌａｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３０ｔｈ　ＩＥＥＥ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｔｏｏｌｓ ｗｉｔｈ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１８： ６２３－６３０
［８８］ Ｘｕ　Ｋａｉ，Ｐａｒｋ　Ｄ　Ｈ，Ｙｉ　Ｃｈａｎｇ，ｅｔ　ａｌ．Ｉｎｔｅｒｐｒｅｔｉｎｇ　ｄｅｅｐ ｃｌａｓｓｉｆｉｅｒ　ｂｙ　ｖｉｓｕａｌ　ｄｉｓｔｉｌｌａｔｉｏｎ　ｏｆ　ｄａｒｋ　ｋｎｏｗｌｅｄｇｅ ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１８０３．０４０４２，２０１８
［８９］ Ｔａｎ　Ｓ，Ｃａｒｕａｎａ　Ｒ，Ｈｏｏｋｅｒ　Ｇ，ｅｔ　ａｌ．Ｄｉｓｔｉｌｌ－ａｎｄ－ｃｏｍｐａｒｅ： Ａｕｄｉｔｉｎｇ　 ｂｌａｃｋ－ｂｏｘ　 ｍｏｄｅｌｓ　 ｕｓｉｎｇ　 ｔｒａｎｓｐａｒｅｎｔ　 ｍｏｄｅｌ ｄｉｓｔｉｌｌａｔｉｏｎ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２０１８ ＡＡＡＩ?ＡＣＭ　Ｃｏｎｆ　ｏｎ　ＡＩ， Ｅｔｈｉｃｓ，ａｎｄ　Ｓｏｃｉｅｔｙ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１８：３０３－３１０
［９０］ Ｔａｎ　Ｓ，Ｃａｒｕａｎａ　Ｒ，Ｈｏｏｋｅｒ　Ｇ，ｅｔ　ａｌ．Ｄｅｔｅｃｔｉｎｇ　ｂｉａｓ　ｉｎ　ｂｌａｃｋ－ ｂｏｘ　ｍｏｄｅｌｓ　ｕｓｉｎｇ　ｔｒａｎｓｐａｒｅｎｔ　ｍｏｄｅｌ　ｄｉｓｔｉｌｌａｔｉｏｎ ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７１０．０６１６９，２０１７
［９１］ Ｚｈａｎｇ　Ｑｕａｎｓｈｉ，Ｗａｎｇ　Ｗｅｎｇｕａｎ，Ｚｈｕ　Ｓｏｎｇｃｈｕｎ．Ｅｘａｍｉｎｉｎｇ ＣＮＮ　ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ　ｗｉｔｈ　ｒｅｓｐｅｃｔ　ｔｏ　ｄａｔａｓｅｔ　ｂｉａｓ［Ｃ］??Ｐｒｏｃ ｏｆ　ｔｈｅ　３２ｎｄ　ＡＡＡＩ　Ｃｏｎｆ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ　Ｉｎｔｅｌｌｉｇｅｎｃｅ．Ｐａｌｏ　Ａｌｔｏ， ＣＡ：ＡＡＡＩ，２０１８：４４６４－４４７３
［９２］ Ｚｈａｎｇ　Ｑｕａｎｓｈｉ，Ｗｕ　Ｙｉｎｇｎｉａｎ，Ｚｈｕ　Ｓｏｎｇｃｈｕｎ．Ｉｎｔｅｒｐｒｅｔａｂｌｅ ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３０ｔｈ　ＩＥＥＥ Ｃｏｎｆ　ｏｎ　 Ｃｏｍｐｕｔｅｒ　 Ｖｉｓｉｏｎ　 ａｎｄ　 Ｐａｔｔｅｒｎ　 Ｒｅｃｏｇｎｉｔｉｏｎ． Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１８：８８２７－８８３６
［９３］ Ｂｅｒｋｅｓ　Ｐ，Ｗｉｓｋｏｔｔ　Ｌ．Ｏｎ　ｔｈｅ　ａｎａｌｙｓｉｓ　ａｎｄ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｏｆ ｉｎｈｏｍｏｇｅｎｅｏｕｓ　ｑｕａｄｒａｔｉｃ　ｆｏｒｍｓ　ａｓ　ｒｅｃｅｐｔｉｖｅ　ｆｉｅｌｄｓ ［Ｊ］． Ｎｅｕｒａｌ　Ｃｏｍｐｕｔａｔｉｏｎ，２００６，１８（８）：１８６８－１８９５
［９４］ Ｍｏｎｔａｖｏｎ　Ｇ， Ｓａｍｅｋ　Ｗ， Ｍüｌｌｅｒ　Ｋ－Ｒ． Ｍｅｔｈｏｄｓ　ｆｏｒ ｉｎｔｅｒｐｒｅｔｉｎｇ　ａｎｄ　ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｊ］． Ｄｉｇｉｔａｌ　Ｓｉｇｎａｌ　Ｐｒｏｃｅｓｓｉｎｇ，２０１８，７３：１－１５
［９５］ Ｍａｈｅｎｄｒａｎ　Ａ， Ｖｅｄａｌｄｉ　Ａ． Ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｄｅｅｐ　ｉｍａｇｅ ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ　ｂｙ　ｉｎｖｅｒｔｉｎｇ　ｔｈｅｍ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２７ｔｈ ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ． Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１５：５１８８－５１９６

［９６］ Ｎｇｕｙｅｎ　Ａ， Ｃｌｕｎｅ　Ｊ， Ｂｅｎｇｉｏ　Ｙ，ｅｔ　ａｌ．Ｐｌｕｇ　＆ ｐｌａｙ ｇｅｎｅｒａｔｉｖｅ　ｎｅｔｗｏｒｋｓ： Ｃｏｎｄｉｔｉｏｎａｌ　ｉｔｅｒａｔｉｖｅ　ｇｅｎｅｒａｔｉｏｎ　ｏｆ ｉｍａｇｅｓ　ｉｎ　ｌａｔｅｎｔ　ｓｐａｃｅ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２９ｔｈ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ： ＩＥＥＥ，２０１７：４４６７－４４７７
［９７］ Ｎｇｕｙｅｎ　Ａ， Ｙｏｓｉｎｓｋｉ　Ｊ， Ｃｌｕｎｅ　Ｊ． Ｍｕｌｔｉｆａｃｅｔｅｄ　ｆｅａｔｕｒｅ ｖｉｓｕａｌｉｚａｔｉｏｎ：Ｕｎｃｏｖｅｒｉｎｇ　ｔｈｅ　ｄｉｆｆｅｒｅｎｔ　ｔｙｐｅｓ　ｏｆ　ｆｅａｔｕｒｅｓ ｌｅａｒｎｅｄ　ｂｙ　ｅａｃｈ　ｎｅｕｒｏｎ　ｉｎ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｊ］．ａｒＸｉｖ ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６０２．０３６１６，２０１６
［９８］ Ｓａｌｔｅｌｌｉ　Ａ，Ｔａｒａｎｔｏｌａ　Ｓ，Ｃａｍｐｏｌｏｎｇｏ　Ｆ，ｅｔ　ａｌ．Ｓｅｎｓｉｔｉｖｉｔｙ Ａｎａｌｙｓｉｓ　ｉｎ　Ｐｒａｃｔｉｃｅ：Ａ　Ｇｕｉｄｅ　ｔｏ　Ａｓｓｅｓｓｉｎｇ　Ｓｃｉｅｎｔｉｆｉｃ　Ｍｏｄｅｌｓ ［Ｍ］．Ｎｅｗ　Ｙｏｒｋ：Ｊｏｈｎ　Ｗｉｌｅｙ　＆ Ｓｏｎｓ，２００４
［９９］ Ｃｈａｔｔｅｒｊｅｅ　Ｓ， Ｈａｄｉ　Ａ　Ｓ．Ｓｅｎｓｉｔｉｖｉｔｙ　Ａｎａｌｙｓｉｓ　ｉｎ　Ｌｉｎｅａｒ Ｒｅｇｒｅｓｓｉｏｎ［Ｍ］．Ｎｅｗ　Ｙｏｒｋ：Ｊｏｈｎ　Ｗｉｌｅｙ　＆ Ｓｏｎｓ，２００９
［１００］ Ｈｏｍｍａ　Ｔ， Ｓａｌｔｅｌｌｉ　Ａ．Ｉｍｐｏｒｔａｎｃｅ　ｍｅａｓｕｒｅｓ　ｉｎ　ｇｌｏｂａｌ ｓｅｎｓｉｔｉｖｉｔｙ　ａｎａｌｙｓｉｓ　ｏｆ　ｎｏｎｌｉｎｅａｒ　ｍｏｄｅｌｓ ［Ｊ］．Ｒｅｌｉａｂｉｌｉｔｙ Ｅｎｇｉｎｅｅｒｉｎｇ　＆ Ｓｙｓｔｅｍ　Ｓａｆｅｔｙ，１９９６，５２（１）：１－１７
［１０１］ Ｓａｌｔｅｌｌｉ　Ａ，Ｔａｒａｎｔｏｌａ　Ｓ，Ｃｈａｎ　Ｋ－Ｓ．Ａ　ｑｕａｎｔｉｔａｔｉｖｅ　ｍｏｄｅｌ－
ｉｎｄｅｐｅｎｄｅｎｔ　ｍｅｔｈｏｄ　ｆｏｒ　ｇｌｏｂａｌ　ｓｅｎｓｉｔｉｖｉｔｙ　ａｎａｌｙｓｉｓ　ｏｆ　ｍｏｄｅｌ ｏｕｔｐｕｔ［Ｊ］．Ｔｅｃｈｎｏｍｅｔｒｉｃｓ，１９９９，４１（１）：３９－５６ ［１０２］ Ｇｅｖｒｅｙ　Ｍ，Ｄｉｍｏｐｏｕｌｏｓ　Ｉ，Ｌｅｋ　Ｓ．Ｒｅｖｉｅｗ　ａｎｄ　ｃｏｍｐａｒｉｓｏｎ　ｏｆ ｍｅｔｈｏｄｓ　ｔｏ　ｓｔｕｄｙ　ｔｈｅ　ｃｏｎｔｒｉｂｕｔｉｏｎ　ｏｆ　ｖａｒｉａｂｌｅｓ　ｉｎ　ａｒｔｉｆｉｃｉａｌ ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｍｏｄｅｌｓ ［Ｊ］．Ｅｃｏｌｏｇｉｃａｌ　Ｍｏｄｅｌｌｉｎｇ，２００３， １６０（３）：２４９－２６４ ［１０３］ Ｅｎｇｅｌｂｒｅｃｈｔ　Ａ　Ｐ，Ｃｌｏｅｔｅ　Ｉ，Ｚｕｒａｄａ　Ｊ　Ｍ．Ｄｅｔｅｒｍｉｎｉｎｇ　ｔｈｅ ｓｉｇｎｉｆｉｃａｎｃｅ　ｏｆ　ｉｎｐｕｔ　ｐａｒａｍｅｔｅｒｓ　ｕｓｉｎｇ　ｓｅｎｓｉｔｉｖｉｔｙ　ａｎａｌｙｓｉｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３ｒｄ　Ｉｎｔ　Ｗｏｒｋｓｈｏｐ　ｏｎ　Ａｒｔｉｆｉｃｉａｌ　Ｎｅｕｒａｌ Ｎｅｔｗｏｒｋｓ．Ｂｅｒｌｉｎ：Ｓｐｒｉｎｇｅｒ，１９９５：３８２－３８８ ［１０４］ Ｈａｒｒｉｎｇｔｏｎ　Ｐ　Ｄ　Ｂ，Ｗａｎ　Ｃ．Ｓｅｎｓｉｔｉｖｉｔｙ　ａｎａｌｙｓｉｓ　ａｐｐｌｉｅｄ　ｔｏ ａｒｔｉｆｉｃｉａｌ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ： Ｗｈａｔ　ｈａｓ　ｍｙ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ ａｃｔｕａｌｌｙ　ｌｅａｒｎｅｄ？ ［Ｊ］．Ａｎａｌｙｔｉｃａｌ　Ｃｈｅｍｉｓｔｒｙ，１９９８，７０： ２９８３－２９９０ ［１０５］ Ｓｕｎｇ　Ａ．Ｒａｎｋｉｎｇ　ｉｍｐｏｒｔａｎｃｅ　ｏｆ　ｉｎｐｕｔ　ｐａｒａｍｅｔｅｒｓ　ｏｆ　ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ［Ｊ］．Ｅｘｐｅｒｔ　Ｓｙｓｔｅｍｓ　ｗｉｔｈ　Ａｐｐｌｉｃａｔｉｏｎｓ，１９９８，１５ （３?４）：４０５－４１１ ［１０６］ Ｒｏｂｎｉｋ－ｉｋｏｎｊａ　Ｍ，Ｋｏｎｏｎｅｎｋｏ　Ｉ．Ｅｘｐｌａｉｎｉｎｇ　ｃｌａｓｓｉｆｉｃａｔｉｏｎｓ ｆｏｒ　ｉｎｄｉｖｉｄｕａｌ　ｉｎｓｔａｎｃｅｓ ［Ｊ］． ＩＥＥＥ　 Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ Ｋｎｏｗｌｅｄｇｅ　ａｎｄ　Ｄａｔａ　Ｅｎｇｉｎｅｅｒｉｎｇ，２００８，２０（５）：５８９－６００ ［１０７］ Ｌｉ　Ｊｉｗｅｉ， Ｍｏｎｒｏｅ　Ｗ，Ｊｕｒａｆｓｋｙ　Ｄ．Ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ　ｔｈｒｏｕｇｈ　ｒｅｐｒｅｓｅｎｔａｔｉｏｎ　ｅｒａｓｕｒｅ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ ａｒＸｉｖ：１６１２．０８２２０，２０１６ ［１０８］ Ｒｉｂｅｉｒｏ　Ｍ　Ｔ，Ｓｉｎｇｈ　Ｓ，Ｇｕｅｓｔｒｉｎ　Ｃ．Ｎｏｔｈｉｎｇ　ｅｌｓｅ　ｍａｔｔｅｒｓ： Ｍｏｄｅｌ－ａｇｎｏｓｔｉｃ　 ｅｘｐｌａｎａｔｉｏｎｓ　 ｂｙ　 ｉｄｅｎｔｉｆｙｉｎｇ　 ｐｒｅｄｉｃｔｉｏｎ ｉｎｖａｒｉａｎｃｅ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６１１．０５８１７，２０１６ ［１０９］ Ｌａｎｄｅｃｋｅｒ　Ｗ，Ｔｈｏｍｕｒｅ　Ｍ　Ｄ，Ｂｅｔｔｅｎｃｏｕｒｔ　Ｌ　Ｍ，ｅｔ　ａｌ．
Ｉｎｔｅｒｐｒｅｔｉｎｇ　 ｉｎｄｉｖｉｄｕａｌ　 ｃｌａｓｓｉｆｉｃａｔｉｏｎｓ　 ｏｆ　 ｈｉｅｒａｒｃｈｉｃａｌ ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｓｙｍｐ　ｏｎ　Ｃｏｍｐｕｔａｔｉｏｎａｌ Ｉｎｔｅｌｌｉｇｅｎｃｅ　ａｎｄ　Ｄａｔａ　Ｍｉｎｉｎｇ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ， ２０１３：３２－３８ ［１１０］ Ｄｉｎｇ　Ｙａｎｚｈｕｏ，Ｌｉｕ　Ｙａｎｇ，Ｌｕａｎ　Ｈｕａｎｂｏ，ｅｔ　ａｌ．Ｖｉｓｕａｌｉｚｉｎｇ ａｎｄ　ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｎｅｕｒａｌ　ｍａｃｈｉｎｅ　ｔｒａｎｓｌａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ ｔｈｅ　５５ｔｈ　 Ａｎｎｕａｌ　 Ｍｅｅｔｉｎｇ　 ｏｆ　 ｔｈｅ　 Ａｓｓｏｃｉａｔｉｏｎ　 ｆｏｒ Ｃｏｍｐｕｔａｔｉｏｎａｌ　Ｌｉｎｇｕｉｓｔｉｃｓ．Ｓｔｒｏｕｄｓｂｕｒｇ，ＰＡ：Ａｓｓｏｃｉａｔｉｏｎ ｆｏｒ　Ｃｏｍｐｕｔａｔｉｏｎａｌ　Ｌｉｎｇｕｉｓｔｉｃｓ，２０１７：１１５０－１１５９．

纪 守 领 等 ：机 器 学 习 模 型 可 解 释 性 方 法 、应 用 与 安 全 研 究 综 述

２０９５

［１１１］ Ａｒｒａｓ　Ｌ，Ｈｏｒｎ　Ｆ，Ｍｏｎｔａｖｏｎ　Ｇ，ｅｔ　ａｌ．Ｗｈａｔ　ｉｓ　ｒｅｌｅｖａｎｔ　ｉｎ　ａ ｔｅｘｔ　ｄｏｃｕｍｅｎｔ？： Ａｎ　 ｉｎｔｅｒｐｒｅｔａｂｌｅ　 ｍａｃｈｉｎｅ　 ｌｅａｒｎｉｎｇ ａｐｐｒｏａｃｈ［Ｊ］．ＰｌｏＳ　Ｏｎｅ，２０１７，１２（８）：ｅ０１８１１４２
［１１２］ Ｃａｒｔｅｒ　Ｓ．Ｅｘｐｌｏｒｉｎｇ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ　ｗｉｔｈ　ａｃｔｉｖａｔｉｏｎ　ａｔｌａｓｅｓ ［ＯＬ］．［２０１９－０６－１１］．ｈｔｔｐｓ：??ａｉ．ｇｏｏｇｌｅｂｌｏｇ．ｃｏｍ?２０１９?０３? ｅｘｐｌｏｒｉｎｇ－ｎｅｕｒａｌ－ｎｅｔｗｏｒｋｓ．ｈｔｍｌ
［１１３］ Ｄｏｓｏｖｉｔｓｋｉｙ　Ａ，Ｂｒｏｘ　Ｔ．Ｉｎｖｅｒｔｉｎｇ　ｖｉｓｕａｌ　ｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ ｗｉｔｈ　ｃｏｎｖｏｌｕｔｉｏｎａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２８ｔｈ　ＩＥＥＥ Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ　Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１６：４８２９－４８３７
［１１４］ Ｚｈｏｕ　Ｂｏｌｅｉ，Ｋｈｏｓｌａ　Ａ，Ｌａｐｅｄｒｉｚａ　Ａ，ｅｔ　ａｌ．Ｏｂｊｅｃｔ　ｄｅｔｅｃｔｏｒｓ ｅｍｅｒｇｅ　ｉｎ　ｄｅｅｐ　ｓｃｅｎｅ　ＣＮＮｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１４１２． ６８５６，２０１４
［１１５］ Ｓｈｒｉｋｕｍａｒ　Ａ，Ｇｒｅｅｎｓｉｄｅ　Ｐ，Ｋｕｎｄａｊｅ　Ａ．Ｌｅａｒｎｉｎｇ　ｉｍｐｏｒｔａｎｔ ｆｅａｔｕｒｅｓ　ｔｈｒｏｕｇｈ　ｐｒｏｐａｇａｔｉｎｇ　ａｃｔｉｖａｔｉｏｎ　ｄｉｆｆｅｒｅｎｃｅｓ ［Ｃ］?? Ｐｒｏｃ　ｏｆ　ｔｈｅ　３４ｔｈ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ．Ｔａｈｏｅ，ＣＡ： Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｓｏｃｉｅｔｙ，２０１７：３１４５－３１５３
［１１６］ Ｌａｐｕｓｃｈｋｉｎ　Ｓ，Ｂｉｎｄｅｒ　Ａ， Ｍｏｎｔａｖｏｎ　Ｇ，ｅｔ　ａｌ．Ａｎａｌｙｚｉｎｇ ｃｌａｓｓｉｆｉｅｒｓ：Ｆｉｓｈｅｒ　ｖｅｃｔｏｒｓ　ａｎｄ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｃ］?? Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　Ｖｉｓｉｏｎ　ａｎｄ　Ｐａｔｔｅｒｎ Ｒｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１６：２９１２－２９２０
［１１７］ Ｃａｄａｍｕｒｏ　Ｇ，Ｇｉｌａｄ－Ｂａｃｈｒａｃｈ　Ｒ，Ｚｈｕ　Ｘ．Ｄｅｂｕｇｇｉｎｇ　ｍａｃｈｉｎｅ ｌｅａｒｎｉｎｇ　ｍｏｄｅｌｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３３ｒｄ　ＩＣＭＬ　Ｗｏｒｋｓｈｏｐ　ｏｎ Ｒｅｌｉａｂｌｅ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　ｉｎ　ｔｈｅ　Ｗｉｌｄ．Ｔａｈｏｅ， ＣＡ： Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｍａｃｈｉｎｅ　Ｌｅａｒｎｉｎｇ　Ｓｏｃｉｅｔｙ，２０１６
［１１８］ Ｋｕｌｅｓｚａ　Ｔ，Ｂｕｒｎｅｔｔ　Ｍ， Ｗｏｎｇ　Ｗ－Ｋ，ｅｔ　ａｌ．Ｐｒｉｎｃｉｐｌｅｓ　ｏｆ ｅｘｐｌａｎａｔｏｒｙ　ｄｅｂｕｇｇｉｎｇ　ｔｏ　ｐｅｒｓｏｎａｌｉｚｅ　ｉｎｔｅｒａｃｔｉｖｅ　ｍａｃｈｉｎｅ ｌｅａｒｎｉｎｇ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２０ｔｈ　ＡＣＭ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｉｎｔｅｌｌｉｇｅｎｔ Ｕｓｅｒ　Ｉｎｔｅｒｆａｃｅｓ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１５：１２６－１３７
［１１９］ Ｋｕｌｅｓｚａ　Ｔ，Ｓｔｕｍｐｆ　Ｓ， Ｂｕｒｎｅｔｔ　Ｍ，ｅｔ　ａｌ．Ｅｘｐｌａｎａｔｏｒｙ ｄｅｂｕｇｇｉｎｇ： Ｓｕｐｐｏｒｔｉｎｇ　ｅｎｄ－ｕｓｅｒ　ｄｅｂｕｇｇｉｎｇ　ｏｆ　ｍａｃｈｉｎｅ－ ｌｅａｒｎｅｄ　ｐｒｏｇｒａｍｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｓｙｍｐ　ｏｎ　Ｖｉｓｕａｌ Ｌａｎｇｕａｇｅｓ　ａｎｄ　Ｈｕｍａｎ－Ｃｅｎｔｒｉｃ　Ｃｏｍｐｕｔｉｎｇ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１０：４１－４８
［１２０］ Ｂａｓｔａｎｉ　Ｏ，Ｋｉｍ　Ｃ，Ｂａｓｔａｎｉ　Ｈ．Ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ　ｖｉａ　ｍｏｄｅｌ ｅｘｔｒａｃｔｉｏｎ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７０６．０９７７３，２０１７
［１２１］ Ｋａｈｎｇ　Ｍ，Ａｎｄｒｅｗｓ　Ｐ　Ｙ，Ｋａｌｒｏ　Ａ，ｅｔ　ａｌ．Ａｃｔｉｖｉｓ：Ｖｉｓｕａｌ ｅｘｐｌｏｒａｔｉｏｎ　ｏｆ　ｉｎｄｕｓｔｒｙ－ｓｃａｌｅ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｍｏｄｅｌｓ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｖｉｓｕａｌｉｚａｔｉｏｎ　ａｎｄ　Ｃｏｍｐｕｔｅｒ Ｇｒａｐｈｉｃｓ，２０１８，２４（１）：８８－９７
［１２２］ Ｓｔｒｏｂｅｌｔ　Ｈ，Ｇｅｈｒｍａｎｎ　Ｓ，Ｐｆｉｓｔｅｒ　Ｈ，ｅｔ　ａｌ．Ｌｓｔｍｖｉｓ：Ａ　ｔｏｏｌ ｆｏｒ　ｖｉｓｕａｌ　ａｎａｌｙｓｉｓ　ｏｆ　ｈｉｄｄｅｎ　ｓｔａｔｅ　ｄｙｎａｍｉｃｓ　ｉｎ　ｒｅｃｕｒｒｅｎｔ ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｖｉｓｕａｌｉｚａｔｉｏｎ ａｎｄ　Ｃｏｍｐｕｔｅｒ　Ｇｒａｐｈｉｃｓ，２０１８，２４（１）：６６７－６７６
［１２３］ Ｗｏｎｇｓｕｐｈａｓａｗａｔ　Ｋ， Ｓｍｉｌｋｏｖ　Ｄ， Ｗｅｘｌｅｒ　Ｊ， ｅｔ　ａｌ． Ｖｉｓｕａｌｉｚｉｎｇ　ｄａｔａｆｌｏｗ　ｇｒａｐｈｓ　ｏｆ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｍｏｄｅｌｓ　ｉｎ ｔｅｎｓｏｒｆｌｏｗ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ　Ｖｉｓｕａｌｉｚａｔｉｏｎ　ａｎｄ Ｃｏｍｐｕｔｅｒ　Ｇｒａｐｈｉｃｓ，２０１８，２４（１）：１－１２
［１２４］ Ｚｈａｎｇ　Ｊｉａｗｅｉ，Ｗａｎｇ　Ｙａｎｇ，Ｍｏｌｉｎｏ　Ｐ，ｅｔ　ａｌ．Ｍａｎｉｆｏｌｄ：Ａ ｍｏｄｅｌ－ａｇｎｏｓｔｉｃ　ｆｒａｍｅｗｏｒｋ　ｆｏｒ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ａｎｄ　ｄｉａｇｎｏｓｉｓ ｏｆ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｍｏｄｅｌｓ ［Ｊ］．ＩＥＥＥ　Ｔｒａｎｓａｃｔｉｏｎｓ　ｏｎ Ｖｉｓｕａｌｉｚａｔｉｏｎ　ａｎｄ　Ｃｏｍｐｕｔｅｒ　Ｇｒａｐｈｉｃｓ，２０１９，２５（１）：３６４－ ３７３

［１２５］ Ｋｒａｕｓｅ　Ｊ，Ｐｅｒｅｒ　Ａ，Ｎｇ　Ｋ．Ｉｎｔｅｒａｃｔｉｎｇ　ｗｉｔｈ　ｐｒｅｄｉｃｔｉｏｎｓ： Ｖｉｓｕａｌ　ｉｎｓｐｅｃｔｉｏｎ　ｏｆ　ｂｌａｃｋ－ｂｏｘ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ　ｍｏｄｅｌｓ［Ｃ］ ??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＡＣＭ　ＣＨＩ　Ｃｏｎｆ　ｏｎ　Ｈｕｍａｎ　Ｆａｃｔｏｒｓ　ｉｎ Ｃｏｍｐｕｔｉｎｇ　Ｓｙｓｔｅｍｓ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１６：５６８６－５６９７
［１２６］ Ｋｒａｕｓｅ　Ｊ，Ｄａｓｇｕｐｔａ　Ａ，Ｓｗａｒｔｚ　Ｊ，ｅｔ　ａｌ．Ａ　ｗｏｒｋｆｌｏｗ　ｆｏｒ
ｖｉｓｕａｌ　ｄｉａｇｎｏｓｔｉｃｓ　ｏｆ　ｂｉｎａｒｙ　ｃｌａｓｓｉｆｉｅｒｓ　ｕｓｉｎｇ　ｉｎｓｔａｎｃｅ－ｌｅｖｅｌ ｅｘｐｌａｎａｔｉｏｎｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｖｉｓｕａｌ Ａｎａｌｙｔｉｃｓ　Ｓｃｉｅｎｃｅ　ａｎｄ　Ｔｅｃｈｎｏｌｏｇｙ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ， ２０１７：１６２－１７２ ［１２７］ Ｐａｉｖａ　Ｊ　Ｇ　Ｓ，Ｓｃｈｗａｒｔｚ　Ｗ　Ｒ，Ｐｅｄｒｉｎｉ　Ｈ，ｅｔ　ａｌ．Ａｎ　ａｐｐｒｏａｃｈ ｔｏ　ｓｕｐｐｏｒｔｉｎｇ　ｉｎｃｒｅｍｅｎｔａｌ　ｖｉｓｕａｌ　ｄａｔａ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ ［Ｊ］． ＩＥＥＥ　 Ｔｒａｎｓａｃｔｉｏｎｓ　 ｏｎ　 Ｖｉｓｕａｌｉｚａｔｉｏｎ　 ａｎｄ　 Ｃｏｍｐｕｔｅｒ Ｇｒａｐｈｉｃｓ，２０１５，２１（１）：４－１７ ［１２８］ Ｂｒｏｏｋｓ　Ｍ，Ａｍｅｒｓｈｉ　Ｓ，Ｌｅｅ　Ｂ，ｅｔ　ａｌ．ＦｅａｔｕｒｅＩｎｓｉｇｈｔ：Ｖｉｓｕａｌ
ｓｕｐｐｏｒｔ　ｆｏｒ　ｅｒｒｏｒ－ｄｒｉｖｅｎ　ｆｅａｔｕｒｅ　ｉｄｅａｔｉｏｎ　ｉｎ　ｔｅｘｔ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　ＩＥＥＥ　Ｃｏｎｆ　ｏｎ　Ｖｉｓｕａｌ　Ａｎａｌｙｔｉｃｓ　Ｓｃｉｅｎｃｅ　ａｎｄ Ｔｅｃｈｎｏｌｏｇｙ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１５：１０５－１１２ ［１２９］ Ａｒｖａｎｉｔｉ　Ｅ，Ｆｒｉｃｋｅｒ　Ｋ　Ｓ， Ｍｏｒｅｔ　Ｍ，ｅｔ　ａｌ．Ａｕｔｏｍａｔｅｄ
Ｇｌｅａｓｏｎ　ｇｒａｄｉｎｇ　ｏｆ　ｐｒｏｓｔａｔｅ　ｃａｎｃｅｒ　ｔｉｓｓｕｅ　ｍｉｃｒｏａｒｒａｙｓ　ｖｉａ ｄｅｅｐ　ｌｅａｒｎｉｎｇ［Ｊ］．Ｓｃｉｅｎｔｉｆｉｃ　Ｒｅｐｏｒｔｓ，２０１８，８（１）：１２０５４－ １２０５４ ［１３０］ Ａｓｐｕｒｕ－Ｇｕｚｉｋ　Ａ， Ｌｉｎｄｈ　Ｒ， Ｒｅｉｈｅｒ　Ｍ． Ｔｈｅ　ｍａｔｔｅｒ ｓｉｍｕｌａｔｉｏｎ （ｒ）ｅｖｏｌｕｔｉｏｎ［Ｊ］．ＡＣＳ　Ｃｅｎｔｒａｌ　Ｓｃｉｅｎｃｅ，２０１８，４ （２）：１４４－１５２ ［１３１］ Ｂｏｕｋｏｕｖａｌａｓ　Ｚ，Ｅｌｔｏｎ　Ｄ　Ｃ，Ｃｈｕｎｇ　Ｐ　Ｗ，ｅｔ　ａｌ．Ｉｎｄｅｐｅｎｄｅｎｔ ｖｅｃｔｏｒ　ａｎａｌｙｓｉｓ　ｆｏｒ　ｄａｔａ　ｆｕｓｉｏｎ　ｐｒｉｏｒ　ｔｏ　ｍｏｌｅｃｕｌａｒ　ｐｒｏｐｅｒｔｙ ｐｒｅｄｉｃｔｉｏｎ　ｗｉｔｈ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ： １８１１．００６２８，２０１８ ［１３２］ Ｈｓｅ　Ｆ，Ｇａｌｖáｎ　Ｉ　Ｆ，Ａｓｐｕｒｕ－Ｇｕｚｉｋ　Ａ，ｅｔ　ａｌ．Ｈｏｗ　ｍａｃｈｉｎｅ
ｌｅａｒｎｉｎｇ　ｃａｎ　ａｓｓｉｓｔ　ｔｈｅ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｏｆ　ａｂ　ｉｎｉｔｉｏ　ｍｏｌｅｃｕｌａｒ ｄｙｎａｍｉｃｓ　ｓｉｍｕｌａｔｉｏｎｓ　ａｎｄ　ｃｏｎｃｅｐｔｕａｌ　ｕｎｄｅｒｓｔａｎｄｉｎｇ　ｏｆ ｃｈｅｍｉｓｔｒｙ［Ｊ］．Ｃｈｅｍｉｃａｌ　Ｓｃｉｅｎｃｅ，２０１９，１０（８）：２２９８－２３０７ ［１３３］ Ｓｃｈüｔｔ　Ｋ　Ｔ，Ａｒｂａｂｚａｄａｈ　Ｆ，Ｃｈｍｉｅｌａ　Ｓ，ｅｔ　ａｌ．Ｑｕａｎｔｕｍ－ ｃｈｅｍｉｃａｌ　ｉｎｓｉｇｈｔｓ　ｆｒｏｍ　ｄｅｅｐ　ｔｅｎｓｏｒ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｊ］． ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１６０９．０８５２９，２０１６ ［１３４］ Ｙｕｅ　Ｔｉａｎｗｅｉ，Ｗａｎｇ　Ｈａｏｈａｎ．Ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｆｏｒ　ｇｅｎｏｍｉｃｓ： Ａ　ｃｏｎｃｉｓｅ　ｏｖｅｒｖｉｅｗ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１８０２．００８１０，
２０１８ ［１３５］ Ａｎｇｅｒｍｕｅｌｌｅｒ　Ｃ，Ｌｅｅ　Ｈ　Ｊ， Ｒｅｉｋ　Ｗ，ｅｔ　ａｌ．ＤｅｅｐＣｐＧ：
Ａｃｃｕｒａｔｅ　ｐｒｅｄｉｃｔｉｏｎ　ｏｆ　ｓｉｎｇｌｅ－ｃｅｌｌ　ＤＮＡ　ｍｅｔｈｙｌａｔｉｏｎ　ｓｔａｔｅｓ ｕｓｉｎｇ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ［Ｊ］．Ｇｅｎｏｍｅ　Ｂｉｏｌｏｇｙ，２０１７，１８（１）：６７ ［１３６］ Ｑｕａｎｇ　Ｄ，Ｘｉｅ　Ｘｉａｏｈｕｉ．ＤａｎＱ：Ａ　ｈｙｂｒｉｄ　ｃｏｎｖｏｌｕｔｉｏｎａｌ　ａｎｄ ｒｅｃｕｒｒｅｎｔ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋ　ｆｏｒ　ｑｕａｎｔｉｆｙｉｎｇ　ｔｈｅ　ｆｕｎｃｔｉｏｎ ｏｆ　ＤＮＡ　ｓｅｑｕｅｎｃｅｓ ［Ｊ］．Ｎｕｃｌｅｉｃ　Ａｃｉｄｓ　Ｒｅｓｅａｒｃｈ，２０１６，４４ （１１）：１０７ ［１３７］ Ｌａｎｃｈａｎｔｉｎ　Ｊ， Ｓｉｎｇｈ　Ｒ， Ｗａｎｇ　Ｂ， ｅｔ　ａｌ．Ｄｅｅｐ　ｍｏｔｉｆ ｄａｓｈｂｏａｒｄ： Ｖｉｓｕａｌｉｚｉｎｇ　 ａｎｄ　 ｕｎｄｅｒｓｔａｎｄｉｎｇ　 ｇｅｎｏｍｉｃ ｓｅｑｕｅｎｃｅｓ　ｕｓｉｎｇ　ｄｅｅｐ　ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２２ｎｄ Ｐａｃｉｆｉｃ　Ｓｙｍｐ　ｏｎ　Ｂｉｏｃｏｍｐｕｔｉｎｇ．Ｓｉｎｇａｐｏｒｅ：Ｗｏｒｌｄ　Ｓｃｉｅｎｔｉｆｉｃ Ｐｕｂｌｉｓｈｉｎｇ　Ｃｏ，２０１７：２５４－２６５ ［１３８］ Ａｌｉｐａｎａｈｉ　Ｂ，Ｄｅｌｏｎｇ　Ａ，Ｗｅｉｒａｕｃｈ　Ｍ　Ｔ，ｅｔ　ａｌ．Ｐｒｅｄｉｃｔｉｎｇ　ｔｈｅ ｓｅｑｕｅｎｃｅ　ｓｐｅｃｉｆｉｃｉｔｉｅｓ　ｏｆ　ＤＮＡ－ａｎｄ　ＲＮＡ－ｂｉｎｄｉｎｇ　ｐｒｏｔｅｉｎｓ　ｂｙ ｄｅｅｐ　ｌｅａｒｎｉｎｇ ［Ｊ］．Ｎａｔｕｒｅ　Ｂｉｏｔｅｃｈｎｏｌｏｇｙ，２０１５，３３（８）： ８３１－８３８

２０９６

计 算 机 研 究 与 发 展 　２０１９，５６（１０）

［１３９］ Ｇｕｌｉａ　Ｎ，Ｓｉｎｇｈ　Ｓ，Ｓａｐｒａ　Ｌ．Ａ　ｓｔｕｄｙ　ｏｎ　ｄｉｆｆｅｒｅｎｔ　ｃｌａｓｓｉｆｉｃａｔｉｏｎ ｍｏｄｅｌｓ　ｆｏｒ　ｋｎｏｗｌｅｄｇｅ　ｄｉｓｃｏｖｅｒｙ［Ｊ］．Ｉｎｔｅｒｎａｔｉｏｎａｌ　Ｊｏｕｒｎａｌ　ｏｆ Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ　Ｍｏｂ　Ｃｏｍｐｕｔｅｒ，２０１５，４（６）：２４１－２４８
［１４０］ Ｈｅｌｍａ　Ｃ．Ｄａｔａ　ｍｉｎｉｎｇ　ａｎｄ　ｋｎｏｗｌｅｄｇｅ　ｄｉｓｃｏｖｅｒｙ　ｉｎ　ｐｒｅｄｉｃｔｉｖｅ ｔｏｘｉｃｏｌｏｇｙ［Ｊ］．ＳＡＲ　ａｎｄ　ＱＳＡＲ　ｉｎ　Ｅｎｖｉｒｏｎｍｅｎｔａｌ　Ｒｅｓｅａｒｃｈ， ２００４，１５（５－６）：３６７－３８３
［１４１］ Ｂｅｒｔｉｎｉ　Ｅ，Ｌａｌａｎｎｅ　Ｄ．Ｉｎｖｅｓｔｉｇａｔｉｎｇ　ａｎｄ　ｒｅｆｌｅｃｔｉｎｇ　ｏｎ　ｔｈｅ ｉｎｔｅｇｒａｔｉｏｎ　ｏｆ　ａｕｔｏｍａｔｉｃ　ｄａｔａ　ａｎａｌｙｓｉｓ　ａｎｄ　ｖｉｓｕａｌｉｚａｔｉｏｎ　ｉｎ ｋｎｏｗｌｅｄｇｅ　ｄｉｓｃｏｖｅｒｙ ［Ｊ］． ＡＣＭ　ＳＩＧＫＤＤ　Ｅｘｐｌｏｒａｔｉｏｎｓ Ｎｅｗｓｌｅｔｔｅｒ，２０１０，１１（２）：９－１８
［１４２］ Ｆａｙｙａｄ　Ｕ， Ｐｉａｔｅｔｓｋｙ－Ｓｈａｐｉｒｏ　Ｇ，Ｓｍｙｔｈ　Ｐ．Ｆｒｏｍ　ｄａｔａ ｍｉｎｉｎｇ　ｔｏ　ｋｎｏｗｌｅｄｇｅ　ｄｉｓｃｏｖｅｒｙ　ｉｎ　ｄａｔａｂａｓｅｓ ［Ｊ］． ＡＩ Ｍａｇａｚｉｎｅ，１９９６，１７（３）：３７－３７
［１４３］ Ｇｏｏｄｆｅｌｌｏｗ　Ｉ　Ｊ， Ｓｈｌｅｎｓ　Ｊ， Ｓｚｅｇｅｄｙ　Ｃ．Ｅｘｐｌａｉｎｉｎｇ　ａｎｄ ｈａｒｎｅｓｓｉｎｇ　ａｄｖｅｒｓａｒｉａｌ　ｅｘａｍｐｌｅｓ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ： １４１２．６５７２，２０１４
［１４４］ Ｃａｒｌｉｎｉ　Ｎ，Ｗａｇｎｅｒ　Ｄ．Ｔｏｗａｒｄｓ　ｅｖａｌｕａｔｉｎｇ　ｔｈｅ　ｒｏｂｕｓｔｎｅｓｓ　ｏｆ ｎｅｕｒａｌ　ｎｅｔｗｏｒｋｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３８ｔｈ　ＩＥＥＥ　Ｓｙｍｐ　ｏｎ Ｓｅｃｕｒｉｔｙ　ａｎｄ　Ｐｒｉｖａｃｙ．Ｐｉｓｃａｔａｗａｙ，ＮＪ：ＩＥＥＥ，２０１７：３９－５７
［１４５］ Ｔａｏ　Ｇｕａｎｈｏｎｇ， Ｍａ　Ｓｈｉｑｉｎｇ，Ｌｉｕ　Ｙｉｎｇｑｉ，ｅｔ　ａｌ．Ａｔｔａｃｋｓ ｍｅｅｔ　 ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ： Ａｔｔｒｉｂｕｔｅ－ｓｔｅｅｒｅｄ　 ｄｅｔｅｃｔｉｏｎ　 ｏｆ ａｄｖｅｒｓａｒｉａｌ　ｓａｍｐｌｅｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３２ｎｄ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ Ｎｅｕｒａｌ　Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ，ＮＹ： Ｃｕｒｒａｎ　Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，２０１８：７７１７－７７２８
［１４６］ Ｌｉｕ　Ｎｉｎｇｈａｏ，Ｙａｎｇ　Ｈｏｎｇｘｉａ，Ｈｕ　Ｘｉａ．Ａｄｖｅｒｓａｒｉａｌ　ｄｅｔｅｃｔｉｏｎ ｗｉｔｈ　ｍｏｄｅｌ　ｉｎｔｅｒｐｒｅｔａｔｉｏｎ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２４ｔｈ　ＡＣＭ ＳＩＧＫＤＤ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｋｎｏｗｌｅｄｇｅ　Ｄｉｓｃｏｖｅｒｙ　＆ Ｄａｔａ　Ｍｉｎｉｎｇ． Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１８：１８０３－１８１１
［１４７］ Ｓｈｉ　Ｃｈｅｎｇｈｕｉ，Ｘｕ　Ｘｉａｏｇａｎｇ，Ｊｉ　Ｓｈｏｕｌｉｎｇ，ｅｔ　ａｌ．Ａｄｖｅｒｓａｒｉａｌ ＣＡＰＴＣＨＡｓ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１９０１．０１１０７，２０１９
［１４８］ Ｐａｐｅｒｎｏｔ　Ｎ，Ｍｃｄａｎｉｅｌ　Ｐ，Ｊｈａ　Ｓ，ｅｔ　ａｌ．Ｔｈｅ　ｌｉｍｉｔａｔｉｏｎｓ　ｏｆ ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｉｎ　ａｄｖｅｒｓａｒｉａｌ　ｓｅｔｔｉｎｇｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　１ｓｔ ＩＥＥＥ　Ｅｕｒｏｐｅａｎ　Ｓｙｍｐ　ｏｎ　Ｓｅｃｕｒｉｔｙ　ａｎｄ　Ｐｒｉｖａｃｙ．Ｐｉｓｃａｔａｗａｙ， ＮＪ：ＩＥＥＥ，２０１６：３７２－３８７
［１４９］ Ｌｉ　Ｘｕｒｏｎｇ，Ｊｉ　Ｓｈｏｕｌｉｎｇ， Ｈａｎ　Ｍｅｎｇ，ｅｔ　ａｌ．Ａｄｖｅｒｓａｒｉａｌ ｅｘａｍｐｌｅｓ　ｖｅｒｓｕｓ　ｃｌｏｕｄ－ｂａｓｅｄ　ｄｅｔｅｃｔｏｒｓ： Ａ　ｂｌａｃｋ－ｂｏｘ ｅｍｐｉｒｉｃａｌ　ｓｔｕｄｙ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１９０１．０１２２３，２０１９
［１５０］ Ｐａｐｅｒｎｏｔ　Ｎ， Ｍｃｄａｎｉｅｌ　Ｐ，Ｇｏｏｄｆｅｌｌｏｗ　Ｉ，ｅｔ　ａｌ．Ｐｒａｃｔｉｃａｌ ｂｌａｃｋ－ｂｏｘ　ａｔｔａｃｋｓ　ａｇａｉｎｓｔ　ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ １２ｔｈ　ＡＣＭ　Ａｓｉａ　Ｃｏｎｆ　ｏｎ　Ｃｏｍｐｕｔｅｒ　ａｎｄ　Ｃｏｍｍｕｎｉｃａｔｉｏｎｓ Ｓｅｃｕｒｉｔｙ．Ｎｅｗ　Ｙｏｒｋ：ＡＣＭ，２０１７：５０６－５１９
［１５１］ Ｇｈｏｒｂａｎｉ　Ａ， Ａｂｉｄ　Ａ， Ｚｏｕ　Ｊ．Ｉｎｔｅｒｐｒｅｔａｔｉｏｎ　ｏｆ　ｎｅｕｒａｌ ｎｅｔｗｏｒｋｓ　ｉｓ　ｆｒａｇｉｌｅ ［Ｊ］．ａｒＸｉｖ　ｐｒｅｐｒｉｎｔ　ａｒＸｉｖ：１７１０．１０５４７， ２０１７
［１５２］ Ｚｈａｎｇ　Ｘｉｎｙａｎｇ， Ｗａｎｇ　Ｎｉｎｇｆｅｉ， Ｊｉ　Ｓｈｏｕｌｉｎｇ， ｅｔ　ａｌ． Ｉｎｔｅｒｐｒｅｔａｂｌｅ　ｄｅｅｐ　ｌｅａｒｎｉｎｇ　ｕｎｄｅｒ　ｆｉｒｅ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　２９ｔｈ ＵＳＥＮＩＸ　Ｓｅｃｕｒｉｔｙ　Ｓｙｍｐ．Ｂｅｒｋｅｌｅｙ，ＣＡ：ＵＳＥＮＩＸ　Ａｓｓｏｃｉａｔｉｏｎ， ２０２０

［１５３］ Ｄａｂｋｏｗｓｋｉ　Ｐ，Ｇａｌ　Ｙ．Ｒｅａｌ　ｔｉｍｅ　ｉｍａｇｅ　ｓａｌｉｅｎｃｙ　ｆｏｒ　ｂｌａｃｋ　ｂｏｘ ｃｌａｓｓｉｆｉｅｒｓ ［Ｃ］??Ｐｒｏｃ　ｏｆ　ｔｈｅ　３１ｓｔ　Ｉｎｔ　Ｃｏｎｆ　ｏｎ　Ｎｅｕｒａｌ Ｉｎｆｏｒｍａｔｉｏｎ　Ｐｒｏｃｅｓｓｉｎｇ　Ｓｙｓｔｅｍｓ．Ｒｅｄ　Ｈｏｏｋ，ＮＹ：Ｃｕｒｒａｎ Ａｓｓｏｃｉａｔｅｓ　Ｉｎｃ，２０１７：６９６７－６９７６
Ｊｉ　Ｓｈｏｕｌｉｎｇ，ｂｏｒｎ　ｉｎ　１９８６．Ｒｅｃｅｉｖｅｄ　ａ　ＰｈＤ ｉｎ　ｅｌｅｃｔｒｉｃａｌ　ａｎｄ　ｃｏｍｐｕｔｅｒ　ｅｎｇｉｎｅｅｒｉｎｇ　ｆｒｏｍ Ｇｅｏｒｇｉａ　Ｉｎｓｔｉｔｕｔｅ　ｏｆ　Ｔｅｃｈｎｏｌｏｇｙ　ａｎｄ　ａ　ＰｈＤ ｉｎ　ｃｏｍｐｕｔｅｒ　ｓｃｉｅｎｃｅ　ｆｒｏｍ　Ｇｅｏｒｇｉａ　Ｓｔａｔｅ Ｕｎｉｖｅｒｓｉｔｙ． Ｃｕｒｒｅｎｔｌｙ　ＺＪＵ　 １００－Ｙｏｕｎｇ Ｐｒｏｆｅｓｓｏｒ　ｉｎ　ｔｈｅ　Ｃｏｌｌｅｇｅ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ ａｎｄ　Ｔｅｃｈｎｏｌｏｇｙ　ａｔ　Ｚｈｅｊｉａｎｇ　Ｕｎｉｖｅｒｓｉｔｙ　ａｎｄ ｒｅｓｅａｒｃｈ　ｆａｃｕｌｔｙ　ｉｎ　ｔｈｅ　Ｓｃｈｏｏｌ　ｏｆ　Ｅｌｅｃｔｒｉｃａｌ ａｎｄ　Ｃｏｍｐｕｔｅｒ　Ｅｎｇｉｎｅｅｒｉｎｇ　ａｔ　Ｇｅｏｒｇｉａ Ｉｎｓｔｉｔｕｔｅ　ｏｆ　Ｔｅｃｈｎｏｌｏｇｙ．Ｍｅｍｂｅｒ　ｏｆ　ＩＥＥＥ ａｎｄ　ＡＣＭ．Ｈｉｓ　ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ ｉｎｃｌｕｄｅ　ｂｉｇ　ｄａｔａ　ｓｅｃｕｒｉｔｙ　ａｎｄ　ｐｒｉｖａｃｙ，ｂｉｇ ｄａｔａ　 ｄｒｉｖｅｎ　 ｄｅｃｕｒｉｔｙ　 ａｎｄ　 ｐｒｉｖａｃｙ， ａｄｖｅｒｓａｒｉａｌ　ｌｅａｒｎｉｎｇ， ｇｒａｐｈ　ｔｈｅｏｒｙ　ａｎｄ ａｌｇｏｒｉｔｈｍｓ，ａｎｄ　ｗｉｒｅｌｅｓｓ　ｎｅｔｗｏｒｋｓ．
Ｌｉ　Ｊｉｎｆｅｎｇ，ｂｏｒｎ　ｉｎ　１９９４．ＭＳｃ　ｃａｎｄｉｄａｔｅ　ｉｎ ｔｈｅ　Ｃｏｌｌｅｇｅ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ　ａｎｄ Ｔｅｃｈｎｏｌｏｇｙ　ａｔ　Ｚｈｅｊｉａｎｇ　Ｕｎｉｖｅｒｓｉｔｙ．Ｈｉｓ ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ　ｂｉｇ　ｄａｔａ ｄｒｉｖｅｎ　ｓｅｃｕｒｉｔｙ， ＡＩ　ｓｅｃｕｒｉｔｙ　ａｎｄ　ｄｅｅｐ ｌｅａｒｎｉｎｇ　ｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ．
Ｄｕ　Ｔｉａｎｙｕ，ｂｏｒｎ　ｉｎ　１９９６．ＰｈＤ　ｃａｎｄｉｄａｔｅ　ｉｎ ｔｈｅ　Ｃｏｌｌｅｇｅ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ　ａｎｄ Ｔｅｃｈｎｏｌｏｇｙ　ａｔ　Ｚｈｅｊｉａｎｇ　Ｕｎｉｖｅｒｓｉｔｙ．Ｈｅｒ ｍａｉｎ　ｒｅｓｅａｒｃｈ　ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ　ｂｉｇ　ｄａｔａ ｄｒｉｖｅｎ　ｓｅｃｕｒｉｔｙ，ａｄｖｅｒｓａｒｉａｌ　ｌｅａｒｎｉｎｇ　ａｎｄ ＡＩ　ｓｅｃｕｒｉｔｙ．
Ｌｉ　Ｂｏ，ｂｏｒｎ　ｉｎ　１９８９．Ａｓｓｉｓｔａｎｔ　ｐｒｏｆｅｓｓｏｒ　ｉｎ ｔｈｅ　Ｄｅｐａｒｔｍｅｎｔ　ｏｆ　Ｃｏｍｐｕｔｅｒ　Ｓｃｉｅｎｃｅ　ａｔ Ｕｎｉｖｅｒｓｉｔｙ　 ｏｆ　 Ｉｌｌｉｎｏｉｓ　 ａｔ　 Ｕｒｂａｎａ－ Ｃｈａｍｐａｉｇｎ，ａｎｄ　ｒｅｃｉｐｉｅｎｔ　ｏｆ　ｔｈｅ　Ｓｙｍａｎｔｅｃ Ｒｅｓｅａｒｃｈ　Ｌａｂｓ　Ｆｅｌｌｏｗｓｈｉｐ． Ｍｅｍｂｅｒ　ｏｆ ＩＥＥＥ　ａｎｄ　ＡＣＭ． Ｈｅｒ　ｍａｉｎ　ｒｅｓｅａｒｃｈ ｉｎｔｅｒｅｓｔｓ　ｉｎｃｌｕｄｅ　ｔｈｅｏｒｅｔｉｃａｌ　ａｎｄ　ｐｒａｃｔｉｃａｌ ａｓｐｅｃｔｓ　ｏｆ　ｓｅｃｕｒｉｔｙ， ｍａｃｈｉｎｅ　ｌｅａｒｎｉｎｇ， ｐｒｉｖａｃｙ，ｇａｍｅ　ｔｈｅｏｒｙ，ａｎｄ　ｂｌｏｃｋｃｈａｉｎ．

