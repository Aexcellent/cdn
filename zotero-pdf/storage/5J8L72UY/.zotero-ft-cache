Towards Automated Machine Learning: Evaluation and Comparison of AutoML Approaches and Tools

Anh Truong∗, Austin Walters∗, Jeremy Goodsitt∗, Keegan Hines∗, C. Bayan Bruss∗, Reza Farivar∗†

∗Applied Research, Center for Machine Learning Capital One
McLean, VA, USA {anh.truong, austin.walters, jeremy.goodsitt keegan.hines, bayan.bruss}@capitalone.com

†Department of Computer Science University of Illinois
Urbana-Champaign, IL, USA farivar2@illinois.edu

arXiv:1908.05557v2 [cs.LG] 3 Sep 2019

Abstract—There has been considerable growth and interest in industrial applications of machine learning (ML) in recent years. ML engineers, as a consequence, are in high demand across the industry, yet improving the efﬁciency of ML engineers remains a fundamental challenge. Automated machine learning (AutoML) has emerged as a way to save time and effort on repetitive tasks in ML pipelines, such as data pre-processing, feature engineering, model selection, hyperparameter optimization, and prediction result analysis. In this paper, we investigate the current state of AutoML tools aiming to automate these tasks. We conduct various evaluations of the tools on many datasets, in different data segments, to examine their performance, and compare their advantages and disadvantages on different test cases.
Index Terms—AutoML, automated machine learning, driverless AI, model selection, hyperparameter optimization
I. INTRODUCTION
Automated Machine Learning (AutoML) promises major productivity boosts for data scientists, ML engineers and ML researchers by reducing repetitive tasks in machine learning pipelines. There are currently a number of different tools and platforms (both open-source and commercially available solutions) that try to automate these tasks. The goal of this paper is to address the following questions: (i) what are the available ML functionalities provided by the tools; (ii) how the tools perform when facing a wide spectrum of real world datasets; (iii) ﬁnd the trade-off between optimization speed and accuracy of the results; and (iv) the reproducibility of the results (a.k.a. tool robustness).
The rest of the paper is organized as follows. Section II covers the history and background of AutoML tools. Next, in Section III we compare the tools’ features and functionalities on an automated ML pipeline including data preprocessing, model selection, hyperparameter optimization, and model interpretation. After that, in Section IV we experimentally evaluate the performance of a selected subset of these tools on a large variety of datasets and a range of supervised ML tasks. Finally, we conclude the paper in Section V.
II. BACKGROUND AND HISTORY
Between 1995 to 2015 many ML libraries and tools were developed, spanning from Weka (1990s), RapidMiner (2001),

Scikit-learn (2007-2010), H2O (2011), and Spark MLlib (2013) among many others. Deep Neural Network platforms have also gained popularity in the last 5 years. Tensorﬂow (2015), Keras (2015) and MXNet (2015) contributed to the wide adoption of deep learning models.
During this time period it became evident to many ML practitioners that extracting the best performance from machine learning models requires substantial human expertise. Developing good models from a dataset is almost an art form involving intuition, experience, and many tedious manual tasks to tune algorithmic parameters. The combination of market pressure for more ML engineers, and the tedious nature of developing ‘optimal’ ML solutions sparked the idea of automating the ML tasks.
AutoML’s initial effort came out of academia and ML practitioners ﬁrst, and later startups. One of the ﬁrst attempts was Auto-Weka (2013) [1] from Universities of British Columbia (UBC) and Freiburg, which utilizes algorithms provided by Weka [2]. Auto-sklearn (2014) [3] came next from the University of Freiburg. TPOT [4] was developed at the University of Pennsylvania (2015). Auto-ml [5], an open-source python package, was released in 2016 (to avoid confusion with the general term ‘AutoML’, please note the spelling for this tool). Auto-sklearn, Auto-ml, and TPOT are all built on the well-known ‘scikit-learn’ ML package. Other tools followed, including Auto-Keras (2017) [6] from the Texas A&M University running on top of Keras, Tensorﬂow and Scikit-learn. MLjar (2018) [7] also uses Scikit-learn, in conjunction with Tensorﬂow. On the same timeframe, some startups started developing their tools for AutoML. Datarobot [8], [9], [10] launched its automated machine learning tool in 2015. H2O-Automl [11], [12] was introduced by the H2O (2016), using ML models from the H2O platform. The H2O team later released their commercialized H2O-DriverlessAI product (2017) [13], and SparkCognition introduced Darwin (2018) [14] utilizing their own ML platform.
After a while, the large cloud providers and technology companies followed suit, offering Automated Machine Learning as a Service (AMLaaS) or standalone products. Google Cloud Automl (2017) [15] runs on Google Cloud platform.

Microsoft AzureML (2018) [16] takes advantage of algorithms on Azure, and Salesforce’s TransmogrifAI (2018) [17] runs on top of Spark ML, and Uber’s Ludwig (2019) [18] runs model training on Horovod, Uber’s open-source distributed training framework.
The aforementioned platforms emphasize different aspects of the AutoML space. For example, Darwin, H2ODriverlessAI and DataRobot provide the functionality of detecting and processing time-series data. They also offer interactive UI to help customers experiment quickly with different machine learning tasks. H2O-DriverlessAI exports a Plain Old Java Object (POJO) or a Model ObJect Optimized (MOJO) for the optimized models to be easily deployed in any Javasupported platform. TPOT exports optimized code for developers. Auto-ml offers ‘categorical ensembling’, where segments of categories in a column can have different models. Google Cloud AutoML and Auto-keras conduct neural network search [19], [20], for both image and text data.
III. AUTOML PLATFORMS’ FEATURES AND FUNCTIONALITY COMPARISON: THE COMMON PIPELINE
Fig. 1. The common AutoML pipeline.
Most AutoML tools follow a common three stage pipeline illustrated in Figure 1. In general, these three components are optimized iteratively to obtain the best outcome. Figure 2 brieﬂy summarizes the comparison across the surveyed tools. More detailed comparisons follow in the subsequent sections.
A. Data Preprocessing and Feature Engineering
Data preprocessing is typically the ﬁrst task in ML pipelines. At the moment, this task is not handled very well by any of the AutoML tools and still requires considerable human intervention. In particular, this task requires data type and schema detection which have not been widely supported among the AutoML tools. However, once data types are identiﬁed, the tools provide appropriate feature engineering for the next component in the pipeline. TransmogrifAI seems to be further ahead in this regard by supporting different detailed data types detection (e.g., addresses, phone numbers, names, currency, etc), however this functionality appears to not be very stable on multiple datasets. H2O-Automl, H2ODriverlessAI, DataRobot, MLjar and Darwin gain some advantage by offering the ability to detect basic data types or schemas, currently limited to numerical, categorical and timeseries data. Auto-ml, Auto-sklearn, AzureML and Ludwig are less favorable here, in the sense that they can only do feature engineering from user-input speciﬁcations, e.g. data types for each column. The other tools need much more human interaction on feature engineering. Auto-sklearn requires users

input to convert categorical data into integers (e.g., using label encoder) before any other transformation. TPOT and Autokeras do not provide either data preprocessing or feature generation steps and instead require users to manually perform data pre-processing, and only accept numerical feature matrices.
B. Model Selection, Hyperparameter Optimization, and Architecture Search
In this step, the extracted features from the previous step are used to train many different types of models, each with many different sets of parameters (hyperparameter optimization), then the best model (or an ensemble of models) is selected as the ﬁnal model. Each tool supports a collection of existing machine learning algorithms to build models. They include, but not limited to, Logistic Regression, tree-based algorithms, SVM, and neural network models. H2O-Automl, Ludwig, DataRobot, Darwin, Auto-ml, Auto-sklearn, MLjar, TransmogrifAI, and TPOT all work in this fashion for supervised methods. DataRobot, H2O-DriverlessAI and Darwin provide additional unsupervised methods such as clustering and outlier detection. TPOT and Darwin also utilize genetic algorithms to iteratively select the best traits of each model and pass them to the next generation. Google Cloud AutoML and Auto-keras work differently, utilizing neural architecture search to select the best neural network model.
For hyperparameter optimization, some of the most popular methods are grid search, random search, and Bayesian search. Auto-Weka uses SMAC (Sequential Model-based Algorithm Conﬁguration, [21]) while Auto-sklearn utilizes SMAC3, a re-implementation of SMAC to efﬁciently perform Bayesian optimization. H2O-Automl and MLjar apply random search on the parameter spaces, while H2O-DriverlessAI, Ludwig, Auto-ml, TransmogrifAI and Auto-keras use both random and Bayesian search.
In order to reduce time for model search and hyperparameter optimization, it is common to prune the parameter space. In the ﬁrst approach, the tools attempt to quickly ﬁnd an initial parameter set. Auto-sklearn and Darwin use pre-processed ‘meta-features’ from previously trained datasets, each with a known ‘meta-learner’. Given a target dataset, they ﬁnd a similar dataset based on ‘meta-feature’, and use the closest ’meta-learners’ as the initial model. The second approach is to use the relationship between model selection and hyperparameter optimization. H2O-Automl uses the combination of random grid search with stacked ensembles, as diversiﬁed models improve the accuracy of ensemble method. The third approach is to ﬁx an allowed runtime for the tools to search for a best model. All AutoML tools, except Auto-ml, currently offer this option. The fourth approach (only applies for H2OAutoml and Auto-sklearn) is to restrict the parameters that cause a slow optimization. For example, non-linear feature approximation combined with KNN models is restricted as it dramatically slows down the optimization.

Fig. 2. Comparison table of functionality for AutoML tools. (+): commercialized tools; (∗): the function is not very stable, it fails for some datasets; (2∗): categorical input must be converted into integers; (3∗): datasets have to include headers; (4∗): missing values must be represented as NA; (5∗): multiclass classiﬁcation not provided; (6∗): need some users’ input for dataset description such as column types; (7∗): ability to detect primitive data types and rich data types such as: text (id, url, phone), numerical (integer, real); (8∗): advanced feature processing: bucketing of values, removing features with zero variance or features with drift over time; (9∗): supervised learning includes binary classiﬁcation, multiclass classiﬁcation, regression; (10∗): unsupervised learning includes clustering and anomaly detection; (11∗): model interpretation and explainability refers to techniques such as LIME, Shapley, Decision Tree Surrogate,
Partial Dependence, Individual Conditional Expectation, Lift chart, feature ﬁt, prediction distribution plot, accuracy over time, hot spot and reason codes; (12∗): conﬁrmed by a company spokesperson, we could not ﬁnd public documentation at the time of publication; In a few empty cells, it is not clear that
the functionality is provided from documentations of the tools, to the best of our knowledge.

C. Model Interpretation and Prediction Analysis
This component is currently applied to most commercialized tools such as H2O-DriverlessAI, DataRobot and Darwin whereas it is not the concentration of non-commercialized tools. In essence, it provides detailed result representation through model dashboards, feature importance and different visualization methods, e.g., lift chart and prediction distribution. Those tools even highlight outlier data points that the best model was not conﬁdent in predictions, and support ‘reason code’, LIME, Shapley, and partial dependence, etc., for better model interpretation.
IV. EXPERIMENTAL EVALUATION
We evaluate a selected subset 1 of AutoML tools on nearly 300 datasets collected from Openml [22], which allows users to query data for different use cases. Detailed descriptions on the datasets are given on the Table I in the Appendix. The two advantages of using Openml datasets are: (i) the
1Due to the unavailability of the licensed or trial versions, we have not evaluated most commercialized tools. Some other open-sourced tools have not been evaluated due to the lack of widely supported Python wrappers.

Fig. 3. Data segments used for evaluation. Each cell is referred to as a ‘data segment’. For example, in the ﬁrst row, ’Less than one third’ stands for datasets with the categorical proportion less than 1/3.
datasets are already pre-processed into the numerical features 2, therefore the same data will be fed to all AutoML tools, minimizing the risk of bias from data selection process; and (ii) guarantee a fair comparison among the tools as some do not provide the pre-processing steps for raw datasets. In order to evaluate AutoML tools on a variety of dataset characteristics, we selected multiple datasets according to the criteria depicted in Figure 3. For the sake of clarity, each cell in this table is referred to as a ‘data segment’, each containing datasets with different sample sizes, feature dimensions, categorical
2Although there are still a few datasets containing text or non-numerical features, those are not included in this paper.

Fig. 4. Evaluation of AutoML tools on binary classiﬁcation task across ten data segments (depicted in Figure 3). Each diagram refers to a data segment. All experiments are run up to 15 minutes. Some experiments are completed faster but in some other cases, several tools cannot obtain results after that time limit. Speciﬁcally, the percentage of experiments that did not ﬁnish in 15 minutes are: Ludwig 4%, H2O-Automl 8%, TPOT 13%, Darwin 26%, Auto-sklearn 30%

features ratio (deﬁned as the ratio of number of categorical features over total number of features), missing proportion (proportion of samples with at least one missing feature), and class imbalance (samples in minority class vs. in majority class). Each dataset is divided into two parts, one for training and another for testing with the ratio 4 : 1. All AutoML tools are applied to the same training and testing proportions of all datasets. For all evaluations, the following tools and associated versions are used: Darwin 1.6, Auto-sklearn 0.5.2, Auto-keras 0.4.0, Auto-ml 2.9.10, Ludwig 0.1.2, H2O-Automl 3.24.0.5, TPOT 0.10.1.
In the next subsections, we will evaluate AutoML tools on different test cases, each with three different supervised learning tasks: binary classiﬁcation, multiclass classiﬁcation, and regression. All experiments are run on Amazon EC2 p2.xlarge instances, which provide 1 Tesla K80 GPU, 4 vCPUs (Intel Xeon E5-2686, 2.30Ghz) and 61 GiB of host memory.
Setting a time-limit for all experiments is not straightforward. On the one hand, we would like to let each tool run as long as it takes to produce the best results. On the other hand, with 3 ML tasks, 300 datasets and 6 tools, we have 5,400 experiments to run. To keep the experiment run-time and cost to practical limits, we aim for a ‘completion target’ of 70%, i.e., we select a run-time for which all tools are able to ﬁnish the AutoML tasks for 70% of the datasets. All the

AutoML tools proved to be capable of hitting the 70% target within 15 minutes for binary classiﬁcation. 5 out of the 6 tools (all but Darwin) hit 70% target for regression, and 4 out of 6 tools hit the 70% target in multiclass classiﬁcation, (TPOT nearly reaches the target, and Darwin misses the target again). As Darwin appears to be slow in convergence, and to be fair to the other tools, it is excluded from our completion target analysis. We therefore decide to run all our extensive experiments (5,400) for 15 minutes time-limits, for a total of 1,350-hour EC2 run-time (which includes the overhead of benchmark harness code), where the results are detailed in Section IV-A. We then run another experiment with a randomized subset of our datasets for longer time limits to evaluate the performance of the tools when more time is given to ﬁnish. The results of this latter experiment (3 tasks, 5 data segments, 6 run-time periods, 7 tools, for a total of 717 EC2 hours including benchmark harness overhead) are detailed in Section IV-B. Note that the Auto-ml tool was not included in the extensive experiments as it does not offer an option to limit its run-time from a user-input value (15 minutes in our case), it simply can only run to completion. As such, its results are only included among the experiments in Section IV-B.

Fig. 5. Evaluation of AutoML tools on multiclass classiﬁcation task across ten data segments (depicted in Figure 3). Each diagram refers to a data segment. All experiments are run up to 15 minutes. Some experiments are completed faster but in some other cases, several tools cannot obtain results after that time limit. Speciﬁcally, the percentage of experiments that did not ﬁnish in 15 minutes are: Auto-keras 2%, H2O-Automl 21%, Ludwig 24%, Auto-sklearn 30%, TPOT 35%, Darwin 51%.

A. Evaluation on multiple data segments
In this section, we investigate the performance of the tools across many datasets and applications (please see Table I in the Appendix for the detailed descriptions on the datasets). To that end, the evaluated data is divided into ten segments (as shown in Figure 3), each including ten random datasets. ‘Accuracy’ is the comparison metric used for binary and multiclass classiﬁcation tasks and ‘Mean Squared Error (MSE)’ is used for regression tasks.
Figure 4 shows the performance of AutoML tools for binary classiﬁcation task in different data segments. In this Figure, the performance is represented in the box-whisker format, where each box shows the median, and the ﬁrst and third quartiles of the performance at the two ends. Note that, for the data segment with class imbalance (third row in Figure 4), F1score is used instead of the regular accuracy as it is a more appropriate metric for imbalanced data.
It can be observed from Figure 4 that, the performance of AutoML tools ﬂuctuate more with a larger number of categorical features, and ﬂuctuate less with more data samples. This makes intuitive sense, as the tools will learn better with more data samples, and each tool has different approaches to encode categorical values that result in different performance. In addition, most tools suffer from the imbalanced datasets

except Ludwig and Darwin. Comparing the tools against each other, H2O-Automl and Darwin slightly outperform the rest, however it is worth reiterating that Darwin cannot deliver results for 26% of all datasets. Auto-sklearn and TPOT perform slightly worse than the aforementioned tools. Auto-keras does not perform as well as other tools for most datasets in binary classiﬁcation. As noted before, in this experiment, we limit the optimization time to 15 minutes. Here, TPOT manages to complete and deliver results within the 15-minutes time limit for 87% of datasets, while Darwin and Auto-sklearn suffer slightly higher non-delivering ratios of 26% and 30% respectively. Ludwig’s performance appears to ﬂuctuate the most compared to other tools.
The performance of the tools for muticlass classiﬁcation is illustrated in Figure 5. Here, minimal variation was found when evaluating between data segments of the same categories (the two graphs in each row). For this multiclass classiﬁcation task, Auto-keras and Auto-sklearn slightly outperform the rest, even though Auto-sklearn cannot deliver results within the time limit for 30% of datasets. TPOT comes next after these two tools. Finally, Ludwig, H2O-Automl and Darwin perform slightly worse than the rest.
Figure 6 shows the performance of the tools for regression task. The results from this task has similar trends to binary

Fig. 6. Evaluation of AutoML tools on regression task across ten data segments (depicted in Figure 3). Each diagram refers to a data segment. All experiments are run up to 15 minutes. Some experiments are completed faster but in some other cases, several tools cannot obtain results after that time limit. Speciﬁcally, the percentage of experiments that did not ﬁnish in 15 minutes are: Auto-keras 4%, H2O-Automl 11%, Auto-sklearn 20%, Ludwig 24%, TPOT 25%, Darwin 56%

classiﬁcation on categorical features. Furthermore, the performance variance tends to increase for all tools when the features dimensions decreases, or missing proportion increases. For this task, H2O-Automl and Auto-sklearn slight outperform Autokeras and TPOT while Darwin cannot deliver results on half of the datasets.
To summarize what we have seen from the three different ML tasks, Auto-keras does not perform as well as other tools for some datasets in binary classiﬁcation. In other words, whether Auto-keras can perform well or not (in binary classiﬁcation) depends signiﬁcantly on the nature of the dataset. For multiclass classiﬁcation, H2O-Automl performs slightly worse than the rest. For the regression task, Auto-keras, H2OAutoml and Auto-sklearn outperform the rest for most data segments (even though Auto-sklearn struggles somewhat more to complete results in the allotted 15 minutes, failing in 26% of datasets). TPOT performs slightly worse than those three tools, Ludwig’s performance varies across the datasets, and Darwin can only complete work on about half of the datasets in the allotted 15 minutes.
B. Evaluation on time limit
Our next targeted evaluation is to explore the impact of time limit in order to investigate how quickly the tools can deliver the results, and whether the tools can consistently guarantee

better results given more time availability. We performed various time-limit experiments for datasets with different sample sizes. Here, we randomly select a dataset given a sample size range (i.e., we pick a uniformly random dataset among all datasets in each sample size range) and evaluate each tools’ accuracy bounded by the time limits: 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours and 3 hours. Since the dataset sizes do not exceed one million samples, the maximum allotted time of 3 hours should allow the tools to converge. Figure 7 shows the results of this evaluation. As observed from the ﬁgure, most tools can generally improve the performance (increase the accuracy for classiﬁcation tasks and decrease the meansquared error for regression task) given more time for their optimization. Among the tools, H2O-Automl, Auto-keras and Ludwig converge to the optimal performance very quickly for most cases, roughly within 15 minutes. Auto-sklearn needs almost 2-3 hours to obtain reasonable results while TPOT converges slightly faster. Darwin appears ﬂuctuating its performance even with more time for optimization.
C. Evaluation on robustness
In this evaluation, we test the robustness of AutoML tools, i.e., whether the tools deliver similar results across multiple runs on the same input datasets. For each task, we select a random dataset with the sample size from 10000 to 50000

Fig. 7. Evaluation of AutoML on multiple time limits. The left (middle) subgraphs show the accuracy of tools for binary (multiclass) classiﬁcation. The right subgraphs show the mean squared error of tools for regression. From top-to-bottom: each row shows a random dataset in the increasing order of the sample size, from 1000 to 100000. Note: In the left graph of the third row, all tools except Darwin obtain the same performance although the graph displays only the result for TPOT; in the graphs at the rows 3 & 4, column 3, all tools except Auto-keras and Auto-ml cannot deliver results due to the large number of features, roughly 62000 and 21000, respectively; in the second graph of third column, we omit the results of Ludwig as its error is roughly 100-times larger than the others.

Fig. 8. Evaluation of AutoML tools on robustness.

(this is a common sample size for many real-world datasets) and run each tool on it for ten different times, each times in 10 minutes. The results are illustrated in Figure 8. We observe that H2O-Automl and Ludwig obtain very stable performance across three different tasks. Darwin, Auto-keras and Auto-ml get slightly less stable performance than H2O-Automl. TPOT and Auto-sklearn are somewhat unstable in regression task. It is worth noting that even though Ludwig’s performance is very stable, it deviates largely from others.

V. CONCLUSIONS AND FUTURE WORK
In this paper, we have evaluated AutoML tools on their capabilities in the common machine learning pipeline. At the current state, different tools have different approaches for model selection and hyperparameter optimization. Commercialized tools such as H2O-DriverlessAI, DataRobot and Darwin extend their offering functionality on the ﬁrst and the third component of the pipeline where they are able to detect the data schema, run feature engineering, and analyze the detailed results for interpretation purpose. In contrast, open

source tools focus more on the second task in the pipeline, which is training and selecting the best model itself.
In addition, we have evaluated tools across many datasets on different data segments. We observed that most AutoML tools obtain reasonable results in terms of their performance across many datasets. However, there is no perfect tool at the current state yet, no tool managed to outperform all others on a plurality of tasks. Across the various evaluations and benchmarks we have tested, H2O-Automl, Auto-keras and Auto-sklearn performed better than Ludwig, Darwin, TPOT and Auto-ml. In particular, H2O-Automl slightly outperforms the rest for binary classiﬁcation and regression, and quickly converges to the optimal results. However, it suffers from low performance in multiclass classiﬁcation. Auto-keras is very stable across all tasks and performs slightly better than the rest for multiclass classiﬁcation and ties with H2O-Automl for regression, but suffers from low performance in binary classiﬁcation. For a production environment where the computation speed and performance stability are key requirements, these two tools might be good candidates depending on applications and machine learning tasks. Auto-sklearn ties with H2OAutoml and Auto-keras for all tasks but it is comparatively slower than the other two and usually requires longer run time. Other tools such as Ludwig, Darwin, TPOT and Automl showed more varying results depending on the dataset and task.
Ultimately there is no one AutoML tool at this point that can clearly outperform every other tool. We are at an early juncture for Automated Machine Learning, and there are many innovations announced at a rapid pace. We believe as the tools mature and borrow ideas from each other, they will gain more strength in their core task. We also observed a gap in the AutoML tools’ support for the ﬁrst and third stages of the AutoML pipeline, and expect major developments to happen in those areas in near future.
Disclaimer: For commercialized tools, our analyses and descriptions are consistent with our understanding derived from publicly available documentation and product descriptions. In some of these cases, we are unable to explore source code and regret any factual errors that may arise.
A subsidiary of Capital One - Capital One Ventures - is an investor in H2O.ai. During the course of our research we were neither in contact with H2O.ai nor Capital One Ventures.

[4] R. S. Olson, N. Bartley, R. J. Urbanowicz, and J. H. Moore, “Evaluation of a tree-based pipeline optimization tool for automating data science,” in Proceedings of the Genetic and Evolutionary Computation Conference (GECCO) 2016. New York, NY, USA: ACM, 2016, pp. 485–492.
[5] “Auto-ml: Automated machine learning for production and analytics,” https://github.com/ClimbsRocks/$auto ml$, accessed: 2019-04-10.
[6] H. Jin, Q. Song, and X. Hu, “Auto-keras: An efﬁcient neural architecture search system,” in arXiv, 2018.
[7] “Mljar,” https://github.com/mljar/mljar-api-python, accessed: 2019-0410.
[8] “Datarobot usage examples,” https://github.com/datarobot/ datarobot-sagemaker-examples, accessed: 2019-04-10.
[9] “Datarobot documentation,” https://www.datarobot.com/about-us/, accessed: 2019-04-10.
[10] “Datarobot python client,” https://datarobot-public-api-client. readthedocs-hosted.com/en/v2.11.0/setup/getting{ }started.html, accessed: 2019-04-10.
[11] “H2o.ai automl github,” https://github.com/h2oai/h2o-3, accessed: 201904-10.
[12] “H2o.ai automl documentation,” http://docs.h2o.ai/h2o/latest-stable/ h2o-docs/automl.html, accessed: 2019-04-10.
[13] “H2o-driverlessai,” http://docs.h2o.ai/driverless-ai/latest-stable/docs/ userguide/index.html, accessed: 2019-04-10.
[14] “Darwin-sparkcognition,” https://github.com/sparkcognition/darwin-sdk, accessed: 2019-04-10.
[15] “Google cloud automl,” https://cloud.google.com/automl/, accessed: 2019-04-10.
[16] “Automated machine learning with azureml,” https://github.com/ Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/ automated-machine-learning, accessed: 2019-04-10.
[17] “Transmogrifai,” https://github.com/salesforce/TransmogrifAI, accessed: 2019-04-10.
[18] “Ludwig,” https://github.com/uber/ludwig, accessed: 2019-04-10. [19] B. Zoph and Q. Le, “Neural architecture search with reinforcement
learning,” in arXiv, Nov. 2016. [20] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efﬁcient neural
architecture search via parameters sharing,” in Proceedings of the 35th International Conference on Machine Learning, vol. 80, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018, pp. 4095–4104. [21] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Sequential model-based optimization for general algorithm conﬁguration,” in Proceedings of the 5th International Conference on Learning and Intelligent Optimization, ser. LION’05, 2011, pp. 507–523. [22] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, “Openml: Networked science in machine learning,” ACM SIGKDD Explorations Newsletter, vol. 15, pp. 49–60, Jun. 2014.
APPENDIX

REFERENCES
[1] L. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and K. LeytonBrown, “Auto-weka 2.0: Automatic model selection and hyperparameter optimization in weka,” The Journal of Machine Learning Research, vol. 18, no. 1, pp. 826–830, Jan. 2017.
[2] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, “The weka data mining software: An update,” ACM SIGKDD Explorations Newsletter, vol. 11, no. 1, pp. 10–18, 2009.
[3] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter, “Efﬁcient and robust automated machine learning,” in Advances in Neural Information Processing Systems 28, 2015, pp. 2962– 2970.

TABLE I DATASET DESCRIPTIONS.

Id 954 23499 862 905 724 40978 983 40649 41156 40648 959 881 977 734 357 1111 1112 41150 1212 1369 875 472 461 796 37 1590 1059 1442 917 806 742 1116 1563 40910 357 1157 1123 4134 1164 41159 734 41142 1448 1021 868 990 749 162 874 1130 950 804 43 963 40536 992 960 72 1010 40680 40647 804 1462 714 719 726 933 808 904 40994 55 56 172 757 802 1114 1000 470 38 51 40713 1142 951 1018 40910 1056 977 1001 450 1045 803 920 739 1486 1167 1125 787 481 935 763

Binary classiﬁcation Name
spectrometer breast-cancer-dropped-missing-attributes-values
sleuth-ex2016 chscase-adopt analcatdata-vineyard Internet-Advertisements
cmc GAMETES-Heterogeneity-20atts-1600-Het-0.4-0.2-50-EDM-2-001
ada GAMETES-Epistasis-3-Way-20atts-0.2H-EDM-1-1
nursery mv letter
ailerons vehicle-sensIT KDDCup09-appetency KDDCup09-churn MiniBooNE BNG(SPECTF) ” BNG(kr-vs-kp, 1000, 1)” analcatdata-chlamydia
lupus analcatdata-creditscore
cpu diabetes
adult ar1 MegaWatt1 fri-c1-1000-25 fri-c3-1000-50 fri-c4-500-100 musk dbworld-subjects-stemmed Speech vehicle-sensIT AP-Endometrium-Kidney AP-Endometrium-Breast Bioresponse AP-Endometrium-Uterus guillermo ailerons christine KnuggetChase3 page-blocks fri-c4-100-25 eucalyptus fri-c3-500-5 SEA(50000) rabe-131 OVA-Lung arsenic-female-lung hutsof99-logis haberman heart-h SpeedDating analcatdata-broadway postoperative-patient-data BNG(kr-vs-kp) dermatology mofn-3-7-10 GAMETES-Epistasis-2-Way-20atts-0.4H-EDM-1-1 hutsof99-logis banknote-authentication fruitﬂy veteran fri-c2-100-5 fri-c4-250-25 fri-c0-100-10 fri-c0-1000-50 climate-model-simulation-crashes hepatitis vote shuttle-landing-control meta pbcseq KDDCup09-upselling hypothyroid profb sick heart-h dis OVA-Endometrium arsenic-male-lung ipums-la-99-small Speech mc1 letter sponge analcatdata-lawsuit kc1-top5 delta-ailerons fri-c2-500-50 sleep nomao pc1-req AP-Omentum-Prostate witmer-census-1980 biomed fri-c1-250-10 fri-c0-250-10

Id 342 385 48 1565 1516 1535 40708 40476 181 28 1044 41163 40926 1536 119 255 554 23397 148 157 455 1413 1528 11 1499 41169 4535 188 453 1361 40670 41082 313 1548 41166 1082 1078 41163 391 1514 1086 40996 1544 477 377 1539 1087 1523 1117 277 41000 155 380 457 379 148 184 4541 1363 130 400 1520 1113 1404 1356 186 74 1177 375 401 171 327 328 40966
2 5 7 57 378 381 71 1086 388 39 279 183 40498 1402 30 1541 1554 61 1548 40996 46 6 1493 1385 1041 147

Multiclass classﬁcation Name
squash-unstored tr31.wc tae heart-h
robot-failures-lp1 volcanoes-b5 allrep thyroid-allhypo yeast optdigits eye-movements dilbert
CIFAR-10-small volcanoes-b6
” BNG(cmc, nominal, 55296)” BNG(cmc) mnist-784
COMET-MC-SAMPLE ” BNG(zoo, nominal, 1000000)”
RandomRBF-10-1E-3 cars
MyIris volcanoes-a2 balance-scale
seeds helena Census-Income eucalyptus analcatdata-bondrate ” BNG(anneal.ORIG, 1000, 5)”
dna USPS spectrometer autoUniv-au4-2500 volkert rsctc2010-6 rsctc2010-2 dilbert re0.wc micro-mass ovarianTumour Fashion-MNIST volcanoes-e3 ﬂ2000 synthetic-control volcanoes-d2 hepatitisC vertebra-column desharnais meta-ensembles.arff jungle-chess-2pcs-endgame-panther-elephant pokerhand SyskillWebert-Bands prnn-cushings SyskillWebert-Goats ” BNG(zoo, nominal, 1000000)” kropt Diabetes130US ” BNG(anneal.ORIG, 5000, 1)” BNG(segment) tr41.wc robot-failures-lp5 KDDCup99 ” BNG(lymph, 1000, 10)” ” BNG(anneal, 5000, 10)” braziltourism ” BNG(letter, nominal, 1000000)” BNG(primary-tumor) JapaneseVowels oh10.wc primary-tumor bridges bridges MiceProtein anneal arrhythmia audiology hypothyroid ipums-la-99-small ipums-la-98-small ” BNG(anneal.ORIG, nominal, 1000000)” ovarianTumour tr23.wc ecoli meta-stream-intervals.arff abalone wine-quality-white ” BNG(lymph, 1000, 1)” page-blocks volcanoes-d4 autoUniv-au7-500
iris autoUniv-au4-2500
Fashion-MNIST splice letter
one-hundred-plants-texture ” BNG(letter, 10000, 5)”
gina-prior2 ” BNG(waveform-5000, nominal, 1000000)”

Id 3584 3536 3682 4096 4057 197 3394 573 1028 558 1594 1583 564 344 1578 1588 1591 273 1191 1192 533 556 689 551 678 4553 287 1191 608 1572 1586 315 422 1584 412 3626 3363 3574 3586 3955 3266 3836 40753 3707 4050 620 3230 223 3298 1093 200 534 665 515 553
31 203 213 224 301 3915 3668 3925 3760 193 1432 3833 619 3625 3522 200 222 232 516 524 566 231 315 1072 41021 383 384 385 387 388 395 397 398 400 40596 31 273 300 386 390 392 393 394 396 399

Regression Name
QSAR-TID-12665 QSAR-TID-12868 QSAR-TID-100790 QSAR-TID-30028 QSAR-TID-10547
cpu-act QSAR-TID-20154
cpu-act SWD bank32nh news20 w3a fried mv real-sim w8a connect-4 IMDB.drama BNG(pbc) BNG(autoHorse) arsenic-female-bladder analcatdata-apnea2 chscase-vine2 analcatdata-michiganacc visualizing-environmental TurkiyeStudentEvaluation wine-quality BNG(pbc) fri-c3-1000-10 german.numer w6a us-crime topo-2-1 w4a Phen QSAR-TID-103437 QSAR-TID-19905 QSAR-TID-10958 QSAR-TID-104390 QSAR-TID-12406 QSAR-TID-11169 QSAR-TID-142 delays-zurich-transport QSAR-TID-25 QSAR-TID-218 fri-c1-1000-25 QSAR-TID-10930 stock QSAR-TID-12163 Brainsize pbc cps-85-wages sleuth-case2002 baseball-team kidney credit-g lowbwt pharynx breastTumor ozone-level QSAR-TID-11109 QSAR-TID-100071 QSAR-TID-101090 QSAR-TID-117 bolts colon-cancer QSAR-TID-12898 fri-c4-250-50 QSAR-TID-11000 QSAR-TID-100629 pbc echoMonths ﬁshcatch pbcseq pbc meta hungarian us-crime qqdefects-numeric Moneyball tr45.wc tr21.wc tr31.wc tr11.wc tr23.wc re1.wc tr12.wc wap.wc tr41.wc slashdot credit-g IMDB.drama isolet oh15.wc new3s.wc oh0.wc la2s.wc oh5.wc la1s.wc ohscal.wc

