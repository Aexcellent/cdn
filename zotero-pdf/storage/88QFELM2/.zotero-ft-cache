arXiv:2212.00622v2 [cs.LG] 9 Apr 2023

Vertical Federated Learning: A Structured Literature Review
Afsana Khan1*, Marijn ten Thij1 and Anna Wilbik1
1*Department of Advanced Computing Sciences, Maastricht University, Paul-Henri Spaaklaan, Maastricht, 6229, The Netherlands.
*Corresponding author(s). E-mail(s): a.khan@maastrichtuniversity.nl;
Contributing authors: m.tenthij@maastrichtuniversity.nl; a.wilbik@maastrichtuniversity.nl;
Abstract Federated Learning (FL) has emerged as a promising distributed learning paradigm with an added advantage of data privacy. With the growing interest in having collaboration among data owners, FL has gained signiﬁcant attention of organizations. The idea of FL is to enable collaborating participants train machine learning (ML) models on decentralized data without breaching privacy. In simpler words, federated learning is the approach of “bringing the model to the data, instead of bringing the data to the model”. Federated learning, when applied to data which is partitioned vertically across participants, is able to build a complete ML model by combining local models trained only using the data with distinct features at the local sites. This architecture of FL is referred to as vertical federated learning (VFL), which diﬀers from the conventional FL on horizontally partitioned data. As VFL is diﬀerent from conventional FL, it comes with its own issues and challenges. In this paper, we present a structured literature review discussing the state-of-the-art approaches in VFL. Additionally, the literature review highlights the existing solutions to challenges in VFL and provides potential research directions in this domain.
Keywords: Federated Learning, Vertically Partitioned Data, Privacy-Preserving Machine Learning, Literature Review
1

2 Vertical Federated Learning: A Structured Literature Review
1 Introduction
The use of machine learning (ML) has enabled organizations to more quickly identify potentially proﬁtable opportunities as well as risks that may be involved. As more and more data becomes accessible over time, there has been a corresponding rise in interest in the application of machine learning across a variety of ﬁelds. For instance, in healthcare, the use of machine learning to analyze the health data generated by the growing number of wearable devices like smart watches and ﬁt bits is gaining momentum (Bhardwaj et al, 2017). Moreover, the growing use of ML in ﬁnancial systems has transformed industries and societies. From traditional hedge fund management ﬁrms to FinTech service providers, many ﬁnancial ﬁrms are investing in data science and ML expertise (Goodell et al, 2021). ML has also made a signiﬁcant contribution to the agriculture sector by creating new opportunities to unravel, quantify, and understand data intensive processes in agricultural operational environments (Liakos et al, 2018).
While organizations can beneﬁt from applying machine learning techniques to their own data, doing the same with data from other comparable organizations could result in signiﬁcant improvements to the existing organizational processes. In order to build sophisticated machine learning models for improving consumer service and acquisition, substantial emphasis has been placed on integrating data from various organizations, indicating the importance of collaboration. However, the traditional approach of bringing data located at diﬀerent sites into a central server for training machine learning models is not always feasible as it raises numerous concerns. At present, sharing data among organizations has become critical due to concerns about privacy, maintaining competitive advantages, and/or other constraints. Data security and privacy are issues that are being prioritized not just by individuals or organizations but also by the larger society. The General Data Protection Regulations (GDPR), which the European Union put into place on May 25, 2018 (Albrecht, 2016) aims to protect users’ personal privacy and data security. To address this issue, federated learning (FL), a new distributed learning paradigm, has recently received a lot of attention. FL allows collaboration among organizations to train machine learning models while ensuring that private data of these organizations are not disclosed (Ma et al, 2020). Kairouz et al (2021) formally deﬁned federated learning as
“A machine learning setting where multiple entities (clients) collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each client’s raw data is stored locally and not exchanged or transferred; instead focused updates intended for immediate aggregation are used to achieve the learning objective”
Depending on how the data is partitioned or distributed among organizations, FL can be classiﬁed into three scenarios; Horizontal, Vertical and Hybrid. Horizontal federated learning (HFL) is suitable in scenarios when organizations

Vertical Federated Learning: A Structured Literature Review 3

(a) Horizontal Federated Learning,

(b) Vertical Federated Learning,

(c) Hybrid Federated Learning,
Fig. 1: Types of Federated Learning
share the same attribute space but diﬀer in samples (Figure 1a). An example of HFL is a group of hospitals collaborating to build a ML model used to predict health risks for their patients, based on agreed-upon data. However, HFL sometimes has limited applications in practical scenarios, for example, in fostering collaboration among organizations with competing interests. Due to business reasons, it is more likely that organizations will not be willing to collaborate with their competitors (Cheng et al, 2020). On the other hand, vertical federated learning (VFL) is suitable for scenarios where organizations have the same set of samples as their data but diﬀer in feature space (Figure 1b). VFL promotes collaboration among non-competing organizations with vertically partitioned data. In such cases, typically one organization has the ground truth, or labels, with some of the features of a number of samples. The rest of the participants take part in the federation by providing additional feature information of the same sample space but at the same time ensuring that their data is not disclosed directly to other participants. In return these participants can be compensated with monetary and/or reputational rewards. Examples where VFL is applicable could be a telecom company collaborating with a home entertainment company (cable TV provider) or an airline collaborating with a car rental agency. Hybrid FL refers to the hybrid situation of horizontally and vertically partitioned data (Figure 1c). In this scenario, the data owners hold diﬀerent attributes for diﬀerent data instances. However, hybrid FL is not yet explored signiﬁcantly in the literature.
Although VFL is a promising paradigm for privacy-preserving learning, limited research exists which explored the core challenges and methodologies of VFL due to the fact that, FL itself is a comparatively new concept. The pre-print articles (Li et al, 2023b; Liu et al, 2022b) provide a comprehensive overview of VFL, its taxonomy, threats, and prospects, applications in various

4 Vertical Federated Learning: A Structured Literature Review
domains and oﬀers insights into the potential beneﬁts and challenges of using this approach. However, to the best of our knowledge there is no published article yet that provides a structured review on the current research trends on VFL. Following a structured protocol for review ensures that the review is comprehensive, rigorous, and transparent, and that the results are easily accessible and understandable to the intended audience. Considering this as a motivation, in this article, we present a structured literature review (SLR) designed to summarize existing methodologies of VFL as well as to pinpoint potential future directions in order to address the challenges. Our goal for presenting this review is not diving in depth into particular implementations of VFL but rather providing direction for the researchers interested in VFL. The readers would be able to ﬁnd detailed information on VFL methods and techniques from the articles cited.
2 Methodology
The goal of the following structured literature review is to investigate major challenges and existing solutions for vertical federated learning. The study not only provides an overview of the major publications in VFL, but also to identify potential gaps and opportunities for further research. The review was planned, conducted, and reported in accordance with the SLR process proposed by Armitage and Keeble-Allen (2008). The SLR consists of ﬁve main steps including deﬁning research questions, designing search strategy, selecting studies, extracting data and ﬁnally synthesis of data. Below we explain these steps in more detail.
2.1 Research Questions
Taking into consideration the objective of the review, the following set of research questions were formulated as the initial stage of this SLR.
• RQ1: What are the existing methods in VFL and what problems do they tackle?
• RQ2: What are the existing applications of VFL? • RQ3: What are the potential future directions for research in VFL?
2.2 Search Strategy
After the formulation of the research questions, a plan was made to design the search strategy for the SLR. The search strategy includes the initial task of selecting literature databases. Kitchenham et al. listed several high-quality databases for searching research resources (Kitchenham, 2012). Since VFL is a trending research topic, there are many articles which are pre-prints. We chose to include both published (conference and journal papers) and pre-prints as a part of the SLR. The selected databases were Google Scholar, Web of Science (WoS), IEEE Xplore and arXiv. We experimented with diﬀerent search terms

Vertical Federated Learning: A Structured Literature Review 5

Search Terms
“Vertical federated learning” “Vertical” AND “Federated Learning” “Vertical” AND “privacy-preserving federated learning” “Vertical” AND “Heterogeneous Federated Learning” Total

Google Web of IEEE arXiv

Scholar Science Xplore

141 42

63

57

102 37

88

97

17

4

7

5

6

0

0

0

Table 1: Search Results

No. of Unique

Articles Articles

303

205

324

219

33

23

6

6

453

271

on the chosen databases. Finally, it was decided that the following search term would yield the most relevant results: (“Vertical federated learning”) OR (”Vertical” AND (“Federated Learning”
OR “privacy-preserving federated learning” OR “Heterogeneous federated learning”))
2.3 Study Selection
The results obtained using the deﬁned search strategy were ﬁltered based on a set of criteria. Only the articles which met the following criteria were considered further for ﬁrst round screening:
• Published after 2015 since the term ”Federated Learning” was ﬁrst coined in 2016
• Published until 15/03/2023 • Written in English language • Availability of full text • Title and abstract speciﬁcally mention the focus on vertical federated
learning
After the initial screening, the chosen articles were checked for redundancy which resulted in 122 unique articles. The unique articles were then investigated by going through the full text. Additionally, we used ”snowballing” (Streeton et al, 2004) technique to identify relevant articles. This was done by identifying relevant articles from the reference section of the previously selected articles. Use of the ”snowballing” technique lead to ﬁnding 14 additional relevant articles which were included for this review. The ﬁnal articles which are included in this SLR were selected based on the fact that they answered the following questions
• Did the article provide an answer to any of the research questions?

6 Vertical Federated Learning: A Structured Literature Review • Was the article focused on VFL and not general FL? • Was the method proposed in the article evaluated? • Were the experiments and results properly documented? In addition, survey papers were also considered where vertical federated learning had been addressed.
2.4 Data Extraction
We extracted data from each of the included papers and organized it in a manner such that we could provide an analysis of the reviewed literature and use them for synthesis in the next section. The data which were extracted from the articles were title & year of publication, source of publication, research question/problem solved, proposed method, availability of theoretical analysis, dataset evaluated on and model evaluated with.
2.5 Data Synthesis
As a last step of the literature review, we performed an analysis of 136 articles relevant to VFL and clustered them based on the problem those have addressed and solved. A detailed review of these articles have been provided in the further section.
Fig. 2: Overview of No. of Articles Found; Yearly Trend (Left Panel) & Databases(Right Panel)
3 Research Results
We conduct statistical analysis and present the results of a structured literature review by reading and analyzing articles related to FL that are found in the four major databases mentioned earlier. These results of the study provide answers to the research questions that were presented in Section 2.1.

Vertical Federated Learning: A Structured Literature Review 7

To understand the research trend of VFL, we conducted statistics for the publication year of literature as shown in Figure 2. Although the concept of federated learning was ﬁrst introduced in 2016, research on FL until 2018 primarily concentrated on horizontal federated learning. But from 2019 a signiﬁcant increase has been observed in the number of published articles focusing on VFL. Therefore, it can be concluded that VFL is still in its early stages of development.

Observing from the sources of publications, we found that more than 60% of the included papers in this literature review are published work in journals and conferences which implies that VFL has produced mature results. Emerging topics frequently have a substantial number of pre-prints available in databases like arXiV. Among the articles studied for this review, 36 percent were preprints and available on arXiv. This indicates the rapid development in VFL.

3.1 Vertical Federated Learning

In a vertical federated learning setting, the features of data points are dis-

tributed among diﬀerent partitions. There are two basic architectures for VFL;

with co-ordinator (Hardy et al, 2017) and without co-ordinator (Yang et al,

2019c). (Hardy et al, 2017) proposed a framework comprised of one trusted

coordinator and two parties, each of which represents a single client. The task

of the coordinator was to compute training loss as well as generate homomor-

phic encryption key pairs for privacy. Later on, some research works proposed

(Yang et al, 2019c; He et al, 2021; Sun et al, 2022a) proposed a two-party

architecture that eliminated the need for a trusted coordinator, thus reducing

the complexity of the system. This architecture was further extended by imple-

menting it in case of multiple collaborating parties/clients (Cheng et al, 2021;

Zhao et al, 2022). Figure 3 illustrates the basic protocol for VFL with multiple

parties where the party holding the labels is the active party/guest party and

rest others are passive parties/host parties. The active party is responsible for

computing training loss and generating key pairs to preserve privacy.

The vertical federated learning problem can be deﬁned in a more formal man-

ner. Let {(xi, yi), i = 1, 2, .., n} be a dataset where xi Rd and yi denotes the feature vector and output labels respectively. The feature dimension is rep-

resented by d. In case of a VFL setting, the dataset is partitioned vertically

across M parties/clients where each of the M parties possesses a disjoint subset

of features vector x[i,m](x[i,m] Rdm (i = 1, 2, .., M )), where dm is the feature

dimension of the mth party and

M m=1

dm

=

d.

Similarly,

Θ

=

(θ1, θ2, .., θM )

can be deﬁned where, θm Rdm denotes the model parameter of the m-th party.

Ideally, in VFL one of the collaborating parties is assumed to have the data

labels. The party possessing label information is referred to as active party

and the ones without label information as passive party. Considering the M-th

party as the active party that holds the label information y[i,M](y[i,M] R),

8 Vertical Federated Learning: A Structured Literature Review

Fig. 3: Vertical Federated Learning Architecture without Coordinator (Adapted from (Yang et al, 2019b))

the following function is minimized while training models in a VFL setting.

1n M

L(Θ) = n

l(

x[i,m]θi, y[i,M]) + λR(Θ)

(1)

i=1 m=1

Here l(.) and R(.) denote loss function and regularizer respectively while λ is the tuning parameter. VFL has been known to be used in solving regression (He et al, 2021) and classiﬁcation (Feng and Yu, 2020) problems. The Algorithm 1 (Zhu et al, 2021b) shows the process of solving a logistic regression problem in a VFL setting. VFL has also been proposed for other machine learning algorithms such as linear regression (Gasc´on et al, 2017; Kikuchi et al, 2018), decision trees (Khodaparast et al, 2018), random forests (Liu et al, 2020) and neural networks (Zhou et al, 2020) etc.

Extensions for Vertical Federated Learning: There has been some existing research conducted focusing on implementing the idea of VFL in extended scenarios. For instance, current VFL systems are developed on the assumption that the labels are possessed by only one client i.e. the active party. However, there might be practical cases where multiple collaborating clients possess labels which arises the need to apply VFL in a modiﬁed manner. The Multi-VFL proposed by Mugunthan et al (2021) makes use of split learning in a scenario where there are multiple data and label owners. Here, forward propagation is performed by the data owners on their corresponding partial models until the cut layer and then, their activations are sent to the label owners. These activations are concatenated by the label owners in order to complete their forward propagation. Subsequently, the losses are computed and back propagation is performed to compute the gradients. The gradients are then send back to the data owners who are supposed to use them for completing their back propagation. Moreover, Zhu et al (2021a) proposed a secure vertical FL framework PIVODL, to train gradient boosting decision trees (GBDTs) with data labels distributed on multiple devices. PIVODL

Vertical Federated Learning: A Structured Literature Review 9

Algorithm 1 Vertical Federated Logistic Regression

1: M → Number of Clients

2: T → Number of Communication Rounds

3: B → Number of Batches

4: Training Data X = {X1, X2, .., XM }

5: Client 1 → Active Party/Guest Client

6: Client (2..M ) → Passive Party/Host Client 7: Initialize local model θ0m, m (1..M )

8: for each communication round t = 1, 2, . . . , T do 9: for each batch data Xbm (X1m, X2m, .., XBm) do

10:

for each Client m = 1, 2, ..M do

11:

Compute zbm = Xbmθtm

12:

if m = 1 then

13:

send zbm to Client 1

14:

end if

15:

end for

16:

Compute yˆb =

M m=1

zbm

and

L(yb, yˆb)

on

Client

1

17:

Compute

∂L ∂zb

on

Client

1

18:

Send

∂L ∂ zbm

to

Client

m, m

(2, k)

19:

for each Client m = 1, 2, ..M do

20:

θtm

←

θtm

−

η

∂L ∂ zbm

∂ zbm ∂ θtm

21:

end for

22: end for

23: end for

presents a setting of training XGBoost decision tree models in VFL where each of the participating client holds parts of data labels that cannot be disclosed to others during the training process. A similar approach is also observed in (Wang et al, 2022a) in order to deal with VFL when labels are distributed among multiple parties.
3.2 Improvements to Vertical Federated Learning
The existing literature on vertical federated learning can be categorized into four groups; communication, learning, privacy & security and business value. A brief overview of recent research on VFL in these ﬁelds is presented in Figure 4. On the other hand, Figure 5 depicts the trend of the selected studies per category over the years.
3.2.1 Communication
Eﬃcient communication and asynchronous updates are critical in VFL to achieve eﬃcient and scalable distributed machine learning. As VFL involves multiple parties collaborating to train a model while keeping their data private, eﬀective communication protocols are necessary to ensure that parties

10 Vertical Federated Learning: A Structured Literature Review
Fig. 4: Categorization of Vertical Federated Learning Literature
Fig. 5: Observed Trend in Selected Studies in VFL per Category
can exchange information and update model parameters accurately and eﬃciently. Asynchronous updates enable parties to update model parameters independently, reducing the waiting time for other parties to complete their updates. This approach can improve the eﬃciency of the training process and allow parties to work independently, making VFL more scalable. This section discusses some of the existing approaches found in the studies to deal with communication eﬃciency and asynchronism in VFL settings. Communication Eﬃciency When following the conventional VFL approach, each of the passive/host clients share their updated gradients or intermediate results with the active/guest client during every training iteration. The total communication for each client can signiﬁcantly increase over the course of hundreds or thousands of training iterations for very large data sets. As a result, the learning process might become ineﬃcient due to communication cost and bandwidth constraints. Some existing researches (Liu et al, 2022c; Yang et al, 2019a; Xu et al, 2021b; Xie et al, 2022; Fu et al, 2022b) deals with the communication overhead problem in VFL by reducing the number of local model updates during training. A Federated Stochastic Block Coordinate Descent (FedBCD)

Vertical Federated Learning: A Structured Literature Review 11

Modiﬁcation in Local Updates
Compression

Article (Liu et al, 2022c) (Yang et al, 2019a), (WenJie and Xuan, 2021) (Xu et al, 2021b)
(Xie et al, 2022) (Fu et al, 2022b) (Zhang et al, 2022a) (Castiglia et al, 2022) (Yang et al, 2021) (Li et al, 2020) (Khan et al, 2022) (Ratadiya et al, 2020) (Cha et al, 2021)
(Sha et al, 2021) (Wu et al, 2022)

Method Stochastic Block Coordinate Descent with multiple update of local models Quasi-Newton Method
Eliminates need for peer to peer communication among clients by using functional encryption schemes
Allowed multiple local updates in each round by using alternating direction of multipliers Cache enabled local updates at each client Adaptive selection of local updates
Arbitrary compression scheme on gradients of local models Transmission of selective gradients after compression Double-end sparse compression on local models Compression on local data using Autoencoders Compression on local data using Autoencoders Compression on local data using Autoencoders
Compression on local data containing images using feature maps Compression on local data using unsupervised representation learning

Model Logistic Regression, Neural Network Logistic Regression
Linear regression, Logistic regression, linear SVM
Convolutional Neural Network Neural Network Logistic Regression, Neural Network Neural Network
Logistic Regression
Logistic Regression, Neural Network Logistic Regression, SVM Logistic Regression
Neural Network
Neural Network
Neural Network

Dataset

MIMIC-III,

NUS-WIDE,

MNIST, Default-Credit

Default-Credit

Website phishing, Ionosphere,

Landsat satellite, Optical recog-

nition of handwritten digits,

MNIST

MNIST,

CIFAR-10,

NUS-WIDE, ModelNet40

Criteo5, zu6

a9a, MNIST, Citeseer

MIMIC-III, CIFAR-10, ModelNet40 Default Credit

Default Credit, Insurance claim dataset Adult income, Wine-quality, Breast cancer, Rice MSC Bank loan dataset

Adult income, Vestibular Schwannoma Dataset, The eICU Collaborative Research Database CIFAR-10, CIFAR-100, CINIC10 NUS-WIDE, MNIST

Table 2: Approaches to Reduce Communication Overhead in VFL

algorithm was proposed by Liu et al (2022c) for vertically partitioned data, wherein each party performs multiple local updates before each communication in order to reduce the number of client communication rounds signiﬁcantly. Furthermore, Quasi-Newton method based vertical federated learning systems (Yang et al, 2019a; WenJie and Xuan, 2021) proposed where descent steps scaled by approximate Hessian information are performed leading to faster convergence than Stochastic Gradient Descent (SGD)-based methods This allowed signiﬁcant reduction in the number of communication rounds. Zhang et al (2022a) proposed an algorithm to minimize the training time of VFL by adaptive selection of the number of local training updates for each party.
Another widely used strategy to achieve communication eﬃciency in VFL is the application of compression schemes to the data that is being shared among the clients. Castiglia et al (2022) demonstrates eﬃciency of VFL improves when the data to be transmitted such as gradients of clients are compressed (using quantization or sparsiﬁcation) before sharing. Based on this idea, Yang et al (2021) proposed a method of gradient sharing and compression in VFL where only the gradients greater than a certain threshold are selected and then compressed by each of the clients before sharing in order to reduce communication bandwidth. Similarly, Li et al (2020) proposed an eﬃcient vertical federated learning framework with gradient prediction and double-end sparse compression, where the compression occurs at the local models to reduce training time as well as transmission cost. Compression can also be directly applied on the local data of the host clients in a way that the compressed data contains relevant information of the local data. Later on these compressed local data are send to and aggregated by the guest client to train the model. Satisfactory outcomes to this approach has been observed in some of the research works where

12 Vertical Federated Learning: A Structured Literature Review
the compression of local data by extracting relevant information is done by using unsupervised techniques like Autoencoders (Khan et al, 2022; Ratadiya et al, 2020; Cha et al, 2021), Feature Maps (Sha et al, 2021) and Representation Learning (Wu et al, 2022). An overview of the approaches found in the studies for reducing communication overhead in VFL has been shown in Table 2.
Asynchronism
Vertical federated learning setting involves collaboration of multiple clients having diﬀerent features of a single data instance to train a machine learning model. But it is not always practical to assume that all the clients would be identical in terms of in storage, hardware, network connectivity, etc. Due to such variability among the clients there may be cases when one or more clients aren’t participating in model updates at the same time typically resulting in asynchronous updates. Thus asynchronism can pose challenges in the proper functioning of VFL which is addresses by some of the research works by Zhang et al (2021a); Gu et al (2021); Wei et al (2021). A vertical asynchronous FL scheme was proposed (Zhang et al, 2021b) incorporating a backward updating mechanism and a bi-level asynchronous parallel architecture. The two level parallel architecture: the inner level between active (available to share gradients) clients and the intra level within each client. The updates at both the levels are performed asynchronously which improves the eﬃciency and scalability. Moreover, Chen et al (2020) solved vertical FL in an asynchronous manner by allowing each client to run stochastic gradient algorithms without coordination of other clients. Thus, temporary inactivity of any client does not pose any problem in the overall training.
3.2.2 Learning
Some learning approaches that has been used to address speciﬁc challenges in vertical federated learning have been discussed below:
Data Alignment
Most of the research on VFL is conducted assuming that all the participating clients possess exactly same number of samples but diﬀerent features. However, in real world scenario, this assumption may not be suitable as clients may not have identical records of data. The necessity to determine the common set of samples among the clients arises but also the aspect of privacy has to be considered such that no raw data is revealed. A common technique to solve this problem is Private Set Intersection (PSI) (Freedman et al, 2004; Huang et al, 2012; Cristofaro and Tsudik, 2010) which is a privacy-preserving technique that is commonly used in vertical federated learning to enable parties to perform joint analysis on their data without revealing their private data. PSI allows two parties to compute the intersection of their private datasets without revealing the contents of the datasets. The basic idea behind PSI is that each party privately computes a hash function on their dataset and shares

Vertical Federated Learning: A Structured Literature Review 13
the hashes with the other party. The parties then compare their hashes to ﬁnd the intersection of their datasets. The comparison is done without revealing any information about the actual data. After the intersection is computed, the parties can continue with the collaborative learning process using only the intersection of the datasets. Over the past few years, diﬀerent hashing techniques for PSI have been proposed in (Buddhavarapu et al, 2020; Ion et al, 2020; Chase and Miao, 2020; Lu and Ding, 2020) such as Bloom Filters and Oblivious Hashing. Bloom Filters are a probabilistic data structure that allow for eﬃcient set membership testing, while Oblivious Hashing is a cryptographic technique that enables parties to privately compute hash functions without revealing the inputs to each other.
Limited Training Samples
It is more practical to apply any of the PSI protocols before the implementation of VFL in a real world application in order to determine the common set of data records. But a crucial fact which is to be considered is that, there might not always be enough common or overlapping samples of data available among the clients. As VFL might not produce satisfactory results due to lack of suﬃcient data. To address this problem a solution could be to expand the training data which has to be done also in a privacy preserving manner. A data augmentation method, FedDA proposed by Zhang and Jiang (2022) uses generative adversarial network (GAN) to generate more overlap data by learning the features of ﬁnite overlap data and many locally existing non-overlap data among the clients. Similarly, the semi-supervised learning approach FedCVT in (Kang et al, 2022) improves the performance of the VFL model with limited aligned samples by expanding the training data through estimation of representations for missing features and predicting pseudo-labels. Some other approaches to tackle the issue with limited samples include determining inferences from non-overalpping data by using Federated Transfer Learning (Gao et al, 2019) and Oblivious Transfer (Ren et al, 2022).
Feature Selection
To improve accuracy and training time of machine learning models, feature selection is a widely used strategy. It refers to the is the practice of choosing a subset of relevant features (predictors and variables) for use in a model construction. Conventional feature selection methods like Principal Component Analysis are simpler to apply in HFL setting compared to VFL. Since features are distributed across multiple clients, it becomes challenging to use the typical feature selection methods. Federated PCA approaches have been proposed (Cheung et al, 2021, 2022) where feature selection is achieved in a VFL setting at each client end by sharing of eigen vectors and eigen values of the host clients with the guest client. Furthermore, (Feng, 2022) proposed a VFLbased feature selection method that leverages deep learning models as well as complementary information from features in the same samples at multiple parties without data disclosure. A Federated Stochastic Dual-Gate based Feature

14 Vertical Federated Learning: A Structured Literature Review
Selection (FedSDGFS) approach has been described in (Li et al, 2023a) which eﬃciently approximates the probability of a feature being selected, with privacy protection through Partially Homomorphic Encryption without a trusted third-party. Other approaches like Gini-impurity based feature selection & sparse learning-based unsupervised feature selection (Zhang et al, 2022b; Feng and Yu, 2020) have also been investigated in VFL settings which resulted in better performance when compared to traditional VFL.
Model Fairness
Machine learning models in some cases may manifest unexpected and erratic behaviors. These behaviors when have undesirable eﬀects on users, the model can be deemed as “unfair” based on some criteria. The existing bias in the training data is one of the key causes of a model becoming unfair. As real-world data encodes bias on sensitive features such as age, gender, and so on, VFL models may adopt bias from data and become unfair to particular user groups (Wu et al, 2021). Again due to features being decentralized across diﬀerent parties, applying existing fair ML methods to VFL models becomes challenging. Qi et al (2022) have addressed this issue by proposing a FairVFL framework where uniﬁed and fair representations of samples are learned based on the decentralized features in a privacy-preserving way. In order to obtain fair representations adversarial learning has been used to eliminate bias from the data. A superior performance in training fair VFL model was achieved (Liu et al, 2021) in which the fair learning task was modeled as a non-convex constrained optimization problem. The equivalent dual form of the optimization problem was considered and subsequently, an asynchronous gradient coordinate descent ascent algorithm was proposed to solve the dual problem.
3.2.3 Privacy and Security
Federated learning ensures privacy of data while federation among clients since training of models occurs locally and data never leaves their local sites. In vertical federated learning process, the federated model is trained by sharing of gradients or intermediate results among clients. However, some studies conclude that, there are still possibilities of sensitive private data being leaked through the local gradients in (Aono et al, 2017), and participants’ data can be inferred through a generative adversarial network during the prediction stage in VFL (Luo et al, 2021).
Privacy-preserving Protocols
According to recent studies on VFL, the widely used privacy-preserving protocols include homomorphic encryption (HE) is and diﬀerential privacy (DP).
Homomorphic encryption (HE) (Yi et al, 2014) is a cryptography technique which allows speciﬁc types of computations to be carried out on ciphertexts and generates an encrypted result which, when decrypted, matches the result of operations performed on the plaintexts. For the purpose of encrupting

Vertical Federated Learning: A Structured Literature Review 15
intermediate results (e.g. gradients), VFL typically utilizes additively homomorphic encryption like Paillier (Paillier, 1999). Additively homomorphic encryption allows participants to encrypt their data with a known public key and perform computation with the encrypted data by other participants with the same public key. The encrypted data needs to be sent to the private key holder so that it can be decrypted. A secure cooperative learning framework was proposed (Hardy et al, 2017) for vertically partitioned data using additional HE. The framework was evaluated to be precise as the non-private solution of centralized data. Moreover, it scaled to problems with large number of samples and features. Similarly, (Dai et al, 2021) proposed a privacy-preserving DNN model training scheme based on homomorphic encryption is for vertically segmented datasets. Moreover, Several other studies (Yang et al, 2019c,a; Ou et al, 2020) also have used HE as a privacy preserving protocol while proposing vertical federated learning approaches.
Diﬀerential privacy (DP) is a privacy-preserving protocol for bounding and quantifying the privacy leakage of sensitive data when performing learning tasks. It relies on adding noise to original data or training results to protect privacy. Too much noise can degrade the model’s performance, while too less data can breach privacy. Hence, a balance between performance and privacy has to be achieved here. Wang et al. (Wang et al, 2020) designed a DP-based privacy-preserving algorithm to ensure the data conﬁdentiality of VFL participants. The algorithm, when implemented, was quantitatively and qualitatively similar to generalized linear models, learned in an idealized non-private VFL setting. A multiparty learning framework for vertically partitioned datasets proposed in (Xu et al, 2021a) achieves diﬀerential privacy of the released model by incorporating noise to the objective function. In this case, the framework requires only a single round of noise addition and secure aggregation. In addition to using DP during model training, it can also be used during the model evaluation phase in VFL since there is also possibility of leaking private label information. Sun et al. proposed two evaluation algorithms in (Sun et al, 2022c) that accurately computes the widely used AUC (area under curve) metric when using label DP (Ghazi et al, 2021) in VFL.
Privacy Attacks in VFL
Attacks in federated learning refer to malicious attempts by an attacker to manipulate or compromise the integrity of the federated learning process. Label inference attacks and feature inference attacks are the most commonly explored adversarial attacks in VFL. A label inference attack in vertical federated learning is a type of privacy attack where a malicious participant tries to infer the labels (i.e., the output values) associated with the training data of other participants, with the aim of obtaining sensitive information. Because no raw data is shared between the two parties, VFL initially appears to be private. At the end of a passive party, a considerable amount of information still exists in the cut layer embedding that can be used by the active party

16 Vertical Federated Learning: A Structured Literature Review
to leak raw data (Fu et al, 2022a). Fu et. al in (Fu et al, 2022a) explored the possibilities of inferring labels exploiting the gradient update mechanism in VFL and as well as inferring labels from the bottom model trained locally by each participant to embed the input features to a latent space, which avoids the raw features being directly sent to the server. (Zou et al, 2022) also showed that private labels could be reconstructed with high accuracy by training a gradient inversion model even with HE-communication. Moreover, (Sun et al, 2022b) proposed a label inference method where it was possible to steal private labels eﬀectively from the shared intermediate embedding even though label diﬀerential privacy and gradients perturbation were applied.
On the other hand, a feature inference attack is a type of privacy attack that aims to infer private information about a party’s data based on the model’s output. In vertical federated learning, an attacker may try to infer a party’s private features by analyzing the model’s output on the shared features. For example, an attacker may try to infer a person’s medical condition based on their zip code and medical procedures, which are shared between diﬀerent parties. (Ye et al, 2022) proved theoretically and experimentally that, it is possible to perform a reconstruction attack on the features of the parties by applying a robust search-based attack algorithm unless the feature dimension is considerably large enough. Furthermore, (Luo et al, 2021) proposed a general feature inference attack which learns the correlations between the features of the adversary and the attack target’s features based on multiple predictions accumulated by the adversary.
Defense Mechanisms
Defense mechanisms are crucial for vertical federated learning because this approach involves training machine learning models on data that is distributed across diﬀerent entities or organizations. This means that the data is often sensitive and private, and its exposure could lead to signiﬁcant risks, such as identity theft, ﬁnancial fraud, or discrimination. Mainstream defense mechanisms like adding noise to gradients (Zhu et al, 2019), gradient compression (Lin et al, 2017), and randomly pruning part of the gradients to zero (Shokri and Shmatikov, 2015) are not able to mitigate possible label information leakage in VFL settings (Fu et al, 2022a). (Sun et al, 2022b) proposed adding an additional optimization goal at the label party and limiting the label stealing ability of the adversary by minimizing the distance correlation between the intermediate embedding and corresponding private labels. Similarly, a dispersed training framework was also been proposed in (Wang et al, 2022b) where secret sharing had been used in order to break the correlation between the bottom model and the training data. In addition, (Wang et al, 2022b) also described a customized model aggregation method such that the shared model can be privately combined, and as well as the training accuracy in ensured by the linearity of secret sharing schemes. Furthermore, (Zou et al, 2022) demonstrated that using confusional autoencoder (CAE);

Vertical Federated Learning: A Structured Literature Review 17
a technique based on autoencoder and entropy regularization and its variant DiscreteSGD-enhanced CAE could successfully block label inference attacks without signiﬁcantly hampering the accuracy of the main task.
Additionally, (Qiu et al, 2022) proposed HashVFL which is a hashing based VFL framework to deal with feature reconstruction attacks in a VFL setting. The one way nature of hashing enables blocking of all attempts to recover data from hash codes. Eﬃciency of HashVFL in mitigating reconstruction attacks have also been demonstrated through experimental results.An adversarial training based framework for VFL has been designed in (Sun et al, 2021a) which simulates the game between an attacker (i.e. the active party) who actively reconstructs raw input from the cut layer embedding and a defender (i.e. the passive party) who aims to prevent the input leakage. A similar approach is observed in (Luo et al, 2021) which deals with malicious attack by active party during model evaluation stage of VFL.
3.2.4 Business Value
The concept of fairness in FL also includes collaboration fairness which implies the incentive mechanism in a FL setting. Since FL is based on collaboration, rewarding mechanism is crucial for the allocating rewards to current and potential participants of FL. To achieve that, FL needs a fair evaluation mechanism to give agents reasonable rewards. Moreover, to analyse business value of VFL in real world application determing a proper incentive mechanism for the participating clients is crucial since not doing do may result in lack of motivation from the clients to collaborate. The Shapley value (SV) (Roth, 1988) is a provably fair contribution valuation metric originated from cooperative game theory. The contribution of participants can be measured by Shapley Values to calculate grouped feature importance (Wang et al, 2019). This idea can also be extended to both synchronous and asynchronous vertical federated algorithms (Fan et al, 2022). (Zhang et al, 2022c) further extended the idea of contribution measurement of participants in VFL by incorporating the data pricing model based on Stackelberg with the hosts as the leader and the guest as the follower. This approach provided a managerial guidance on revenue distribution for FL platform owners based on their contributions.
3.3 VFL Applications
Google initiated project in 2016 in order to establish federated learning among Android mobile users (Bonawitz et al, 2017). The goal was to improve the keyboard input prediction quality, while at the same time ensure the security and privacy of users. In the later stage, many use cases were addressed in several surveys and studies where FL could be implemented. There has been signiﬁcant eﬀorts in designing federated learning frameworks for industrial usage but most of them are still in their development stage. Among the existing FL frameworks, only few of them support VFL either completely

18 Vertical Federated Learning: A Structured Literature Review

Framework Regression

FATE FedML PaddleFL FedLearner FederatedScope Crypten FedTree

-

-

Model Support

Neural Network

-

-

-

Tree-Based Model Third Party Collaborator Requirement
Complete Privacy of Model Parameters

-

-

-

-

-

-

-

-

-

Privacy

Complete Privacy of Model Gradients Availability of Complete Documentation

-

-

-

-

Table 3: Overview of Federated Learning Frameworks Supporting Vertical

Partitioning (Adjusted from (Liu et al, 2022a))

or partially. Table 3 provides an overview of the existing VFL frameworks. It can be observed from the overview that, most listed frameworks (FedML (He et al, 2020), FedLearner, FederatedScope) do not support a variety of ML models for implementation. Besides, few of the frameworks don’t have complete privacy features incorporated and don’t possess complete documentations. Thus, it can be concluded that VFL frameworks are still in its developing stage which indicates a lot of scope to work on.
Federated learning is a cutting-edge modeling mechanism which is capable of training a uniﬁed machine learning model using decentralized data while maintaining privacy and security. Thus, it has a promising application in ﬁnancial, healthcare and many other industries where direct aggregation of data for training models is not feasible due to concerns like data security, privacy protection and intellectual property. Most applications are focused on horizontal federated learning and it is quite diﬃcult to observe VFL being used in realworld applications yet. However, in our literature review, we found a limited number of studies proposing and as well as implementing VFL in applications. (Sun et al, 2021b) considered a scenario involving two hospitals; Inner and Outer hospital possessing records of daily performance and clinical test of patients. Due to patients privacy regulations, it was not allowed to share raw data among the hospitals even though they had records of the same patients. Hence, the Inner- and Outer-hospital information had been bridged via vertical Federated Learning for perioperative complications prognostic prediction. In (Zhou et al, 2022) a VFL scheme is developed for the purpose of human activity recognition (HAR) across a variety of diﬀerent devices from multiple individual users by integrating shareable features from heterogeneous data across diﬀerent devices into a full feature space. VFL has also been useful in the ﬁeld of e-commerce as observed in (Zhang and Jiang, 2021) where a method based on clustering and latent factor model under the vertical federated recommendation system was implemented. Taking into account the diversity of a large number of diﬀerent users in each participant and the complexity of the matrix factorization of the user-item matrix, the users were clustered to

Vertical Federated Learning: A Structured Literature Review 19
reduce the dimension of the matrix and improve the accuracy of user recommendations. Similarly, eﬃcient online advertising was achieved through the application of VFL (Li et al, 2022). Vertical federated learning when applied to ﬁnancial institutions boosted their proﬁts by collaboration of data among them maintaining privacy (Liang et al, 2021; Efe, 2021). A special use case of VFL was observed in the aviation domain (Guo et al, 2021) where a ﬂight delay prediction model based on federated learning was designed by integrating horizontal and vertical federated frameworks.
4 Open Challenges and Future Directions
The structured literature review has provided insights to existing approaches adapted to improve diﬀerent aspects of VFL while also pointing out potential research directions by identifying the gaps in the literature, highlighting unresolved issues and noting new developments in VFL. The research directions concluded from the review are discussed as follows:
4.1 Communication Eﬃciency
The total communication and computation cost in VFL is proportional to the size of the training data. With the growing amount of data accross multiple platforms, VFL becomes more challenging due to signiﬁcant increase in local model computations, updates and as well as communication cost. To tackle this problem several suﬃcient studies discussed earlier have been computation and communication eﬃcient methods that reduce the communication and computation complexity. But then again data in massively increasing day by day, on the other hand computing resources of participants are not increasing at the same rate. Hence, making VFL more eﬃcient by ensuring low communication rounds and computation cost will still remain a challenge.
4.2 Privacy and Security
Privacy and Security has always been a concern in FL since the assurance of the no exposure of raw data is a major motivation for data owners to participate in the federation. Researchers in their studies so far have rigorously tried to identify potential privacy leakage and malicious attacks in FL setup. However, the aspect of security in VFL is not enough explored as most privacy preserving protocols like homomorphic encryption, secret sharing and diﬀerential privacy are used in HFL scenarios. Moreover, possibilities and defense mechanisms for poison attacks; introduction of malicious data during training phase by adversary and backdoor attacks; insertion of malicious functionality into a targeted model through poisoned updates from malicious clients in VFL settings have been very limitedly explored. Hence, there is still scope for improvements in dealing with privacy and integrity leakage in VFL.

20 Vertical Federated Learning: A Structured Literature Review
4.3 Limited Training Data
As a preprocessing step for VFL, the overlapping samples among the clients are determined. Often the overlapping samples are insuﬃcient for achieving a good performance on VFL models. Expanding the training data is a solution but then again that also comes with privacy constraints since local raw data cannot be disclosed. Again not utilizing the non-overlapping samples among the participants would mean making the data useless. Since data is expensive and diﬃcult to obtain, new approaches could be designed such that relevant information is inferred from the non-overlapping data as well.
4.4 Feature Selection
Feature selection, which has been a topic of research in both methodology and practice for decades, is used in a variety of diﬀerent ﬁelds like text mining, image recognition, fault diagnosis, intrusion detection, and so on. VFL has promising potential in many feature selection applications with privacy preservation. Feature selection combined with VFL would allow business from diﬀerent organizations to collaboratively perform feature selection without exposing private data. Privacy-preserving feature selection in VFL has not been fully explored yet, although only a few solutions were presented (Section 3.2.2). Hence, designing eﬃcient and eﬀective privacy-preserving feature selection protocols for VFL could be an interesting direction for future research.
4.5 Model Fairness
There has been rising interest in developing fair methods for machine learning (Yucer et al, 2020). In practical VFL setups, the aspects like data being distributed across multiple platforms and asynchronous parallelized updates makes model fairness even more challenging to enhance. However, such concerns have been less addressed in vertical federated learning settings. The application of fair VFL methods for ensuring bias free model training is an open opportunity for future vertical federated learning research. It is particularly important as VFL has potential to be implemented in applications involving real populations of users without knowledge of their sensitive identities.
4.6 Incentive Mechanism
Motivating data owners to participate in a data federation is a signiﬁcant challenge in federated learning. Even in vertically federated setups, it is essential to encourage more qualiﬁed clients to participate. In order to achieve this goal, incentive schemes are to be designed which are able to fairly share the proﬁts generated due to the collaboration among participants. This cannot be done without ﬁrst establishing a mechanism for assessing each data owner’s contribution to the federated model. Despite the fact that several works have focused on the design of incentive mechanisms for vertical federated learning,

Vertical Federated Learning: A Structured Literature Review 21
one crucial aspect, i.e. security, has been overlooked. Even while evaluating contributions of participants, it is important that privacy is preserved from all ends by ensuring no exposure of data. Improvised incentive mechanism could be designed which not only deals with distribution of proﬁt but also penalizes the participants in case erroneous data is provided by them misleading the federated learning process. Furthermore, almost all studies dealing with the contribution measurement of parties in VFL, have utilized the cooperative game theory concept of Shapley values to calculate participant contribution. However, calculating Shapley values is computationally expensive due to consideration of all possible coalitions. In this case, other cooperative game theory concepts can also be experimented with in context of VFL settings. To design a fair and eﬀective incentive mechanism, in addition to contribution measurement, proper selection of participants in VFL settings (Jiang et al, 2022) is also crucial such that the best parties are only collaborated with to improve the global model. Therefore, participant selection in VFL is an intriguing research direction.
4.7 Explainability
An area of study that has gained a great deal of attention recently is explainable artiﬁcial intelligence (XAI). Models must be explainable in high stake applications like healthcare, when there is a strong need to justify decisions made. The same applies in case of VFL as well. VFL models must be explainable, especially when dealing with sensitive data. In VFL models, each party’s data is kept private and is not accessible to third parties for analysis. For the deployment of VFL, it is crucial to interpret models while making sure the data is stored locally only. Explainability of VFL models may also be attributed to potential privacy violations from unintended data leaks. Thus, there is a potential scope for future research for explainable AI in context of VFL where a trade-oﬀ between explainability and privacy is achieved.
5 Conclusion
With the advancement of big data and artiﬁcial intelligence, the expectations of privacy are becoming increasingly stringent. As a result, federated learning, a novel solution to cross-platform privacy protection, was developed. Apart from privacy, FL now needs to deal with a number of other challenges when applied to data partitioned in various formats. While most studies focus on FL on horizontally partitioned data, this review aims to provide researchers in FL domain with the current state-of-the-art in vertical federated learning. We not only mentioned the challenges observed in VFL in our study, but also clustered them into four categories; communication, learning, privacy & security and business value. The study also discussed the approaches adopted by earlier studies to overcome these challenges and looked into the applicability of VFL in real-world scenarios. Finally, a set of eight prospective future directions for research in this domain have been identiﬁed.

22 Vertical Federated Learning: A Structured Literature Review
Supplementary information. Not applicable.
Declarations
• Funding: Not Applicable • Conﬂict of interest: The authors declare no conﬂict of interest.
References
Albrecht JP (2016) How the gdpr will change the world. Eur Data Prot L Rev 2:287
Aono Y, Hayashi T, Wang L, et al (2017) Privacy-preserving deep learning via additively homomorphic encryption. IEEE Transactions on Information Forensics and Security 13(5):1333–1345
Armitage A, Keeble-Allen D (2008) Undertaking a structured literature review or structuring a literature review: Tales from the ﬁeld. In: Proceedings of the 7th European Conference on Research Methodology for Business and Management Studies: ECRM2008, Regent’s College, London, p 35
Bhardwaj R, Nambiar AR, Dutta D (2017) A study of machine learning in healthcare. In: 2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC), IEEE, pp 236–241
Bonawitz K, Ivanov V, Kreuter B, et al (2017) Practical secure aggregation for privacy-preserving machine learning. In: proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp 1175– 1191
Buddhavarapu P, Knox A, Mohassel P, et al (2020) Private matching for compute. Cryptology ePrint Archive
Castiglia TJ, Das A, Wang S, et al (2022) Compressed-vﬂ: Communicationeﬃcient learning with vertically partitioned data. In: International Conference on Machine Learning, PMLR, pp 2738–2766
Cha D, Sung M, Park YR, et al (2021) Implementing vertical federated learning using autoencoders: Practical application, generalizability, and utility study. JMIR medical informatics 9(6):e26,598
Chase M, Miao P (2020) Private set intersection in the internet setting from lightweight oblivious prf. In: Annual International Cryptology Conference, Springer, pp 34–63
Chen T, Jin X, Sun Y, et al (2020) Vaﬂ: a method of vertical asynchronous federated learning. arXiv preprint arXiv:200706081

Vertical Federated Learning: A Structured Literature Review 23
Cheng K, Fan T, Jin Y, et al (2021) Secureboost: A lossless federated learning framework. IEEE Intelligent Systems 36(6):87–98
Cheng Y, Liu Y, Chen T, et al (2020) Federated learning for privacy-preserving ai. Communications of the ACM 63(12):33–36
Cheung Ym, Lou J, Yu F (2021) Vertical federated principal component analysis on feature-wise distributed data. In: International Conference on Web Information Systems Engineering, Springer, pp 173–188
Cheung Ym, Jiang J, Yu F, et al (2022) Vertical federated principal component analysis and its kernel extension on feature-wise distributed data. arXiv preprint arXiv:220301752
Cristofaro ED, Tsudik G (2010) Practical private set intersection protocols with linear complexity. In: International Conference on Financial Cryptography and Data Security, Springer, pp 143–159
Dai M, Xu A, Huang Q, et al (2021) Vertical federated dnn training. Physical Communication 49:101,465
Efe Y (2021) A vertical federated learning method for multi-institutional credit scoring: Mics. arXiv preprint arXiv:211109038
Fan Z, Fang H, Zhou Z, et al (2022) Fair and eﬃcient contribution valuation for vertical federated learning. arXiv preprint arXiv:220102658
Feng S (2022) Vertical federated learning-based feature selection with non-overlapping sample utilization. Expert Systems with Applications 208:118,097
Feng S, Yu H (2020) Multi-participant multi-class vertical federated learning. arXiv preprint arXiv:200111154
Freedman MJ, Nissim K, Pinkas B (2004) Eﬃcient private matching and set intersection. In: International conference on the theory and applications of cryptographic techniques, Springer, pp 1–19
Fu C, Zhang X, Ji S, et al (2022a) Label inference attacks against vertical federated learning. In: 31st USENIX Security Symposium, Security 2022, USENIX Association, pp 1397–1414
Fu F, Miao X, Jiang J, et al (2022b) Towards communication-eﬃcient vertical federated learning training via cache-enabled local updates. Proceedings of the VLDB Endowment 15(10):2111–2120
Gao D, Liu Y, Huang A, et al (2019) Privacy-preserving heterogeneous federated transfer learning. In: 2019 IEEE International Conference on Big Data

24 Vertical Federated Learning: A Structured Literature Review
(Big Data), IEEE, pp 2552–2559
Gasc´on A, Schoppmann P, Balle B, et al (2017) Privacy-preserving distributed linear regression on high-dimensional data. Proc Priv Enhancing Technol 2017(4):345–364
Ghazi B, Golowich N, Kumar R, et al (2021) Deep learning with label diﬀerential privacy. Advances in Neural Information Processing Systems 34:27,131–27,145
Goodell JW, Kumar S, Lim WM, et al (2021) Artiﬁcial intelligence and machine learning in ﬁnance: Identifying foundations, themes, and research clusters from bibliometric analysis. Journal of Behavioral and Experimental Finance 32:100,577
Gu B, Xu A, Huo Z, et al (2021) Privacy-preserving asynchronous vertical federated learning algorithms for multiparty collaborative learning. IEEE transactions on neural networks and learning systems
Guo L, Wei Q, Chenyu H (2021) Research on ﬂight delay prediction based on horizontal and vertical federated learning framework. In: 2021 IEEE 3rd International Conference on Civil Aviation Safety and Information Technology (ICCASIT), IEEE, pp 38–44
Hardy S, Henecka W, Ivey-Law H, et al (2017) Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption. arXiv preprint arXiv:171110677
He C, Li S, So J, et al (2020) Fedml: A research library and benchmark for federated machine learning. arXiv preprint arXiv:200713518
He D, Du R, Zhu S, et al (2021) Secure logistic regression for vertical federated learning. IEEE Internet Computing 26(2):61–68
Huang Y, Evans D, Katz J (2012) Private set intersection: Are garbled circuits better than custom protocols? In: NDSS
Ion M, Kreuter B, Nergiz AE, et al (2020) On deploying secure computing: Private intersection-sum-with-cardinality. In: 2020 IEEE European Symposium on Security and Privacy (EuroS&P), IEEE, pp 370–389
Jiang J, Burkhalter L, Fangcheng F, et al (2022) Vf-ps: How to select important participants in vertical federated learning, eﬃciently and securely? In: Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems 2022

Vertical Federated Learning: A Structured Literature Review 25
Kairouz P, McMahan HB, Avent B, et al (2021) Advances and open problems in federated learning. Foundations and Trends® in Machine Learning 14(1– 2):1–210
Kang Y, Liu Y, Liang X (2022) Fedcvt: Semi-supervised vertical federated learning with cross-view training. ACM Transactions on Intelligent Systems and Technology (TIST) 13(4):1–16
Khan A, ten Thij M, Wilbik A (2022) Communication-eﬃcient vertical federated learning. Algorithms 15(8):273
Khodaparast F, Sheikhalishahi M, Haghighi H, et al (2018) Privacy preserving random decision tree classiﬁcation over horizontally and vertically partitioned data. In: 2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech), IEEE, pp 600–607
Kikuchi H, Hamanaga C, Yasunaga H, et al (2018) Privacy-preserving multiple linear regression of vertically partitioned real medical datasets. Journal of Information Processing 26:638–647
Kitchenham BA (2012) Systematic review in software engineering: where we are and where we should be going. In: Proceedings of the 2nd international workshop on Evidential assessment of software technologies, pp 1–2
Li A, Peng H, Zhang L, et al (2023a) Fedsdg-fs: Eﬃcient and secure feature selection for vertical federated learning. arXiv preprint arXiv:230210417
Li M, Chen Y, Wang Y, et al (2020) Eﬃcient asynchronous vertical federated learning via gradient prediction and double-end sparse compression. In: 2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV), IEEE, pp 291–296
Li Q, Thapa C, Ong L, et al (2023b) Vertical federated learning: Taxonomies, threats, and prospects. arXiv preprint arXiv:230201550
Li W, Xia Q, Cheng H, et al (2022) Vertical semi-federated learning for eﬃcient online advertising. arXiv preprint arXiv:220915635
Liakos KG, Busato P, Moshou D, et al (2018) Machine learning in agriculture: A review. Sensors 18(8):2674
Liang Y, Liu Z, Song Y, et al (2021) A methodology of trusted data sharing across telecom and ﬁnance sector under china’s data security policy. In: 2021 IEEE International Conference on Big Data (Big Data), IEEE, pp 5406–5412

26 Vertical Federated Learning: A Structured Literature Review
Lin Y, Han S, Mao H, et al (2017) Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:171201887
Liu C, Zhou Z, Shi Y, et al (2021) Achieving model fairness in vertical federated learning. arXiv preprint arXiv:210908344
Liu X, Shi T, Xie C, et al (2022a) Unifed: A benchmark for federated learning frameworks. arXiv preprint arXiv:220710308
Liu Y, Liu Y, Liu Z, et al (2020) Federated forest. IEEE Transactions on Big Data
Liu Y, Kang Y, Zou T, et al (2022b) Vertical federated learning. arXiv preprint arXiv:221112814
Liu Y, Zhang X, Kang Y, et al (2022c) Fedbcd: A communication-eﬃcient collaborative learning framework for distributed features. IEEE Transactions on Signal Processing
Lu L, Ding N (2020) Multi-party private set intersection in vertical federated learning. In: 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), IEEE, pp 707– 714
Luo X, Wu Y, Xiao X, et al (2021) Feature inference attack on model predictions in vertical federated learning. In: 2021 IEEE 37th International Conference on Data Engineering (ICDE), IEEE, pp 181–192
Ma C, Li J, Ding M, et al (2020) On safeguarding privacy and security in the framework of federated learning. IEEE network 34(4):242–248
Mugunthan V, Goyal P, Kagal L (2021) Multi-vﬂ: A vertical federated learning system for multiple data and label owners. arXiv preprint arXiv:210605468
Ou W, Zeng J, Guo Z, et al (2020) A homomorphic-encryption-based vertical federated learning scheme for rick management. Computer Science and Information Systems 17(3):819–834
Paillier P (1999) Public-key cryptosystems based on composite degree residuosity classes. In: International conference on the theory and applications of cryptographic techniques, Springer, pp 223–238
Qi T, Wu F, Wu C, et al (2022) Fairvﬂ: A fair vertical federated learning framework with contrastive adversarial learning. arXiv preprint arXiv:220603200
Qiu P, Zhang X, Ji S, et al (2022) All you need is hashing: Defending against data reconstruction attack in vertical federated learning. arXiv preprint

Vertical Federated Learning: A Structured Literature Review 27
arXiv:221200325
Ratadiya P, Asawa K, Nikhal O (2020) A decentralized aggregation mechanism for training deep learning models using smart contract system for bank loan prediction. arXiv preprint arXiv:201110981
Ren Z, Yang L, Chen K (2022) Improving availability of vertical federated learning: Relaxing inference on non-overlapping data. ACM Transactions on Intelligent Systems and Technology (TIST)
Roth AE (1988) The Shapley value: essays in honor of Lloyd S. Shapley. Cambridge University Press
Sha T, Yu X, Shi Z, et al (2021) Feature map transfer: Vertical federated learning for cnn models. In: International Conference on Data Mining and Big Data, Springer, pp 37–44
Shokri R, Shmatikov V (2015) Privacy-preserving deep learning. In: Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pp 1310–1321
Streeton R, Cooke M, Campbell J (2004) Researching the researchers: using a snowballing technique. Nurse researcher 12(1):35–47
Sun H, Wang Z, Huang Y, et al (2022a) Privacy-preserving vertical federated logistic regression without trusted third-party coordinator. In: 2022 The 6th International Conference on Machine Learning and Soft Computing, pp 132–138
Sun J, Yao Y, Gao W, et al (2021a) Defending against reconstruction attack in vertical federated learning. arXiv preprint arXiv:210709898
Sun J, Yang X, Yao Y, et al (2022b) Label leakage and protection from forward embedding in vertical federated learning. arXiv preprint arXiv:220301451
Sun J, Yang X, Yao Y, et al (2022c) Diﬀerentially private auc computation in vertical federated learning. arXiv preprint arXiv:220512412
Sun W, Chen Y, Yang X, et al (2021b) Fedio: Bridge inner-and outer-hospital information for perioperative complications prognostic prediction via federated learning. In: 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE, pp 3215–3221
Wang C, Liang J, Huang M, et al (2020) Hybrid diﬀerentially private federated learning on vertically partitioned data. arXiv preprint arXiv:200902763
Wang G, Dang CX, Zhou Z (2019) Measure contribution of participants in federated learning. In: 2019 IEEE International Conference on Big Data (Big

28 Vertical Federated Learning: A Structured Literature Review
Data), IEEE, pp 2597–2604
Wang R, Ersoy O, Zhu H, et al (2022a) Feverless: Fast and secure vertical federated learning based on xgboost for decentralized labels. IEEE Transactions on Big Data pp 1–15. https://doi.org/10.1109/TBDATA.2022.3227326
Wang Y, Lv Q, Zhao M, et al (2022b) Beyond model splitting: Preventing label inference attacks in vertical federated learning with dispersed training
Wei Q, Li Q, Zhou Z, et al (2021) Privacy-preserving two-parties logistic regression on vertically partitioned data using asynchronous gradient sharing. Peer-to-Peer Networking and Applications 14(3):1379–1387
WenJie S, Xuan S (2021) Vertical federated learning based on dfp and bfgs. arXiv preprint arXiv:210109428
Wu C, Wu F, Wang X, et al (2021) Fairness-aware news recommendation with decomposed adversarial learning. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp 4462–4469
Wu Z, Li Q, He B (2022) Practical vertical federated learning with unsupervised representation learning. IEEE Transactions on Big Data
Xie C, Chen PY, Zhang C, et al (2022) Improving privacy-preserving vertical federated learning by eﬃcient communication with admm. arXiv preprint arXiv:220710226
Xu D, Yuan S, Wu X (2021a) Achieving diﬀerential privacy in vertically partitioned multiparty learning. In: 2021 IEEE International Conference on Big Data (Big Data), IEEE, pp 5474–5483
Xu R, Baracaldo N, Zhou Y, et al (2021b) Fedv: Privacy-preserving federated learning over vertically partitioned data. In: Proceedings of the 14th ACM Workshop on Artiﬁcial Intelligence and Security, pp 181–192
Yang K, Fan T, Chen T, et al (2019a) A quasi-newton method based vertical federated learning framework for logistic regression. arXiv preprint arXiv:191200513
Yang K, Song Z, Zhang Y, et al (2021) Model optimization method based on vertical federated learning. In: 2021 IEEE International Symposium on Circuits and Systems (ISCAS), IEEE, pp 1–5
Yang Q, Liu Y, Cheng Y, et al (2019b) Federated learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning 13(3):1–207
Yang S, Ren B, Zhou X, et al (2019c) Parallel distributed logistic regression for vertical federated learning without third-party coordinator. arXiv preprint

Vertical Federated Learning: A Structured Literature Review 29
arXiv:191109824
Ye P, Jiang Z, Wang W, et al (2022) Feature reconstruction attacks and countermeasures of dnn training in vertical federated learning. arXiv preprint arXiv:221006771
Yi X, Paulet R, Bertino E (2014) Homomorphic encryption. In: Homomorphic encryption and applications. Springer, p 27–46
Yucer S, Akc¸ay S, Al-Moubayed N, et al (2020) Exploring racial bias within face recognition via per-subject adversarially-enabled data augmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp 18–19
Zhang J, Jiang Y (2021) A vertical federation recommendation method based on clustering and latent factor model. In: 2021 International Conference on Electronic Information Engineering and Computer Science (EIECS), IEEE, pp 362–366
Zhang J, Jiang Y (2022) A data augmentation method for vertical federated learning. Wireless Communications and Mobile Computing 2022
Zhang J, Guo S, Qu Z, et al (2022a) Adaptive vertical federated learning on unbalanced features. IEEE Transactions on Parallel and Distributed Systems 33(12):4006–4018
Zhang Q, Gu B, Deng C, et al (2021a) Asysqn: Faster vertical federated learning algorithms with better computation resource utilization. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp 3917–3927
Zhang Q, Gu B, Deng C, et al (2021b) Secure bilevel asynchronous vertical federated learning with backward updating. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp 10,896–10,904
Zhang R, Li H, Hao M, et al (2022b) Secure feature selection for vertical federated learning in ehealth systems. In: ICC 2022-IEEE International Conference on Communications, IEEE, pp 1257–1262
Zhang Z, Li X, Yang S (2022c) Data pricing in vertical federated learning. In: 2022 IEEE/CIC International Conference on Communications in China (ICCC), pp 932–937, https://doi.org/10.1109/ICCC55456.2022.9880795
Zhao D, Yao M, Wang W, et al (2022) Ntp-vﬂ-a new scheme for non-3rd party vertical federated learning. In: 2022 14th International Conference on Machine Learning and Computing (ICMLC), pp 134–139

30 Vertical Federated Learning: A Structured Literature Review
Zhou J, Chen C, Zheng L, et al (2020) Vertically federated graph neural network for privacy-preserving node classiﬁcation. arXiv preprint arXiv:200511903
Zhou X, Liang W, Ma J, et al (2022) 2d federated learning for personalized human activity recognition in cyber-physical-social systems. IEEE Transactions on Network Science and Engineering
Zhu H, Wang R, Jin Y, et al (2021a) Pivodl: Privacy-preserving vertical federated learning over distributed labels. IEEE Transactions on Artiﬁcial Intelligence
Zhu H, Zhang H, Jin Y (2021b) From federated learning to federated neural architecture search: a survey. Complex & Intelligent Systems 7(2):639–657
Zhu L, Liu Z, Han S (2019) Deep leakage from gradients. Advances in neural information processing systems 32
Zou T, Liu Y, Kang Y, et al (2022) Defending batch-level label inference and replacement attacks in vertical federated learning. IEEE Transactions on Big Data

