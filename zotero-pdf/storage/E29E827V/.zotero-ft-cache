 
Skip to main content 
Cornell University 
We gratefully acknowledge support from 
the Simons Foundation and member institutions. 
arxiv logo > cs > arXiv:2008.07180 
 
Help | Advanced Search 
Search 
Computer Science > Machine Learning 
(cs) 
[Submitted on 17 Aug 2020 ( v1 ), last revised 23 Sep 2020 (this version, v2)] 
Title: Shuffled Model of Federated Learning: Privacy, Communication and Accuracy Trade-offs 
Authors: Antonious M. Girgis , Deepesh Data , Suhas Diggavi , Peter Kairouz , Ananda Theertha Suresh 
Download PDF 
 
    Abstract: We consider a distributed empirical risk minimization (ERM) optimization problem with communication efficiency and privacy requirements, motivated by the federated learning (FL) framework. Unique challenges to the traditional ERM problem in the context of FL include (i) need to provide privacy guarantees on clients' data, (ii) compress the communication between clients and the server, since clients might have low-bandwidth links, (iii) work with a dynamic client population at each round of communication between the server and the clients, as a small fraction of clients are sampled at each round. To address these challenges we develop (optimal) communication-efficient schemes for private mean estimation for several ℓ p spaces, enabling efficient gradient aggregation for each iteration of the optimization solution of the ERM. We also provide lower and upper bounds for mean estimation with privacy and communication constraints for arbitrary ℓ p spaces. To get the overall communication, privacy, and optimization performance operation point, we combine this with privacy amplification opportunities inherent to this setup. Our solution takes advantage of the inherent privacy amplification provided by client sampling and data sampling at each client (through Stochastic Gradient Descent) as well as the recently developed privacy framework using anonymization, which effectively presents to the server responses that are randomly shuffled with respect to the clients. Putting these together, we demonstrate that one can get the same privacy, optimization-performance operating point developed in recent methods that use full-precision communication, but at a much lower communication cost, i.e., effectively getting communication efficiency for "free".  
 
Subjects: 	Machine Learning (cs.LG) ; Information Theory (cs.IT); Machine Learning (stat.ML) 
Cite as: 	arXiv:2008.07180 [cs.LG] 
  	(or arXiv:2008.07180v2 [cs.LG] for this version) 
  	https://doi.org/10.48550/arXiv.2008.07180 
Focus to learn more 
arXiv-issued DOI via DataCite 
Submission history 
From: Antonious Girgis Mamdouh [ view email ] 
[v1] Mon, 17 Aug 2020 09:41:04 UTC (37 KB) 
[v2] Wed, 23 Sep 2020 16:54:50 UTC (43 KB) 
Full-text links: 
Download: 
 
    PDF 
    PostScript 
    Other formats  
 
( license ) 
Current browse context: 
cs.LG 
< prev   |   next > 
new | recent | 2008 
Change to browse by: 
cs 
cs.IT 
math 
math.IT 
stat 
stat.ML 
References & Citations 
 
    NASA ADS 
    Google Scholar 
    Semantic Scholar 
 
DBLP - CS Bibliography 
listing | bibtex 
Antonious M. Girgis 
Deepesh Data 
Suhas N. Diggavi 
Peter Kairouz 
Ananda Theertha Suresh 
a export bibtex citation Loading... 
Bookmark 
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo 
Bibliographic Tools 
Bibliographic and Citation Tools 
Bibliographic Explorer Toggle 
Bibliographic Explorer ( What is the Explorer? ) 
Litmaps Toggle 
Litmaps ( What is Litmaps? ) 
scite.ai Toggle 
scite Smart Citations ( What are Smart Citations? ) 
Code & Data 
Demos 
Related Papers 
About arXivLabs 
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) 
 
    About 
    Help 
 
    contact arXiv Click here to contact arXiv Contact 
    subscribe to arXiv mailings Click here to subscribe Subscribe 
 
    Copyright 
    Privacy Policy 
 
    Web Accessibility Assistance 
 
    arXiv Operational Status 
    Get status notifications via email or slack 
 
